<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[使用Docker搭建Nginx+Php-fpm+Mysql]]></title>
      <url>%2F2018%2F03%2F31%2Fdocker-nginx-phpfpm-mysql%2F</url>
      <content type="text"><![CDATA[搭建MySQLstep1: 启动新的MySQL容器实例1234567&gt; docker container run \ # 启动一个新的容器实例，若容器对应的镜像不存在则自动从配置的远程仓库拉取 -d \ # 指定容器实例在后台运行 --rm \ # 当容器实例停止时，自动清理资源并删除该实例 --name MyDockerMysql \ # 指定容器实例的名称 -p 3307:3306 \ # 绑定本地3307端口至容器实例的3306端口 --env MYSQL_ROOT_PASSWORD=123456 \ # 指定容器实例启动时的环境变量，MYSQL_ROOT_PASSWORD对应MySQL初始化的root密码 mysql:5.7 # 指定容器镜像的名称和标识，用于从本地或远程仓库中查找用于实例化的镜像 step2: 检查是否启动成功1&gt; docker container ls --all 若启动成功，在执行完该命令后可以看到名称为”MyDockerMysql”的运行中的容器实例，同时还可以看到该实例的ID。若启动失败，则没有实例信息，此时可以把启动命令中的”-d”选项删去，重新执行以便查看错误情况。 step3: 查看实例的虚拟IP1&gt; docker container inspect MyDockerMysql | grep IPAddress 一般该IP为172.17.0.xx，后面的php测试脚本需要MySQL的IP信息。 step4: 在MySQL中新建数据库1&gt; mysql -h127.0.0.1 -P3307 -uroot -p123456 执行这一步时，需要在本地先安装mysql客户端，”-h”和”-P”选项用于指定MySQL的服务端IP和端口，由于启动实例时已指定了端口映射，因此指定”127.0.0.1:3307”与”172.17.0.xx:3306”都可以，”-u”和”-p”分别用于指定MySQL的用户名和密码。 1&gt; create database MyDockerDb; 新建名称为MyDockerDb的数据库。 搭建Php-fpmstep1: 创建本地映射目录与测试脚本1&gt; mkdir -p /data/MyDockerPhpfpm &amp;&amp; cd /data/MyDockerPhpfpm &amp;&amp; vim test.php 测试脚本的名称为”test.php”，内容如下： 12345678910111213&lt;?php$host = &apos;172.17.0.4&apos;; // 与上文中的MySQL容器实例的虚拟IP地址保持一致$port = 3306;$user = &apos;root&apos;;$password = &apos;123456&apos;;$db = &apos;MyDockerDb&apos;;$mysqli = new mysqli($host, $port, $user, $password, $db);if ($mysqli-&gt;connect_error) &#123; die(&apos;Connect Db Error(&apos; . $mysqli-&gt;connect_errno . &apos;) &apos; . $mysqli-&gt;connect_error);&#125;echo &apos;Success... &apos; . $mysqli-&gt;host_info . &quot;\n&quot; . PHP_EOL;$mysqli-&gt;close();?&gt; step2: 启动新的Php-fpm实例123456&gt; docker container run \ # 启动一个新的容器实例，若容器对应的镜像不存在则自动从配置的远程仓库拉取 -d \ # 指定容器实例在后台运行 --rm \ # 当容器实例停止时，自动清理资源并删除该实例 --name MyDockerPhpfpm \ # 指定容器实例的名称 -v &quot;/data/MyDockerPhpfpm/&quot;:/data # 指定目录映射，本地的/data/MyDocker映射至容器实例的/data目录 bitnami/php-fpm # 指定容器镜像的名称，用于从本地或远程仓库中查找用于实例化的镜像 step3: 检查是否启动成功1&gt; docker container ls --all 若启动成功，在执行完该命令后可以看到名称为”MyDockerPhpfpm”的运行中的容器实例，同时还可以看到该实例的ID。若启动失败，则没有实例信息，此时可以把启动命令中的”-d”选项删去，重新执行以便查看错误情况。Php-fpm的默认监听端口为9000。 step4: 查看实例的虚拟IP1&gt; docker container inspect MyDockerPhpfpm | grep IPAddress 一般该IP为172.17.0.xx，后面的nginx服务需要php-fpm的IP信息。 搭建Nginxstep1: 创建用于映射的本地nginx配置文件1&gt; mkdir /data/MyDockerNginx &amp;&amp; cd /data/MyDockerNginx &amp;&amp; vim nginx.conf “nginx.conf”是nginx默认配置文件的名称，该文件用于映射至容器实例的nginx配置文件，内容如下： 123456789101112131415161718192021222324252627282930313233worker_processes 1;pid nginx.pid;events &#123; worker_connections 256;&#125;http &#123; include mime.types; default_type application/octet-stream; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; sendfile on; keepalive_timeout 65; port_in_redirect off; tcp_nopush on; server &#123; listen 80; # 监听80端口 server_name localhost; index index.php index.html index.htm; root /data/MyDockerPhpfpm; # 与Php-fpm容器实例中测试脚本所在目录保持一致 error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; location ~ .*\.php$ &#123; # 当访问的资源名称以php结尾时，将请求转发至fastcgi处理器 fastcgi_pass 172.17.0.2:9000; # 与上文中的Php-fpm容器的虚拟IP地址保持一致 fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param SCRIPT_NAME $fastcgi_script_name; include fastcgi_params; &#125; &#125;&#125; 配置文件主要指定nginx监听的端口、fastcgi处理方式等信息，若对此不熟悉可参考nginx官方文档。 step2: 启动新的nginx实例1234567&gt; docker container run \ # 启动一个新的容器实例，若容器对应的镜像不存在则自动从配置的远程仓库拉取 -d \ # 指定容器实例在后台运行 --rm \ # 当容器实例停止时，自动清理资源并删除该实例 --name MyDockerNginx \ # 指定容器实例的名称 -p 8080:80 \ # 绑定本地8080端口至容器实例的80端口 -v &quot;/data/MyDockerNginx/nginx.conf&quot;:/etc/nginx/nginx.conf # 指定文件映射 nginx # 指定容器镜像的名称，用于从本地或远程仓库中查找用于实例化的镜像 /etc/nginx/nginx.conf是nginx容器实例中默认的nginx配置文件，启动的时候docker会把本地的文件自动映射至容器中。本地的8080端口映射容器的80端口，意味着后面我们访问本地的8080端口时，系统会自动将请求转发至nginx容器实例的80端口。 step3: 检查是否启动成功1&gt; docker container ls --all 若启动成功，在执行完该命令后可以看到名称为”MyDockerNginx”的运行中的容器实例，同时还可以看到该实例的ID。若启动失败，则没有实例信息，此时可以把启动命令中的”-d”选项删去，重新执行以便查看错误情况。 验证整体效果 打开本地浏览器，输入localhost:8080/test.php，若页面显示”Success…”说明上文的搭建已成功，否则需要排查问题，这里列举下搭建过程中可能遇到的问题及解决思路。 服务器连接失败 这种错误说明请求未找到对应的服务器，可能是nginx实例未启动或不在运行状态，也可能是url中的域名与端口不正确。 nginx返回404 这种错误说明nginx未找到请求的资源，需要检查Php-fpm容器实例是否正常运行，测试脚本名称是否正确，fastcgi相关配置是否正确，测试脚本是否真的位于Php-fpm容器实例的对应目录中。 nginx返回403 这种错误说明客户端没有权限访问该资源，可能是fastcgi的root根目录选项未正确配置（未与Php-fpm容器实例中的测试目录保持一致），也可能是Php-fpm实例中的测试目录或测试脚本的权限未正确设置。 数据库相关错误 这类错误说明连接数据库失败或选择指定的数据库失败，需要检查MySQL容器实例是否正常运行，数据库的相关信息（服务器地址、用户名、密码、数据库名称等）是否正确。 参考资料 阮一峰网络日志 - Docker入门教程 http://www.ruanyifeng.com/blog/2018/02/docker-tutorial.html 阮一峰网络日志 - Docker微服务教程 http://www.ruanyifeng.com/blog/2018/02/docker-wordpress-tutorial.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[SMART原则]]></title>
      <url>%2F2018%2F03%2F31%2Fsmart-principle%2F</url>
      <content type="text"><![CDATA[SMART原则 一般用于目标管理，适用于制定目标。 目标必须是具体的（Specific） 目标设置要有项目、衡量标准、达成措施、完成期限以及资源要求，使考核人能够很清晰的看到制定人计划要做哪些那些事情，计划完成到什么样的程度。 目标必须是可以衡量的（Measurable） 目标的衡量标准遵循“能量化的量化，不能量化的质化”。使制定人与考核人有一个统一的、标准的、清晰的可度量的标尺，杜绝在目标设置中使用形容词等概念模糊、无法衡量的描述。对于目标的可衡量性应该首先从数量、质量、成本、时间、上级或客户的满意程度五个方面来进行，如果仍不能进行衡量，其次可考虑将目标细化，细化成分目标后再从以上五个方面衡量，如果仍不能衡量，还可以将完成目标的工作进行流程化，通过流程化使目标可衡量。 目标必须是可以达到的（Attainable） 目标设置要坚持员工参与、上下左右沟通，使拟定的工作目标在组织及个人之间达成一致。既要使工作内容饱满，也要具有可达性。 目标必须和其他目标具有相关性（Relevant） 目标的相关性是指实现此目标与其他目标的关联情况。如果实现了这个目标，但对其他的目标完全不相关，或者相关度很低，那这个目标即使被达到了，意义也不是很大。 目标必须具有明确的截止期限（Time-based）目标设置要具有时间限制，根据工作任务的权重、事情的轻重缓急，拟定出完成目标项目的时间要求，定期检查项目的完成进度，及时掌握项目进展的变化情况，以方便对下属进行及时的工作指导，以及根据工作计划的异常情况变化及时地调整工作计划。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[C++ fastcgi模式下MySQL问题汇总]]></title>
      <url>%2F2017%2F05%2F14%2Fcpp-fastcgi-mysql%2F</url>
      <content type="text"><![CDATA[背景介绍 公司内部自研的WebServer支持fastcgi模式，对于C++语言，需要将代码编译为动态库。当WebServer启动的时候，会根据配置文件，启动指定数量的fastcgi进程，将对应的动态库加载到内存。对于后续的http请求，由WebServer指定空闲的fastcgi进行处理。 MySQL相关问题频繁创建与销毁MySQL连接 虽然fastcgi是常驻内存的，但每次fastcgi处理请求的时候，如果涉及MySQL处理，需要实时创建MySQL连接，处理完之后再将其销毁。这种方式虽然能正常处理请求，但是频繁地创建和销毁连接，不仅耗时，而且对数据库也会产生不小的压力。既然fastcgi是常驻内存的，那么可以采用单例模式来保存该连接，每次处理请求的时候，如果没有现成的连接，则创建新的连接并存储到单例中，如果已经有现成的连接，那么直接复用即可。 同时连接多个数据库 使用单例维持MySQL连接，可以解决频繁创建与销毁的问题，但是对于需要连接多个数据库的情况，简单的单例模式无法解决问题。此时需要对这种模式进行改进，对于不同的数据库连接，使用不同的名称进行标识。 MySQL请求超时处理 WebServer启动的fastcgi的数量是有限的，如果不限制MySQL的处理耗时，则有可能会出现大量fastcgi处理卡在MySQL请求等待的情况，这不仅会导致其他的http请求无法被WebServer正常处理，同时还会导致MySQL服务器连接数过高，无法正常处理其他连接请求。MySQL提供三种超时选项，分别是MYSQL_OPT_CONNECT_TIMEOUT（创建连接超时）、MYSQL_OPT_READ_TIMEOUT（读超时）和MYSQL_OPT_WRITE_TIMEOUT（写超时），默认情况下超时时间为0，表示不做超时处理，对于fastcgi模式，需要在建立连接的时候根据实际场景设置默认超时时间。另外，由于客户端与MySQL服务器之间的通讯使用TCP协议，存在重试机制，实际的超时时间一般比设置的时间更长。 MySQL静默连接超时自动断开 这里的静默连接超时与上文的创建连接超时不一样，创建连接超时指的是在创建连接的时候，客户端在特定时间内未收到服务器的正确返回，客户端主动断开连接，而静默连接超时指的是连接成功创建之后，在特定时间内客户端一直未发起其他请求，服务器主动断开连接。创建连接超时时间可以在客户端建立连接的时候进行设置，而静默超时时间需要在服务器进行设置，可以使用”show variables like ‘wait_timeout’;”命令查看当前设置。对于正常的fastcgi而言，很难出现半个小时或一个小时都没有被激活的情况，因此这种问题较难发现。 上文描述了使用单例模式维持MySQL连接，如果在wait_timeout设置时间内服务器一直未收到客户端发起的其他请求，服务器则会自动断开连接，此时客户端无感知，在处理下一个请求的时候，会出现”MySQL server has gone away”的错误提示。MySQL提供MYSQL_OPT_RECONNECT（静默超时之后自动重连）选项，同时提供mysql_ping接口用于检查连接是否仍然有效。对于fastcgi模式，可以在建立连接的时候设置自动重连，并且在每次执行MySQL请求之前使用mysql_ping检查连接是否仍然有效，若连接已失效，MySQL的API会主动重新建立连接。还有一种方式是在服务端把wait_timeout设置得足够长，但这种方式无法根除类似问题，同时可能导致MySQL连接数过高，因此不建议采用这种方式。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[命令行的艺术（转）]]></title>
      <url>%2F2017%2F05%2F05%2Fthe-art-of-command-line%2F</url>
      <content type="text"><![CDATA[命令行的艺术 前言 基础 日常使用 文件及数据处理 系统调试 单行脚本 冷门但有用 仅限 OS X 系统 仅限 Windows 系统 更多资源 免责声明 1curl -s &apos;https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README.md&apos; | egrep -o &apos;`\w+`&apos; | tr -d &apos;`&apos; | cowsay -W50 熟练使用命令行是一种常常被忽视，或被认为难以掌握的技能，但实际上，它会提高你作为工程师的灵活性以及生产力。本文是一份我在 Linux 上工作时，发现的一些命令行使用技巧的摘要。有些技巧非常基础，而另一些则相当复杂，甚至晦涩难懂。这篇文章并不长，但当你能够熟练掌握这里列出的所有技巧时，你就学会了很多关于命令行的东西了。 这篇文章是许多作者和译者共同的成果。这里的部分内容首次出现于 Quora，但已经迁移到了 Github，并由众多高手做出了许多改进。如果你在本文中发现了错误或者存在可以改善的地方，请贡献你的一份力量。 前言涵盖范围： 这篇文章不仅能帮助刚接触命令行的新手，而且对具有经验的人也大有裨益。本文致力于做到覆盖面广（涉及所有重要的内容），具体（给出具体的最常用的例子），以及简洁（避免冗余的内容，或是可以在其他地方轻松查到的细枝末节）。在特定应用场景下，本文的内容属于基本功或者能帮助您节约大量的时间。 本文主要为 Linux 所写，但在仅限 OS X 系统章节和仅限 Windows 系统章节中也包含有对应操作系统的内容。除去这两个章节外，其它的内容大部分均可在其他类 Unix 系统或 OS X，甚至 Cygwin 中得到应用。 本文主要关注于交互式 Bash，但也有很多技巧可以应用于其他 shell 和 Bash 脚本当中。 除去“标准的”Unix 命令，本文还包括了一些依赖于特定软件包的命令（前提是它们具有足够的价值）。 注意事项： 为了能在一页内展示尽量多的东西，一些具体的信息可以在引用的页面中找到。我们相信机智的你知道如何使用 Google 或者其他搜索引擎来查阅到更多的详细信息。文中部分命令需要您使用 apt-get，yum，dnf，pacman，pip 或 brew（以及其它合适的包管理器）来安装依赖的程序。 遇到问题的话，请尝试使用 Explainshell 去获取相关命令、参数、管道等内容的解释。 基础 学习 Bash 的基础知识。具体地，在命令行中输入 man bash 并至少全文浏览一遍; 它理解起来很简单并且不冗长。其他的 shell 可能很好用，但 Bash 的功能已经足够强大并且到几乎总是可用的（ 如果你只学习 zsh，fish 或其他的 shell 的话，在你自己的设备上会显得很方便，但过度依赖这些功能会给您带来不便，例如当你需要在服务器上工作时）。 熟悉至少一个基于文本的编辑器。通常而言 Vim （vi） 会是你最好的选择，毕竟在终端中编辑文本时 Vim 是最好用的工具（甚至大部分情况下 Vim 要比 Emacs、大型 IDE 或是炫酷的编辑器更好用）。 学会如何使用 man 命令去阅读文档。学会使用 apropos 去查找文档。知道有些命令并不对应可执行文件，而是在 Bash 内置好的，此时可以使用 help 和 help -d 命令获取帮助信息。你可以用 type 命令 来判断这个命令到底是可执行文件、shell 内置命令还是别名。 学会使用 &gt; 和 &lt; 来重定向输出和输入，学会使用 | 来重定向管道。明白 &gt; 会覆盖了输出文件而 &gt;&gt; 是在文件末添加。了解标准输出 stdout 和标准错误 stderr。 学会使用通配符 * （或许再算上 ? 和 […]） 和引用以及引用中 &#39; 和 &quot; 的区别（后文中有一些具体的例子）。 熟悉 Bash 中的任务管理工具：&amp;，ctrl-z，ctrl-c，jobs，fg，bg，kill 等。 学会使用 ssh 进行远程命令行登录，最好知道如何使用 ssh-agent，ssh-add 等命令来实现基础的无密码认证登录。 学会基本的文件管理工具：ls 和 ls -l （了解 ls -l 中每一列代表的意义），less，head，tail 和 tail -f （甚至 less +F），ln 和 ln -s （了解硬链接与软链接的区别），chown，chmod，du （硬盘使用情况概述：du -hs *）。 关于文件系统的管理，学习 df，mount，fdisk，mkfs，lsblk。知道 inode 是什么（与 ls -i 和 df -i 等命令相关）。 学习基本的网络管理工具：ip 或 ifconfig，dig。 学习并使用一种版本控制管理系统，例如 git。 熟悉正则表达式，学会使用 grep／egrep，它们的参数中 -i，-o，-v，-A，-B 和 -C 这些是很常用并值得认真学习的。 学会使用 apt-get，yum，dnf 或 pacman （具体使用哪个取决于你使用的 Linux 发行版）来查找和安装软件包。并确保你的环境中有 pip 来安装基于 Python 的命令行工具 （接下来提到的部分程序使用 pip 来安装会很方便）。 日常使用 在 Bash 中，可以通过按 Tab 键实现自动补全参数，使用 ctrl-r 搜索命令行历史记录（按下按键之后，输入关键字便可以搜索，重复按下 ctrl-r 会向后查找匹配项，按下 Enter 键会执行当前匹配的命令，而按下右方向键会将匹配项放入当前行中，不会直接执行，以便做出修改）。 在 Bash 中，可以按下 ctrl-w 删除你键入的最后一个单词，ctrl-u 可以删除行内光标所在位置之前的内容，alt-b 和 alt-f 可以以单词为单位移动光标，ctrl-a 可以将光标移至行首，ctrl-e 可以将光标移至行尾，ctrl-k 可以删除光标至行尾的所有内容，ctrl-l 可以清屏。键入 man readline 可以查看 Bash 中的默认快捷键。内容有很多，例如 alt-. 循环地移向前一个参数，而 alt-* 可以展开通配符。 你喜欢的话，可以执行 set -o vi 来使用 vi 风格的快捷键，而执行 set -o emacs 可以把它改回来。 为了便于编辑长命令，在设置你的默认编辑器后（例如 export EDITOR=vim），ctrl-x ctrl-e 会打开一个编辑器来编辑当前输入的命令。在 vi 风格下快捷键则是 escape-v。 键入 history 查看命令行历史记录，再用 !n（n 是命令编号）就可以再次执行。其中有许多缩写，最有用的大概就是 !$， 它用于指代上次键入的参数，而 !! 可以指代上次键入的命令了（参考 man 页面中的“HISTORY EXPANSION”）。不过这些功能，你也可以通过快捷键 ctrl-r 和 alt-. 来实现。 cd 命令可以切换工作路径，输入 cd ~ 可以进入 home 目录。要访问你的 home 目录中的文件，可以使用前缀 ~（例如 ~/.bashrc）。在 sh 脚本里则用环境变量 $HOME 指代 home 目录的路径。 回到前一个工作路径：cd -。 如果你输入命令的时候中途改了主意，按下 alt-# 在行首添加 # 把它当做注释再按下回车执行（或者依次按下 ctrl-a， #， enter）。这样做的话，之后借助命令行历史记录，你可以很方便恢复你刚才输入到一半的命令。 使用 xargs （ 或 parallel）。他们非常给力。注意到你可以控制每行参数个数（-L）和最大并行数（-P）。如果你不确定它们是否会按你想的那样工作，先使用 xargs echo 查看一下。此外，使用 -I{} 会很方便。例如： 12find . -name '*.py' | xargs grep some_functioncat hosts | xargs -I&#123;&#125; ssh root@&#123;&#125; hostname pstree -p 以一种优雅的方式展示进程树。 使用 pgrep 和 pkill 根据名字查找进程或发送信号（-f 参数通常有用）。 了解你可以发往进程的信号的种类。比如，使用 kill -STOP [pid] 停止一个进程。使用 man 7 signal 查看详细列表。 使用 nohup 或 disown 使一个后台进程持续运行。 使用 netstat -lntp 或 ss -plat 检查哪些进程在监听端口（默认是检查 TCP 端口; 添加参数 -u 则检查 UDP 端口）。 lsof 来查看开启的套接字和文件。 使用 uptime 或 w 来查看系统已经运行多长时间。 使用 alias 来创建常用命令的快捷形式。例如：alias ll=&#39;ls -latr&#39; 创建了一个新的命令别名 ll。 可以把别名、shell 选项和常用函数保存在 ~/.bashrc，具体看下这篇文章。这样做的话你就可以在所有 shell 会话中使用你的设定。 把环境变量的设定以及登陆时要执行的命令保存在 ~/.bash_profile。而对于从图形界面启动的 shell 和 cron 启动的 shell，则需要单独配置文件。 要想在几台电脑中同步你的配置文件（例如 .bashrc 和 .bash_profile），可以借助 Git。 当变量和文件名中包含空格的时候要格外小心。Bash 变量要用引号括起来，比如 &quot;$FOO&quot;。尽量使用 -0 或 -print0 选项以便用 NULL 来分隔文件名，例如 locate -0 pattern | xargs -0 ls -al 或 find / -print0 -type d | xargs -0 ls -al。如果 for 循环中循环访问的文件名含有空字符（空格、tab 等字符），只需用 IFS=$&#39;\n&#39; 把内部字段分隔符设为换行符。 在 Bash 脚本中，使用 set -x 去调试输出（或者使用它的变体 set -v，它会记录原始输入，包括多余的参数和注释）。尽可能地使用严格模式：使用 set -e 令脚本在发生错误时退出而不是继续运行；使用 set -u 来检查是否使用了未赋值的变量；试试 set -o pipefail，它可以监测管道中的错误。当牵扯到很多脚本时，使用 trap 来检测 ERR 和 EXIT。一个好的习惯是在脚本文件开头这样写，这会使它能够检测一些错误，并在错误发生时中断程序并输出信息： 12set -euo pipefailtrap "echo 'error: Script failed: see failed command above'" ERR 在 Bash 脚本中，子 shell（使用括号 (...)）是一种组织参数的便捷方式。一个常见的例子是临时地移动工作路径，代码如下： 123# do something in current dir(cd /some/other/dir &amp;&amp; other-command)# continue in original dir 在 Bash 中，变量有许多的扩展方式。${name:?error message} 用于检查变量是否存在。此外，当 Bash 脚本只需要一个参数时，可以使用这样的代码 input_file=${1:?usage: $0 input_file}。在变量为空时使用默认值：${name:-default}。如果你要在之前的例子中再加一个（可选的）参数，可以使用类似这样的代码 output_file=${2:-logfile}，如果省略了 $2，它的值就为空，于是 output_file 就会被设为 logfile。数学表达式：i=$(( (i + 1) % 5 ))。序列：{1..10}。截断字符串：${var%suffix} 和 ${var#prefix}。例如，假设 var=foo.pdf，那么 echo ${var%.pdf}.txt 将输出 foo.txt。 使用括号扩展（{…}）来减少输入相似文本，并自动化文本组合。这在某些情况下会很有用，例如 mv foo.{txt,pdf} some-dir（同时移动两个文件），cp somefile{,.bak}（会被扩展成 cp somefile somefile.bak）或者 mkdir -p test-{a,b,c}/subtest-{1,2,3}（会被扩展成所有可能的组合，并创建一个目录树）。 通过使用 &lt;(some command) 可以将输出视为文件。例如，对比本地文件 /etc/hosts 和一个远程文件： 1diff /etc/hosts &lt;(ssh somehost cat /etc/hosts) 编写脚本时，你可能会想要把代码都放在大括号里。缺少右括号的话，代码就会因为语法错误而无法执行。如果你的脚本是要放在网上分享供他人使用的，这样的写法就体现出它的好处了，因为这样可以防止下载不完全代码被执行。 123&#123; # 在这里写代码&#125; 了解 Bash 中的“here documents”，例如 cat &lt;&lt;EOF ...。 在 Bash 中，同时重定向标准输出和标准错误：some-command &gt;logfile 2&gt;&amp;1 或者 some-command &amp;&gt;logfile。通常，为了保证命令不会在标准输入里残留一个未关闭的文件句柄捆绑在你当前所在的终端上，在命令后添加 &lt;/dev/null 是一个好习惯。 使用 man ascii 查看具有十六进制和十进制值的ASCII表。man unicode，man utf-8，以及 man latin1 有助于你去了解通用的编码信息。 使用 screen 或 tmux 来使用多份屏幕，当你在使用 ssh 时（保存 session 信息）将尤为有用。而 byobu 可以为它们提供更多的信息和易用的管理工具。另一个轻量级的 session 持久化解决方案是 dtach。 ssh 中，了解如何使用 -L 或 -D（偶尔需要用 -R）开启隧道是非常有用的，比如当你需要从一台远程服务器上访问 web 页面。 对 ssh 设置做一些小优化可能是很有用的，例如这个 ~/.ssh/config 文件包含了防止特定网络环境下连接断开、压缩数据、多通道等选项： 1234567TCPKeepAlive=yesServerAliveInterval=15ServerAliveCountMax=6Compression=yesControlMaster autoControlPath /tmp/%r@%h:%pControlPersist yes 一些其他的关于 ssh 的选项是与安全相关的，应当小心翼翼的使用。例如你应当只能在可信任的网络中启用 StrictHostKeyChecking=no，ForwardAgent=yes。 考虑使用 mosh 作为 ssh 的替代品，它使用 UDP 协议。它可以避免连接被中断并且对带宽需求更小，但它需要在服务端做相应的配置。 获取八进制形式的文件访问权限（修改系统设置时通常需要，但 ls 的功能不那么好用并且通常会搞砸），可以使用类似如下的代码： 1stat -c '%A %a %n' /etc/timezone 使用 percol 或者 fzf 可以交互式地从另一个命令输出中选取值。 使用 fpp（PathPicker）可以与基于另一个命令(例如 git）输出的文件交互。 将 web 服务器上当前目录下所有的文件（以及子目录）暴露给你所处网络的所有用户，使用：python -m SimpleHTTPServer 7777 （使用端口 7777 和 Python 2）或python -m http.server 7777 （使用端口 7777 和 Python 3）。 以其他用户的身份执行命令，使用 sudo。默认以 root 用户的身份执行；使用 -u 来指定其他用户。使用 -i 来以该用户登录（需要输入你自己的密码）。 将 shell 切换为其他用户，使用 su username 或者 sudo - username。加入 - 会使得切换后的环境与使用该用户登录后的环境相同。省略用户名则默认为 root。切换到哪个用户，就需要输入哪个用户的密码。 了解命令行的 128K 限制。使用通配符匹配大量文件名时，常会遇到“Argument list too long”的错误信息。（这种情况下换用 find 或 xargs 通常可以解决。） 当你需要一个基本的计算器时，可以使用 python 解释器（当然你要用 python 的时候也是这样）。例如： 12&gt;&gt;&gt; 2+35 文件及数据处理 在当前目录下通过文件名查找一个文件，使用类似于这样的命令：find . -iname &#39;*something*&#39;。在所有路径下通过文件名查找文件，使用 locate something （但注意到 updatedb 可能没有对最近新建的文件建立索引，所以你可能无法定位到这些未被索引的文件）。 使用 ag 在源代码或数据文件里检索（grep -r 同样可以做到，但相比之下 ag 更加先进）。 将 HTML 转为文本：lynx -dump -stdin。 Markdown，HTML，以及所有文档格式之间的转换，试试 pandoc。 当你要处理棘手的 XML 时候，xmlstarlet 算是上古时代流传下来的神器。 使用 jq 处理 JSON。 使用 shyaml 处理 YAML。 要处理 Excel 或 CSV 文件的话，csvkit 提供了 in2csv，csvcut，csvjoin，csvgrep 等方便易用的工具。 当你要处理 Amazon S3 相关的工作的时候，s3cmd 是一个很方便的工具而 s4cmd 的效率更高。Amazon 官方提供的 aws 以及 saws 是其他 AWS 相关工作的基础，值得学习。 了解如何使用 sort 和 uniq，包括 uniq 的 -u 参数和 -d 参数，具体内容在后文单行脚本节中。另外可以了解一下 comm。 了解如何使用 cut，paste 和 join 来更改文件。很多人都会使用 cut，但遗忘了 join。 了解如何运用 wc 去计算新行数（-l），字符数（-m），单词数（-w）以及字节数（-c）。 了解如何使用 tee 将标准输入复制到文件甚至标准输出，例如 ls -al | tee file.txt。 要进行一些复杂的计算，比如分组、逆序和一些其他的统计分析，可以考虑使用 datamash。 注意到语言设置（中文或英文等）对许多命令行工具有一些微妙的影响，比如排序的顺序和性能。大多数 Linux 的安装过程会将 LANG 或其他有关的变量设置为符合本地的设置。要意识到当你改变语言设置时，排序的结果可能会改变。明白国际化可能会使 sort 或其他命令运行效率下降许多倍。某些情况下（例如集合运算）你可以放心的使用 export LC_ALL=C 来忽略掉国际化并按照字节来判断顺序。 你可以单独指定某一条命令的环境，只需在调用时把环境变量设定放在命令的前面，例如 TZ=Pacific/Fiji date 可以获取斐济的时间。 了解如何使用 awk 和 sed 来进行简单的数据处理。例如，将文本文件中第三列的所有数字求和：awk &#39;{ x += $3 } END { print x }&#39;。这可能比同等功能的 Python 代码快三倍且代码量少三倍。 替换一个或多个文件中出现的字符串： 1perl -pi.bak -e 's/old-string/new-string/g' my-files-*.txt 使用 repren 来批量重命名文件，或是在多个文件中搜索替换内容。（有些时候 rename 命令也可以批量重命名，但要注意，它在不同 Linux 发行版中的功能并不完全一样。） 123456# 将文件、目录和内容全部重命名 foo -&gt; bar:repren --full --preserve-case --from foo --to bar .# 还原所有备份文件 whatever.bak -&gt; whatever:repren --renames --from '(.*)\.bak' --to '\1' *.bak# 用 rename 实现上述功能（若可用）:rename 's/\.bak$//' *.bak 根据 man 页面的描述，rsync 是一个快速且非常灵活的文件复制工具。它闻名于设备之间的文件同步，但其实它在本地情况下也同样有用。在安全设置允许下，用 rsync 代替 scp 可以实现文件续传，而不用重新从头开始。它同时也是删除大量文件的最快方法之一： 1mkdir empty &amp;&amp; rsync -r --delete empty/ some-dir &amp;&amp; rmdir some-dir 使用 shuf 可以以行为单位来打乱文件的内容或从一个文件中随机选取多行。 了解 sort 的参数。显示数字时，使用 -n 或者 -h 来显示更易读的数（例如 du -h 的输出）。明白排序时关键字的工作原理（-t 和 -k）。例如，注意到你需要 -k1，1 来仅按第一个域来排序，而 -k1 意味着按整行排序。稳定排序（sort -s）在某些情况下很有用。例如，以第二个域为主关键字，第一个域为次关键字进行排序，你可以使用 sort -k1，1 | sort -s -k2，2。 如果你想在 Bash 命令行中写 tab 制表符，按下 ctrl-v [Tab] 或键入 $&#39;\t&#39; （后者可能更好，因为你可以复制粘贴它）。 标准的源代码对比及合并工具是 diff 和 patch。使用 diffstat 查看变更总览数据。注意到 diff -r 对整个文件夹有效。使用 diff -r tree1 tree2 | diffstat 查看变更的统计数据。vimdiff 用于比对并编辑文件。 对于二进制文件，使用 hd，hexdump 或者 xxd 使其以十六进制显示，使用 bvi，hexedit 或者 biew 来进行二进制编辑。 同样对于二进制文件，strings（包括 grep 等工具）可以帮助在二进制文件中查找特定比特。 制作二进制差分文件（Delta 压缩），使用 xdelta3。 使用 iconv 更改文本编码。需要更高级的功能，可以使用 uconv，它支持一些高级的 Unicode 功能。例如，这条命令移除了所有重音符号： 1uconv -f utf-8 -t utf-8 -x '::Any-Lower; ::Any-NFD; [:Nonspacing Mark:] &gt;; ::Any-NFC; ' &lt; input.txt &gt; output.txt 拆分文件可以使用 split（按大小拆分）和 csplit（按模式拆分）。 操作日期和时间表达式，可以用 dateutils 中的 dateadd、datediff、strptime 等工具。 使用 zless、zmore、zcat 和 zgrep 对压缩过的文件进行操作。 文件属性可以通过 chattr 进行设置，它比文件权限更加底层。例如，为了保护文件不被意外删除，可以使用不可修改标记：sudo chattr +i /critical/directory/or/file 使用 getfacl 和 setfacl 以保存和恢复文件权限。例如： 12getfacl -R /some/path &gt; permissions.txtsetfacl --restore=permissions.txt 为了高效地创建空文件，请使用 truncate（创建稀疏文件），fallocate（用于 ext4，xfs，btrf 和 ocfs2 文件系统），xfs_mkfile（适用于几乎所有的文件系统，包含在 xfsprogs 包中），mkfile（用于类 Unix 操作系统，比如 Solaris 和 Mac OS）。 系统调试 curl 和 curl -I 可以被轻松地应用于 web 调试中，它们的好兄弟 wget 也是如此，或者也可以试试更潮的 httpie。 获取 CPU 和硬盘的使用状态，通常使用使用 top（htop 更佳），iostat 和 iotop。而 iostat -mxz 15 可以让你获悉 CPU 和每个硬盘分区的基本信息和性能表现。 使用 netstat 和 ss 查看网络连接的细节。 dstat 在你想要对系统的现状有一个粗略的认识时是非常有用的。然而若要对系统有一个深度的总体认识，使用 glances，它会在一个终端窗口中向你提供一些系统级的数据。 若要了解内存状态，运行并理解 free 和 vmstat 的输出。值得留意的是“cached”的值，它指的是 Linux 内核用来作为文件缓存的内存大小，而与空闲内存无关。 Java 系统调试则是一件截然不同的事，一个可以用于 Oracle 的 JVM 或其他 JVM 上的调试的技巧是你可以运行 kill -3 &lt;pid&gt; 同时一个完整的栈轨迹和堆概述（包括 GC 的细节）会被保存到标准错误或是日志文件。JDK 中的 jps，jstat，jstack，jmap 很有用。SJK tools 更高级。 使用 mtr 去跟踪路由，用于确定网络问题。 用 ncdu 来查看磁盘使用情况，它比寻常的命令，如 du -sh *，更节省时间。 查找正在使用带宽的套接字连接或进程，使用 iftop 或 nethogs。 ab 工具（Apache 中自带）可以简单粗暴地检查 web 服务器的性能。对于更复杂的负载测试，使用 siege。 wireshark，tshark 和 ngrep 可用于复杂的网络调试。 了解 strace 和 ltrace。这俩工具在你的程序运行失败、挂起甚至崩溃，而你却不知道为什么或你想对性能有个总体的认识的时候是非常有用的。注意 profile 参数（-c）和附加到一个运行的进程参数 （-p）。 了解使用 ldd 来检查共享库。 了解如何运用 gdb 连接到一个运行着的进程并获取它的堆栈轨迹。 学会使用 /proc。它在调试正在出现的问题的时候有时会效果惊人。比如：/proc/cpuinfo，/proc/meminfo，/proc/cmdline，/proc/xxx/cwd，/proc/xxx/exe，/proc/xxx/fd/，/proc/xxx/smaps（这里的 xxx 表示进程的 id 或 pid）。 当调试一些之前出现的问题的时候，sar 非常有用。它展示了 cpu、内存以及网络等的历史数据。 关于更深层次的系统分析以及性能分析，看看 stap（SystemTap），perf)，以及sysdig。 查看你当前使用的系统，使用 uname，uname -a（Unix／kernel 信息）或者 lsb_release -a（Linux 发行版信息）。 无论什么东西工作得很欢乐（可能是硬件或驱动问题）时可以试试 dmesg。 如果你删除了一个文件，但通过 du 发现没有释放预期的磁盘空间，请检查文件是否被进程占用：lsof | grep deleted | grep &quot;filename-of-my-big-file&quot; 单行脚本一些命令组合的例子： 当你需要对文本文件做集合交、并、差运算时，sort 和 uniq 会是你的好帮手。具体例子请参照代码后面的，此处假设 a 与 b 是两内容不同的文件。这种方式效率很高，并且在小文件和上 G 的文件上都能运用（注意尽管在 /tmp 在一个小的根分区上时你可能需要 -T 参数，但是实际上 sort 并不被内存大小约束），参阅前文中关于 LC_ALL 和 sort 的 -u 参数的部分。 123cat a b | sort | uniq &gt; c # c 是 a 并 bcat a b | sort | uniq -d &gt; c # c 是 a 交 bcat a b b | sort | uniq -u &gt; c # c 是 a - b 使用 grep . *（每行都会附上文件名）或者 head -100 *（每个文件有一个标题）来阅读检查目录下所有文件的内容。这在检查一个充满配置文件的目录（如 /sys、/proc、/etc）时特别好用。 计算文本文件第三列中所有数的和（可能比同等作用的 Python 代码快三倍且代码量少三倍）： 1awk '&#123; x += $3 &#125; END &#123; print x &#125;' myfile 如果你想在文件树上查看大小/日期，这可能看起来像递归版的 ls -l 但比 ls -lR 更易于理解： 1find . -type f -ls 假设你有一个类似于 web 服务器日志文件的文本文件，并且一个确定的值只会出现在某些行上，假设一个 acct_id 参数在 URI 中。如果你想计算出每个 acct_id 值有多少次请求，使用如下代码： 1cat access.log | egrep -o 'acct_id=[0-9]+' | cut -d= -f2 | sort | uniq -c | sort -rn 要持续监测文件改动，可以使用 watch，例如检查某个文件夹中文件的改变，可以用 watch -d -n 2 &#39;ls -rtlh | tail&#39;；或者在排查 WiFi 设置故障时要监测网络设置的更改，可以用 watch -d -n 2 ifconfig。 运行这个函数从这篇文档中随机获取一条技巧（解析 Markdown 文件并抽取项目）： 12345678function taocl() &#123; curl -s https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README-zh.md| pandoc -f markdown -t html | iconv -f 'utf-8' -t 'unicode' | xmlstarlet fo --html --dropdtd | xmlstarlet sel -t -v "(html/body/ul/li[count(p)&gt;0])[$RANDOM mod last()+1]" | xmlstarlet unesc | fmt -80&#125; 冷门但有用 expr：计算表达式或正则匹配 m4：简单的宏处理器 yes：多次打印字符串 cal：漂亮的日历 env：执行一个命令（脚本文件中很有用） printenv：打印环境变量（调试时或在写脚本文件时很有用） look：查找以特定字符串开头的单词或行 cut，paste 和 join：数据修改 fmt：格式化文本段落 pr：将文本格式化成页／列形式 fold：包裹文本中的几行 column：将文本格式化成多个对齐、定宽的列或表格 expand 和 unexpand：制表符与空格之间转换 nl：添加行号 seq：打印数字 bc：计算器 factor：分解因数 gpg：加密并签名文件 toe：terminfo 入口列表 nc：网络调试及数据传输 socat：套接字代理，与 netcat 类似 slurm：网络流量可视化 dd：文件或设备间传输数据 file：确定文件类型 tree：以树的形式显示路径和文件，类似于递归的 ls stat：文件信息 time：执行命令，并计算执行时间 timeout：在指定时长范围内执行命令，并在规定时间结束后停止进程 lockfile：使文件只能通过 rm -f 移除 logrotate： 切换、压缩以及发送日志文件 watch：重复运行同一个命令，展示结果并／或高亮有更改的部分 tac：反向输出文件 shuf：文件中随机选取几行 comm：一行一行的比较排序过的文件 pv：监视通过管道的数据 strings：从二进制文件中抽取文本 tr：转换字母 iconv 或 uconv：文本编码转换 split 和 csplit：分割文件 sponge：在写入前读取所有输入，在读取文件后再向同一文件写入时比较有用，例如 grep -v something some-file | sponge some-file units：将一种计量单位转换为另一种等效的计量单位（参阅 /usr/share/units/definitions.units） apg：随机生成密码 xz：高比例的文件压缩 ldd：动态库信息 nm：提取 obj 文件中的符号 ab：web 服务器性能分析 strace：调试系统调用 mtr：更好的网络调试跟踪工具 cssh：可视化的并发 shell rsync：通过 ssh 或本地文件系统同步文件和文件夹 wireshark 和 tshark：抓包和网络调试工具 ngrep：网络层的 grep host 和 dig：DNS 查找 lsof：列出当前系统打开文件的工具以及查看端口信息 dstat：系统状态查看 glances：高层次的多子系统总览 iostat：硬盘使用状态 mpstat： CPU 使用状态 vmstat： 内存使用状态 htop：top 的加强版 last：登入记录 w：查看处于登录状态的用户 id：用户/组 ID 信息 sar：系统历史数据 iftop 或 nethogs：套接字及进程的网络利用情况 ss：套接字数据 dmesg：引导及系统错误信息 sysctl： 在内核运行时动态地查看和修改内核的运行参数 hdparm：SATA/ATA 磁盘更改及性能分析 lsblk：列出块设备信息：以树形展示你的磁盘以及磁盘分区信息 lshw，lscpu，lspci，lsusb 和 dmidecode：查看硬件信息，包括 CPU、BIOS、RAID、显卡、USB设备等 lsmod 和 modinfo：列出内核模块，并显示其细节 fortune，ddate 和 sl：额，这主要取决于你是否认为蒸汽火车和莫名其妙的名人名言是否“有用” 仅限 OS X 系统以下是仅限于 OS X 系统的技巧。 用 brew （Homebrew）或者 port （MacPorts）进行包管理。这些可以用来在 OS X 系统上安装以上的大多数命令。 用 pbcopy 复制任何命令的输出到桌面应用，用 pbpaste 粘贴输入。 若要在 OS X 终端中将 Option 键视为 alt 键（例如在上面介绍的 alt-b、alt-f 等命令中用到），打开 偏好设置 -&gt; 描述文件 -&gt; 键盘 并勾选“使用 Option 键作为 Meta 键”。 用 open 或者 open -a /Applications/Whatever.app 使用桌面应用打开文件。 Spotlight：用 mdfind 搜索文件，用 mdls 列出元数据（例如照片的 EXIF 信息）。 注意 OS X 系统是基于 BSD UNIX 的，许多命令（例如 ps，ls，tail，awk，sed）都和 Linux 中有微妙的不同（ Linux 很大程度上受到了 System V-style Unix 和 GNU 工具影响）。你可以通过标题为 “BSD General Commands Manual” 的 man 页面发现这些不同。在有些情况下 GNU 版本的命令也可能被安装（例如 gawk 和 gsed 对应 GNU 中的 awk 和 sed ）。如果要写跨平台的 Bash 脚本，避免使用这些命令（例如，考虑 Python 或者 perl ）或者经过仔细的测试。 用 sw_vers 获取 OS X 的版本信息。 仅限 Windows 系统以下是仅限于 Windows 系统的技巧。 在 Windows 10 上，你可以使用 Bash on Ubuntu on Windows，它提供了一个熟悉的 Bash 环境，包含了不少 Unix 命令行工具。好处是它允许 Linux 上编写的程序能够在 Windows 上运行，而另一方面，Windows 上编写的程序却无法在 Bash 命令行中运行。 可以安装 Cygwin 允许你在 Microsoft Windows 中体验 Unix shell 的威力。这样的话，本文中介绍的大多数内容都将适用。 通过 Cygwin 的包管理器来安装额外的 Unix 程序。 使用 mintty 作为你的命令行窗口。 要访问 Windows 剪贴板，可以通过 /dev/clipboard。 运行 cygstart 以通过默认程序打开一个文件。 要访问 Windows 注册表，可以使用 regtool。 注意 Windows 驱动器路径 C:\ 在 Cygwin 中用 /cygdrive/c 代表，而 Cygwin 的 / 代表 Windows 中的 C:\cygwin。要转换 Cygwin 和 Windows 风格的路径可以用 cygpath。这在需要调用 Windows 程序的脚本里很有用。 学会使用 wmic，你就可以从命令行执行大多数 Windows 系统管理任务，并编成脚本。 要在 Windows 下获得 Unix 的界面和体验，另一个办法是使用 Cash。需要注意的是，这个环境支持的 Unix 命令和命令行参数非常少。 要在 Windows 上获取 GNU 开发者工具（比如 GCC）的另一个办法是使用 MinGW 以及它的 MSYS 软件包，该软件包提供了 bash、gawk、make、grep 等工具。然而 MSYS 提供的功能没有 Cygwin 完善。MinGW 在创建 Unix 工具的 Windows 原生移植方面非常有用。 更多资源 awesome-shell：一份精心组织的命令行工具及资源的列表。 awesome-osx-command-line：一份针对 OS X 命令行的更深入的指南。 Strict mode：为了编写更好的脚本文件。 shellcheck：一个静态 shell 脚本分析工具，本质上是 bash／sh／zsh 的 lint。 Filenames and Pathnames in Shell：有关如何在 shell 脚本里正确处理文件名的细枝末节。 Data Science at the Command Line：用于数据科学的一些命令和工具，摘自同名书籍。 免责声明除去特别小的工作，你编写的代码应当方便他人阅读。能力往往伴随着责任，你 有能力 在 Bash 中玩一些奇技淫巧并不意味着你应该去做！;) 授权条款 本文使用授权协议 Creative Commons Attribution-ShareAlike 4.0 International License。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux IO模式及 select、poll、epoll详解（转）]]></title>
      <url>%2F2017%2F04%2F22%2Flinux-IO-select-poll-epoll%2F</url>
      <content type="text"><![CDATA[转自https://segmentfault.com/a/1190000003063859 概念说明在进行解释之前，首先要说明几个概念： 用户空间和内核空间 进程切换 进程的阻塞 文件描述符 缓存 I/O 用户空间与内核空间现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。 进程切换为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。 从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化： 保存处理机上下文，包括程序计数器和其他寄存器。 更新PCB信息。 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。 选择另一个进程执行，并更新其PCB。 更新内存管理的数据结构。 恢复处理机上下文。 注：总而言之就是很耗资源，具体的可以参考这篇文章：进程切换 进程的阻塞正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。 文件描述符fd文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。 文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。 缓存 I/O缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。 缓存 I/O 的缺点：数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。 IO模式刚才说了，对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段： 等待数据准备 (Waiting for the data to be ready) 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process) 正式因为这两个阶段，linux系统产生了下面五种网络模式的方案。 阻塞 I/O（blocking IO） 非阻塞 I/O（nonblocking IO） I/O 多路复用（ IO multiplexing） 信号驱动 I/O（ signal driven IO） 异步 I/O（asynchronous IO） 注：由于signal driven IO在实际中并不常用，所以我这只提及剩下的四种IO Model。 阻塞 I/O（blocking IO）在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程大概是这样： 当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。 所以，blocking IO的特点就是在IO执行的两个阶段都被block了。 非阻塞 I/O（nonblocking IO）linux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程是这个样子： 当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。 所以，nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有。 I/O 多路复用（IO multiplexing）IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。 当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。 所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。 这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。 所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。） 在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。 异步 I/O（asynchronous IO）Linux下的asynchronous IO其实用得很少。先看一下它的流程： 用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 总结blocking和non-blocking的区别调用blocking IO会一直block住对应的进程直到操作完成，而non-blocking IO在kernel还准备数据的情况下会立刻返回。 synchronous IO和asynchronous IO的区别在说明synchronous IO和asynchronous IO的区别之前，需要先给出两者的定义。POSIX的定义是这样子的： A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes; An asynchronous I/O operation does not cause the requesting process to be blocked; 两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。 有人会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的recvfrom这个system call。non-blocking IO在执行recvfrom这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候，recvfrom会将数据从kernel拷贝到用户内存中，这个时候进程是被block了，在这段时间内，进程是被block的。 而asynchronous IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。 各个IO Model的比较如图所示： 通过上面的图片，可以发现non-blocking IO和asynchronous IO的区别还是很明显的。在non-blocking IO中，虽然进程大部分时间都不会被block，但是它仍然要求进程去主动的check，并且当数据准备完成以后，也需要进程主动的再次调用recvfrom来将数据拷贝到用户内存。而asynchronous IO则完全不同。它就像是用户进程将整个IO操作交给了他人（kernel）完成，然后他人做完后发信号通知。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据。 I/O 多路复用之select、poll、epoll详解select，poll，epoll都是IO多路复用的机制。I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。（这里啰嗦下） select1int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); select 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以 通过遍历fdset，来找到就绪的描述符。 select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select的一 个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但 是这样也会造成效率的降低。 poll1int poll (struct pollfd *fds, unsigned int nfds, int timeout); 不同与select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。 12345struct pollfd &#123; int fd; /* file descriptor */ short events; /* requested events to watch */ short revents; /* returned events witnessed */&#125;; pollfd结构包含了要监视的event和发生的event，不再使用select“参数-值”传递的方式。同时，pollfd并没有最大数量限制（但是数量过大后性能也是会下降）。 和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。 从上面看，select和poll都需要在返回后，通过遍历文件描述符来获取已经就绪的socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。 epollepoll是在2.6内核中提出的，是之前的select和poll的增强版本。相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。 epoll操作过程epoll操作过程需要三个接口，分别如下： 123int epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); int epoll_create(int size);函数创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大，这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值，参数size并不是限制了epoll所能监听的描述符最大个数，只是对内核初始分配内部数据结构的一个建议。当创建好epoll句柄后，它就会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);函数是对指定描述符fd执行op操作。 epfd：是epoll_create()的返回值。 op：表示op操作，用三个宏来表示：添加EPOLL_CTL_ADD，删除EPOLL_CTL_DEL，修改EPOLL_CTL_MOD。分别添加、删除和修改对fd的监听事件。 fd：是需要监听的fd（文件描述符） epoll_event：是告诉内核需要监听什么事，struct epoll_event结构如下： 12345678910111213struct epoll_event &#123; __uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */&#125;;//events可以是以下几个宏的集合：EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；EPOLLOUT：表示对应的文件描述符可以写；EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；EPOLLERR：表示对应的文件描述符发生错误；EPOLLHUP：表示对应的文件描述符被挂断；EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里 int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);函数等待epfd上的io事件，最多返回maxevents个事件。参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。 工作模式epoll对文件描述符的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下：LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。 LT模式LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的。 ET模式ET(edge-triggered)是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK 错误）。但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once) ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。 总结假如有这样一个例子： 我们已经把一个用来从管道中读取数据的文件句柄(RFD)添加到epoll描述符 这个时候从管道的另一端被写入了2KB的数据 调用epoll_wait(2)，并且它会返回RFD，说明它已经准备好读取操作 然后我们读取了1KB的数据 调用epoll_wait(2)…… LT模式：如果是LT模式，那么在第5步调用epoll_wait(2)之后，仍然能受到通知。 ET模式：如果我们在第1步将RFD添加到epoll描述符的时候使用了EPOLLET标志，那么在第5步调用epoll_wait(2)之后将有可能会挂起，因为剩余的数据还存在于文件的输入缓冲区内，而且数据发出端还在等待一个针对已经发出数据的反馈信息。只有在监视的文件句柄上发生了某个事件的时候 ET 工作模式才会汇报事件。因此在第5步的时候，调用者可能会放弃等待仍在存在于文件输入缓冲区内的剩余数据。 当使用epoll的ET模型来工作时，当产生了一个EPOLLIN事件后， 读数据的时候需要考虑的是当recv()返回的大小如果等于请求的大小，那么很有可能是缓冲区还有数据未读完，也意味着该次事件还没有处理完，所以还需要再次读取： 1234567891011121314151617181920212223while(rs)&#123; buflen = recv(activeevents[i].data.fd, buf, sizeof(buf), 0); if(buflen &lt; 0)&#123; // 由于是非阻塞的模式,所以当errno为EAGAIN时,表示当前缓冲区已无数据可读 // 在这里就当作是该次事件已处理处. if(errno == EAGAIN)&#123; break; &#125; else&#123; return; &#125; &#125; else if(buflen == 0)&#123; // 这里表示对端的socket已正常关闭. &#125; if(buflen == sizeof(buf)&#123; rs = 1; // 需要再次读取 &#125; else&#123; rs = 0; &#125;&#125; Linux中的EAGAIN含义 Linux环境下开发经常会碰到很多错误(设置errno)，其中EAGAIN是其中比较常见的一个错误(比如用在非阻塞操作中)。 从字面上来看，是提示再试一次。这个错误经常出现在当应用程序进行一些非阻塞(non-blocking)操作(对文件或socket)的时候。 例如，以 O_NONBLOCK的标志打开文件/socket/FIFO，如果你连续做read操作而没有数据可读。此时程序不会阻塞起来等待数据准备就绪返回，read函数会返回一个错误EAGAIN，提示你的应用程序现在没有数据可读请稍后再试。 又例如，当一个系统调用(比如fork)因为没有足够的资源(比如虚拟内存)而执行失败，返回EAGAIN提示其再调用一次(也许下次就能成功)。 代码演示123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118#define IPADDRESS &quot;127.0.0.1&quot;#define PORT 8787#define MAXSIZE 1024#define LISTENQ 5#define FDSIZE 1000#define EPOLLEVENTS 100listenfd = socket_bind(IPADDRESS,PORT);struct epoll_event events[EPOLLEVENTS];//创建一个描述符epollfd = epoll_create(FDSIZE);//添加监听描述符事件add_event(epollfd,listenfd,EPOLLIN);//循环等待for ( ; ; )&#123; //该函数返回已经准备好的描述符事件数目 ret = epoll_wait(epollfd,events,EPOLLEVENTS,-1); //处理接收到的连接 handle_events(epollfd,events,ret,listenfd,buf);&#125;//事件处理函数static void handle_events(int epollfd,struct epoll_event *events,int num,int listenfd,char *buf)&#123; int i; int fd; //进行遍历;这里只要遍历已经准备好的io事件。num并不是当初epoll_create时的FDSIZE。 for (i = 0;i &lt; num;i++) &#123; fd = events[i].data.fd; //根据描述符的类型和事件类型进行处理 if ((fd == listenfd) &amp;&amp;(events[i].events &amp; EPOLLIN)) handle_accpet(epollfd,listenfd); else if (events[i].events &amp; EPOLLIN) do_read(epollfd,fd,buf); else if (events[i].events &amp; EPOLLOUT) do_write(epollfd,fd,buf); &#125;&#125;//添加事件static void add_event(int epollfd,int fd,int state)&#123; struct epoll_event ev; ev.events = state; ev.data.fd = fd; epoll_ctl(epollfd,EPOLL_CTL_ADD,fd,&amp;ev);&#125;//处理接收到的连接static void handle_accpet(int epollfd,int listenfd)&#123; int clifd; struct sockaddr_in cliaddr; socklen_t cliaddrlen; clifd = accept(listenfd,(struct sockaddr*)&amp;cliaddr,&amp;cliaddrlen); if (clifd == -1) perror(&quot;accpet error:&quot;); else &#123; printf(&quot;accept a new client: %s:%d\n&quot;,inet_ntoa(cliaddr.sin_addr),cliaddr.sin_port); //添加一个客户描述符和事件 add_event(epollfd,clifd,EPOLLIN); &#125; &#125;//读处理static void do_read(int epollfd,int fd,char *buf)&#123; int nread; nread = read(fd,buf,MAXSIZE); if (nread == -1) &#123; perror(&quot;read error:&quot;); close(fd); //记住close fd delete_event(epollfd,fd,EPOLLIN); //删除监听 &#125; else if (nread == 0) &#123; fprintf(stderr,&quot;client close.\n&quot;); close(fd); //记住close fd delete_event(epollfd,fd,EPOLLIN); //删除监听 &#125; else &#123; printf(&quot;read message is : %s&quot;,buf); //修改描述符对应的事件，由读改为写 modify_event(epollfd,fd,EPOLLOUT); &#125; &#125;//写处理static void do_write(int epollfd,int fd,char *buf) &#123; int nwrite; nwrite = write(fd,buf,strlen(buf)); if (nwrite == -1)&#123; perror(&quot;write error:&quot;); close(fd); //记住close fd delete_event(epollfd,fd,EPOLLOUT); //删除监听 &#125;else&#123; modify_event(epollfd,fd,EPOLLIN); &#125; memset(buf,0,MAXSIZE); &#125;//删除事件static void delete_event(int epollfd,int fd,int state) &#123; struct epoll_event ev; ev.events = state; ev.data.fd = fd; epoll_ctl(epollfd,EPOLL_CTL_DEL,fd,&amp;ev);&#125;//修改事件static void modify_event(int epollfd,int fd,int state)&#123; struct epoll_event ev; ev.events = state; ev.data.fd = fd; epoll_ctl(epollfd,EPOLL_CTL_MOD,fd,&amp;ev);&#125;//注：另外一端我就省了 epoll总结在 select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而epoll事先通过epoll_ctl()来注册一 个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait() 时便得到通知。(此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是epoll的魅力所在。) epoll的优点主要是一下几个方面： 监视的描述符数量不受限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左 右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。select的最大缺点就是进程打开的fd是有数量限制的。这对 于连接数量比较大的服务器来说根本不能满足。虽然也可以选择多进程的解决方案( Apache就是这样实现的)，不过虽然linux上面创建进程的代价比较小，但仍旧是不可忽视的，加上进程间数据同步远比不上线程间同步的高效，所以也不是一种完美的方案。 IO的效率不会随着监视fd的数量的增长而下降。epoll不同于select和poll轮询的方式，而是通过每个fd定义的回调函数来实现的。只有就绪的fd才会执行回调函数。 如果没有大量的idle -connection或者dead-connection，epoll的效率并不会比select/poll高很多，但是当遇到大量的idle- connection，就会发现epoll的效率大大高于select/poll。 参考用户空间与内核空间，进程上下文与中断上下文进程切换维基百科-文件描述符Linux 中直接 I/O 机制的介绍IO - 同步，异步，阻塞，非阻塞（亡羊补牢篇）Linux中select poll和epoll的区别IO多路复用之select总结IO多路复用之poll总结IO多路复用之epoll总结]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[常用哈希函数]]></title>
      <url>%2F2017%2F04%2F22%2Fhash-functions%2F</url>
      <content type="text"><![CDATA[SDBM Hash123456789101112unsigned int SDBMHash(char *str)&#123; unsigned int hash = 0; while (*str) &#123; // equivalent to: hash = 65599*hash + (*str++); hash = (*str++) + (hash &lt;&lt; 6) + (hash &lt;&lt; 16) - hash; &#125; return (hash &amp; 0x7FFFFFFF);&#125; RS Hash1234567891011121314unsigned int RSHash(char *str)&#123; unsigned int b = 378551; unsigned int a = 63689; unsigned int hash = 0; while (*str) &#123; hash = hash * a + (*str++); a *= b; &#125; return (hash &amp; 0x7FFFFFFF);&#125; JS Hash1234567891011unsigned int JSHash(char *str)&#123; unsigned int hash = 1315423911; while (*str) &#123; hash ^= ((hash &lt;&lt; 5) + (*str++) + (hash &gt;&gt; 2)); &#125; return (hash &amp; 0x7FFFFFFF);&#125; P. J. Weinberger Hash123456789101112131415161718192021unsigned int PJWHash(char *str)&#123; unsigned int BitsInUnignedInt = (unsigned int)(sizeof(unsigned int) * 8); unsigned int ThreeQuarters = (unsigned int)((BitsInUnignedInt * 3) / 4); unsigned int OneEighth = (unsigned int)(BitsInUnignedInt / 8); unsigned int HighBits = (unsigned int)(0xFFFFFFFF) &lt;&lt; (BitsInUnignedInt - OneEighth); unsigned int hash = 0; unsigned int test = 0; while (*str) &#123; hash = (hash &lt;&lt; OneEighth) + (*str++); if ((test = hash &amp; HighBits) != 0) &#123; hash = ((hash ^ (test &gt;&gt; ThreeQuarters)) &amp; (~HighBits)); &#125; &#125; return (hash &amp; 0x7FFFFFFF);&#125; ELF Hash1234567891011121314151617unsigned int ELFHash(char *str)&#123; unsigned int hash = 0; unsigned int x = 0; while (*str) &#123; hash = (hash &lt;&lt; 4) + (*str++); if ((x = hash &amp; 0xF0000000L) != 0) &#123; hash ^= (x &gt;&gt; 24); hash &amp;= ~x; &#125; &#125; return (hash &amp; 0x7FFFFFFF);&#125; BKDR Hash123456789101112unsigned int BKDRHash(char *str)&#123; unsigned int seed = 131; // 31 131 1313 13131 131313 etc.. unsigned int hash = 0; while (*str) &#123; hash = hash * seed + (*str++); &#125; return (hash &amp; 0x7FFFFFFF);&#125; DJB Hash1234567891011unsigned int DJBHash(char *str)&#123; unsigned int hash = 5381; while (*str) &#123; hash += (hash &lt;&lt; 5) + (*str++); &#125; return (hash &amp; 0x7FFFFFFF);&#125; AP Hash12345678910111213141516171819unsigned int APHash(char *str)&#123; unsigned int hash = 0; int i; for (i=0; *str; i++) &#123; if ((i &amp; 1) == 0) &#123; hash ^= ((hash &lt;&lt; 7) ^ (*str++) ^ (hash &gt;&gt; 3)); &#125; else &#123; hash ^= (~((hash &lt;&lt; 11) ^ (*str++) ^ (hash &gt;&gt; 5))); &#125; &#125; return (hash &amp; 0x7FFFFFFF);&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 108 [Convert Sorted Array to Binary Search Tree]]]></title>
      <url>%2F2017%2F04%2F01%2Fleetcode-108%2F</url>
      <content type="text"><![CDATA[题目链接 Convert Sorted Array to Binary Search Tree 题目描述 给定一个由小到大排好序的数组，要求将每个数组的值作为一个结点的值，将结点构建成一棵平衡的二叉排序树。 解题思路 如果二叉树是平衡的，则它的任意一个结点的左子树和右子树的高度差不超过1 数组是由小到大有序的，可以考虑用二分法找到中间元素作为根结点的值，再把根结点的左右两边看成两个子数组，左子数组的元素都比根结点的值小，右子数组的元素都比根结点的值大。若原数组的元素个数是奇数，那么根结点的值刚好是位于中间的元素，左右子数组的元素个数相同；若原数组的元素个数是偶数，那么根结点的值则是中间偏左的元素或中间偏右的元素，左右子数组的元素个数差值为1。继续对左右子数组进行递归处理，直到不能再划分 用这种方法，最后一层得到树必然是平衡的，根结点的父结点对应的树也是平衡的，进而可以推断出最终构建出来的二叉树是二叉平衡树。另外，在每次构建过程中都可以保证左子树的所有结点的值都比根结点小，右子树的所有结点的值都比根结点大，因此最终构建出来的二叉树是二叉排序树 若原数组元素个数为n，则处理过程的时间复杂度为O(log n) C++代码12345678910111213141516171819202122232425262728/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: TreeNode* sortedArrayToBST(vector&lt;int&gt;&amp; nums) &#123; return convert(nums, 0, nums.size() - 1); &#125; TreeNode* convert(vector&lt;int&gt;&amp; nums, int begin, int end) &#123; if (begin &gt; end) &#123; return NULL; &#125; if (begin == end) &#123; return new TreeNode(nums[begin]); &#125; int mid = ((begin + end) &gt;&gt; 1); TreeNode* node = new TreeNode(nums[mid]); node-&gt;left = convert(nums, begin, mid - 1); node-&gt;right = convert(nums, mid + 1, end); return node; &#125;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 117 [Populating Next Right Pointers in Each Node II]]]></title>
      <url>%2F2017%2F04%2F01%2Fleetcode-117%2F</url>
      <content type="text"><![CDATA[题目链接 Populating Next Right Pointers in Each Node II 题目描述 给定一颗二叉树，要求将每个结点的next指针指向它右侧的结点，若它右侧没有结点，则指向NULL，处理过程只能使用常量复杂度的空间。 解题思路 本题与题目116不一样的地方在于给定的二叉树是普通的，除了二叉树的基本特征之外，没有其他可以利用的特点 若不限制使用的空间的复杂度，我们就可以通过遍历，把每一层最近一次被访问的结点记录下来，每次访问到同一层的下一个结点，就把记录的同层的结点的next指针指向当前结点，再把当前结点作为本层最近一次被访问的结点记录下来，这样遍历完之后next指针也就构建完毕了 对于限制空间复杂度的条件，我们只能通过时间换空间的方式来解决。解决的方式是每一次遍历，目标仅针对特定某一层的结点，具体步骤如下： 假如根结点深度为0，叶结点最深的深度为h，在进行每一轮遍历之前，将辅助记录本层上一次访问的结点的指针置为NULL 进行一轮遍历时，从根结点开始，若当前结点的深度与本次遍历的目标层的深度一致，则对应更新next指针与辅助指针，对于深度超过目标的结点不需要遍历 每一轮遍历结束之后，若辅助指针不为NULL，说明需要继续下一轮遍历 经过h轮遍历之后，next指针即构建完毕 假设二叉树高度为h，则处理过程对树遍历了h轮，设 0 &lt; i &lt;= h，则第i轮最多遍历 sum(2^0 + 2^1 + … + 2^(i-1)) 个结点，第i轮遍历的时间复杂度为O(2^i)，因此最终时间复杂度为O(h * 2^h) C++代码1234567891011121314151617181920212223242526272829303132333435/** * Definition for binary tree with next pointer. * struct TreeLinkNode &#123; * int val; * TreeLinkNode *left, *right, *next; * TreeLinkNode(int x) : val(x), left(NULL), right(NULL), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: void travel(TreeLinkNode* node, int height, TreeLinkNode** lastNode, int level) &#123; if (!node) &#123; return; &#125; if (height == level) &#123; if (*lastNode) &#123; (*lastNode)-&gt;next = node; &#125; *lastNode = node; return; &#125; travel(node-&gt;left, height + 1, lastNode, level); travel(node-&gt;right, height + 1, lastNode, level); &#125; void connect(TreeLinkNode *root) &#123; TreeLinkNode* lastNode = NULL; int level = 0; do &#123; lastNode = NULL; travel(root, 0, &amp;lastNode, level); level++; &#125; while (lastNode); &#125;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 116 [Populating Next Right Pointers in Each Node]]]></title>
      <url>%2F2017%2F04%2F01%2Fleetcode-116%2F</url>
      <content type="text"><![CDATA[题目链接 Populating Next Right Pointers in Each Node 题目描述 给定一颗完美二叉树，要求将每个结点的next指针指向它右侧的结点，若它右侧没有结点，则指向NULL，处理过程只能使用常量复杂度的空间。 解题思路 对于任意两个在完美二叉树上左右相邻的结点，它们必定满足以下两个规则之一： 它们有同一个父结点，即任意一个结点的左右子结点必然是相邻的 两个结点的父结点相邻，并且位于左侧的结点是它的父结点的右子结点，位于右侧的结点是它的父结点的左子结点 既然确定了左右相邻的结点的特征，那么只需要从根结点开始遍历树的所有结点并对应处理即可，处理过程如下： 对于每个结点，若左子结点存在，那么右子结点也必然存在，则将左子结点的next指针指向右子结点 对于每个结点，若它的左子结点也存在右子结点，那么它的右子结点也存在左子结点，则将它的左子结点的右子结点的next指针，指向它的右子结点的左子结点 处理过程中，每个结点都被遍历了一次，因此时间复杂度是O(n) C++代码1234567891011121314151617181920/** * Definition for binary tree with next pointer. * struct TreeLinkNode &#123; * int val; * TreeLinkNode *left, *right, *next; * TreeLinkNode(int x) : val(x), left(NULL), right(NULL), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: void connect(TreeLinkNode *root) &#123; if (!root) return; if (root-&gt;next &amp;&amp; root-&gt;right) root-&gt;right-&gt;next = root-&gt;next-&gt;left; if (root-&gt;left) &#123; root-&gt;left-&gt;next = root-&gt;right; connect(root-&gt;left); connect(root-&gt;right); &#125; &#125;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 95 [Unique Binary Search Trees II]]]></title>
      <url>%2F2017%2F04%2F01%2Fleetcode-95%2F</url>
      <content type="text"><![CDATA[题目链接 Unique Binary Search Trees II 题目描述 给定一个整数，要求返回所有由值为1-n的结点组成的二叉排序树。 解题思路 首先需要考虑n的取值，若取值不大于0，则不会形成任何二叉树 对于n&gt;0的情况，可以用分治的思想来处理，每个结点轮流作为根结点，那么它的左子树就是由所有比该结点的值小的其他结点构成的，它的右子树就是由所有比该结点大的值构成的 对于左子树与右子树，又可以轮流以某个结点作为根结点，一直到无法再分为止 由于左子树可能有a种形式，右子树也可能有b种形式，所以以某个结点为根的树的集合其实需要对左右子树的情况进行一一匹配，最终可以得出a*b种形式 需要特别注意一种情况，子树为空或者仅有一个结点，这两种情况实质都包含了一种形式，需要同等考虑 每次以某个结点为根进行计算，平均时间复杂度为O(log n)，对n个结点进行计算，时间复杂度即为O(n * log n) C++代码123456789101112131415161718192021222324252627282930313233343536373839/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: vector&lt;TreeNode*&gt; generateTrees(int n) &#123; vector&lt;TreeNode*&gt; result; if (n &lt; 1) &#123; return result; &#125; return generateTreesOfRange(1, n); &#125; vector&lt;TreeNode*&gt; generateTreesOfRange(int beg, int end) &#123; vector&lt;TreeNode*&gt; result; if (beg &gt; end) &#123; result.push_back(NULL); return result; &#125; for (int i = beg; i &lt;= end; ++i) &#123; vector&lt;TreeNode*&gt; leftTrees = generateTreesOfRange(beg, i - 1); vector&lt;TreeNode*&gt; rightTrees = generateTreesOfRange(i + 1, end); for (int j = 0; j &lt; leftTrees.size(); ++j) &#123; for (int k = 0; k &lt; rightTrees.size(); ++k) &#123; TreeNode* root = new TreeNode(i); root-&gt;left = leftTrees[j]; root-&gt;right = rightTrees[k]; result.push_back(root); &#125; &#125; &#125; return result; &#125;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 113 [Path Sum II]]]></title>
      <url>%2F2017%2F03%2F29%2Fleetcode-113%2F</url>
      <content type="text"><![CDATA[题目链接 Path Sum II 题目描述 给定一棵二叉树与一个整数值，要求查找出所有从根结点到叶子结点的路径，这些路径上的所有结点的和等于给定的值。 解题思路 题目默认条件 不需要考虑子路径结点的和可能溢出的情况 本题主要关注点在于三个方面 如何判断某个结点是不是叶子结点 如果该结点没有左子结点与右子结点，则该结点是叶子结点，根结点也可能是叶子结点 若采用递归的方式，如何减少内存空间的使用 存放临时路径的vector，在整个遍历过程都使用引用来传递该临时路径，每个结点处理之后需要清理对该路径的变更 存放结果的vector，在整个遍历过程中同样使用引用来传递 如何判断某条路径满足条件 在遍历过程中，用一个变量实时记录临时路径的值，若某个结点是叶子结点，并且该变量的值与给定值相等，则当前的临时路径满足条件，可加入存放结果的vector中 C++代码12345678910111213141516171819202122232425262728293031323334353637/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; pathSum(TreeNode* root, int sum) &#123; vector&lt;vector&lt;int&gt;&gt; result; vector&lt;int&gt; subPath; if (root) &#123; calculatePathSum(root, sum, subPath, 0, result); &#125; return result; &#125; void calculatePathSum(TreeNode* node, int sum, vector&lt;int&gt;&amp; subPath, int subSum, vector&lt;vector&lt;int&gt;&gt;&amp; result) &#123; subPath.push_back(node-&gt;val); subSum += node-&gt;val; if (!node-&gt;left &amp;&amp; !node-&gt;right) &#123; if (sum == subSum) &#123; result.push_back(subPath); &#125; &#125; if (node-&gt;left) &#123; calculatePathSum(node-&gt;left, sum, subPath, subSum, result); &#125; if (node-&gt;right) &#123; calculatePathSum(node-&gt;right, sum, subPath, subSum, result); &#125; subSum -= node-&gt;val; subPath.pop_back(); &#125;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 538 [Convert BST to Greater Tree]]]></title>
      <url>%2F2017%2F03%2F29%2Fleetcode-538%2F</url>
      <content type="text"><![CDATA[题目链接 Convert BST to Greater Tree 题目描述 给定一颗二叉排序树，对于每个结点，要求计算出所有比它的值大的结点的值的和，并且将这个计算出来的和加到该结点上面。 解题思路 首先需要确定如何找到所有比某个结点的值大的其他所有结点。在二叉排序树中，父结点的值总是大于左子树所有结点的值，且小于右子树所有结点的值。所以比某个结点的值大的所有结点，也就是在先序遍历中位于它之后的所有结点。 本题的要求是将这些结点的值的和加到该结点上面，因此我们不需要逐个查找结点，再逐个加上去，整体处理就可以了。对于每个结点，我们需要记录父结点传下来的增量（简述为“父增量”），它自身变更前的值（简述为“原值”），左子树所有结点变更之前的值的和（简述为“左侧总和”），右子树所有结点变更之前的值的和（简述为“右侧总和”），然后将“原值+右侧总和+父增量”作为增量加到左子树的每个结点上面，将“父增量”作为增量加到右子树的每个结点上，再将“右侧总和+父增量”作为增量加到自身上面。处理左右结点的流程与其父结点是一致的，递归处理即可。 变更结点的值以及计算当前的树的所有结点的值的总和，这两个流程可以在一次遍历中完成，通过不同的变量来记录变更前后的值即可。 C++代码12345678910111213141516171819202122232425262728293031/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: TreeNode* convertBST(TreeNode* root) &#123; int rootOriginSum, rootNewSum; convertNode(root, rootOriginSum, 0, rootNewSum); return root; &#125; void convertNode(TreeNode* node, int&amp; originSum, int delta, int&amp; newSum) &#123; if (!node) &#123; originSum = 0; newSum = 0; return; &#125; int rightOriginSum, rightNewSum; convertNode(node-&gt;right, rightOriginSum, delta, rightNewSum); int leftOriginSum, leftNewSum; convertNode(node-&gt;left, leftOriginSum, delta + node-&gt;val + rightOriginSum, leftNewSum); originSum = node-&gt;val + leftOriginSum + rightOriginSum; node-&gt;val += delta + rightOriginSum; newSum = node-&gt;val + leftNewSum + rightNewSum; &#125;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 222 [Count Complete Tree Nodes]]]></title>
      <url>%2F2017%2F03%2F20%2Fleetcode-222%2F</url>
      <content type="text"><![CDATA[题目链接 Count Complete Tree Nodes 题目描述 给定一棵完全二叉树，要求计算它的总结点数。完全二叉树指的是除了最后一层之外，其他每一层都是完整的，最后一层可能不是完整的，但是这一层的结点全部聚集在左侧，因此完全二叉树的结点数落在(1，2^h)这个区间内。 解题思路 一开始我使用最简单的方式来处理，完全二叉树是特殊的二叉树，只要把它的结点全部遍历一次就可以了。这种方法可以得到结果，但是时间复杂度为O(n)，直接超时了 接着我改进了计算方式，从最右侧开始遍历，当发现最后一层某个位置有结点的时候就停止。这种方法在平均情况下只需要遍历一半的结点，但是时间复杂度还是O(n)，还是超时 时间复杂度为O(n)行不通，只能考虑O(log n)的算法，而O(log n)的时间复杂度一般对应着二分法。对于这个题目，可以通过二分法查找最后一层的最后一个结点的位置，进而计算出总结点数，具体的算法如下： 首先计算树的高度，由根结点一直往左偏移，计算偏移次数即可，这一步的时间复杂度为O(log n) 除了树为空或者仅有根结点的情况，完全二叉树的倒数第二层肯定是完整的，因此可以通过倒数第二层的结点来判断最后一层的结点的分布情况 从根结点开始处理，以当前结点为中间线，将以当前结点为根的二叉树分成两部分，取倒数第二层中，左半部分最靠近中间线的结点，即之前描述的二分的方式。这个处于“二分位置偏左”的结点，称之为“二分结点”，必然是当前结点的左子树（不考虑原树最后一层）的最右边的结点 如果“二分结点”有右子结点，说明上述的左半部分是完整的，继续考虑右半部分，将当前结点向它的右结点偏移 如果“二分结点”没有右子结点，说明上述的左半部分不是完整的，右半部分最后一层的结点全部缺失，记录对应的缺失结点数之后，将当前结点向它的左结点偏移 对新的当前结点进行处理，直到当前结点位于倒数第二层，这种情况下以当前结点为根的树（不考虑原树最后一层）仅有一个结点，无法拆分为左右两部分，因此直接根据它的左右子结点是否为空，对应记录缺失的结点数即可 倒数第二层处理完之后，处理流程结束，计算相同高度的满二叉树的结点数，用它减去缺失的结点数，即为所求的完全二叉树的结点数 C++代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */#include &lt;math&gt;class Solution &#123;public: int countNodes(TreeNode* root) &#123; // 计算树的高度 int height = 0; TreeNode* tmp = root; while (tmp) &#123; height++; tmp = tmp-&gt;left; &#125; // 记录缺少的结点数 int numRightEmpty = 0; TreeNode* node = root; TreeNode* curNode; int level = 1; int curLevel; // 从根结点开始遍历，直到当前结点为空，或者已遍历到最后一层 while (node &amp;&amp; level &lt; height) &#123; // 如果当前结点位于倒数第二层，则处理完它的子结点的情况之后即可结束 if (level == height - 1) &#123; numRightEmpty += node-&gt;right ? 0 : node-&gt;left ? 1 : 2; break; &#125; // 以当前结点位置为中间线，查找倒数第二层里面，中间线偏左的最后一个元素 // 查找方法为当前结点向左下方偏移，再一直向右下方偏移，直到倒数第二层为止 curNode = node-&gt;left; curLevel = level + 1; while (curLevel &lt; height - 1) &#123; curNode = curNode-&gt;right; curLevel++; &#125; // 如果中间线偏左的最后一个元素有右子结点，则需要将当前结点重置为它的右子结点 // 如果中间线偏左的最后一个元素没有右子结点，那么说明当前结点缺少了右半边的叶子结点，对应处理之后，将当前结点重置为它的左子结点 if (curNode-&gt;right) &#123; node = node-&gt;right; &#125; else &#123; numRightEmpty += (int)pow(2.0, (double)(height - level - 1)); node = node-&gt;left; &#125; level++; &#125; // 总的结点数为（2^h - 1 - 缺少的结点数） return (int)pow(2.0, (double)(height)) - 1 - numRightEmpty; &#125;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 98 [Validate Binary Search Tree]]]></title>
      <url>%2F2017%2F03%2F19%2Fleetcode-98%2F</url>
      <content type="text"><![CDATA[题目链接 Validate Binary Search Tree 题目描述 判断给定的二叉树是不是二叉排序树，假定二叉排序树满足以下条件： 任意一个结点的左子树的所有结点的值都比该结点小 任意一个结点的右子树的所有结点的值都比该结点大 左子树和右子树都是二叉排序树（递归定义） 解题思路 根据二叉排序树的定义，从根结点出发，验证以每一个结点为根的树是否满足条件 对每一个被遍历的结点，若左子结点不为空，则查找左子树的所有结点的最大值，若该值不小于当前结点的值，则说明当前树不是二叉排序树，否则继续查找右子树的所有结点的最小值，若该值不大于当前结点的值，则同样说明当前树不是二叉排序树 实际代码实现使用递归的方式，最大值被初始化为整型的最小值，最小值被初始化为整型的最大值，由上层调用时传入 C++代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: bool isValidBST(TreeNode* root) &#123; if (!root) &#123; return true; &#125; int max = INT_MIN; int min = INT_MAX; return travel(root, max, min); &#125; bool travel(TreeNode* node, int&amp; max, int&amp; min) &#123; max = node-&gt;val &gt; max ? node-&gt;val : max; min = node-&gt;val &lt; min ? node-&gt;val : min; int tmpmax, tmpmin; if (node-&gt;left) &#123; tmpmax = INT_MIN; tmpmin = INT_MAX; if (!travel(node-&gt;left, tmpmax, tmpmin)) &#123; return false; &#125; else if (tmpmax &gt;= node-&gt;val) &#123; return false; &#125; max = tmpmax &gt; max ? tmpmax : max; min = tmpmin &lt; min ? tmpmin : min; &#125; if (node-&gt;right) &#123; tmpmax = INT_MIN; tmpmin = INT_MAX; if (!travel(node-&gt;right, tmpmax, tmpmin)) &#123; return false; &#125; else if (tmpmin &lt;= node-&gt;val) &#123; return false; &#125; max = tmpmax &gt; max ? tmpmax : max; min = tmpmin &lt; min ? tmpmin : min; &#125; return true; &#125;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 129 [Sum Root to Leaf Numbers]]]></title>
      <url>%2F2017%2F03%2F19%2Fleetcode-129%2F</url>
      <content type="text"><![CDATA[题目链接 Sum Root to Leaf Numbers 题目描述 给定一棵二叉树，它的结点的值都在0-9之间，每一条从根结点到叶子结点的路径都可以代表一个数字。比如如果某条路径是1-&gt;2-&gt;3，那么它代表的数字则是123。要求找出给定的二叉树的所有上述路径，返回这些路径代表的数字之和。 解题思路 根据题目中对路径的描述，我们可以知道每条路径代表的数字中，根结点的数字位于最左边，叶子结点的数字位于最右边，每个结点的数字都位于它的子结点左边以及它的父结点右边，也就是说每个结点的进位都比它的子结点多一位，比它的父结点少一位 本题的解法： 从根结点的开始遍历 遍历结点的时候，将之前的数字进一位（乘以10），再加上当前结点的数字，就是从根结点到当前结点为止的子路径代表的数字 若当前遍历的结点是叶子结点（没有任何子结点），那么当前子路径即是所求的路径之一，将上一步计算得到的数字加到总和之中 若当前遍历的结点不是叶子结点，则继续遍历它的子结点，将之前计算的数字传递下去 遍历完所有结点之后，所求的总和也就计算完成了 这个解法对每个结点都访问了一次，因此时间复杂度为O(n)，n为二叉树的结点总数 C++代码1234567891011121314151617181920212223242526272829303132333435363738/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */#include &lt;stack&gt;class Solution &#123;public: int kthSmallest(TreeNode* root, int k) &#123; int num = 0; stack&lt;TreeNode*&gt; s; s.push(root); TreeNode* node = root; while (node || !s.empty()) &#123; if (node &amp;&amp; node-&gt;left) &#123; s.push(node-&gt;left); node = node-&gt;left; continue; &#125; node = s.top(); if (++num == k) &#123; return node-&gt;val; &#125; s.pop(); if (node-&gt;right) &#123; s.push(node-&gt;right); &#125; node = node-&gt;right; &#125; return root-&gt;val; &#125;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 230 [Kth Smallest Element in a BST]]]></title>
      <url>%2F2017%2F03%2F19%2Fleetcode-230%2F</url>
      <content type="text"><![CDATA[题目链接 Kth Smallest Element in a BST 题目描述 给定一棵二叉排序树，要求写一个用于查找第K小的元素的函数，1&lt;=k&lt;=总结点数。 解题思路 二叉排序树是一种特殊的二叉树，它的特点是以任意一个结点为根结点的树，它的左子树的所有结点的值都不大于该结点的值，它的右子树的所有结点的值都不小于该结点的值 二叉树的先序遍历，即先访问左子结点，再访问本身，最后访问右子结点，对应到二叉排序树，得出的访问顺序是严格非递减的。因此题目要求的第K小的元素，即对二叉排序树进行先序遍历得到的第K个结点的值 二叉树遍历一般有递归和迭代两种方法，递归相对比较简单，我采用迭代的方法实现 题目条件明确说明K&gt;=1，因此不需要考虑根结点的空的情况 C++代码1234567891011121314151617181920212223242526272829303132333435363738/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */#include &lt;stack&gt;class Solution &#123;public: int kthSmallest(TreeNode* root, int k) &#123; int num = 0; stack&lt;TreeNode*&gt; s; s.push(root); TreeNode* node = root; while (node || !s.empty()) &#123; if (node &amp;&amp; node-&gt;left) &#123; s.push(node-&gt;left); node = node-&gt;left; continue; &#125; node = s.top(); if (++num == k) &#123; return node-&gt;val; &#125; s.pop(); if (node-&gt;right) &#123; s.push(node-&gt;right); &#125; node = node-&gt;right; &#125; return root-&gt;val; &#125;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 337 [House Robber III]]]></title>
      <url>%2F2017%2F03%2F16%2Fleetcode-337%2F</url>
      <content type="text"><![CDATA[题目链接 House Robber III 题目描述 小偷进入一片区域准备再一次行窃，这片区域仅有一个入口，称为root。在root入口旁边，每个房屋有且仅有一个“父房屋”。聪明的小偷发现这片区域内所有的房屋形成了一棵二叉树。另外，如果同一个晚上，两间直接相连的房屋被闯入的话，自动报警器就会启动。现已知每个房屋里面的金额，要求计算出小偷在不惊动警察的情况下，今晚最多可以偷到多少钱。 解题思路 抽象一下题目要求，其实本题要计算的是二叉树某些结点的和的最大值，这些结点需要满足的条件是两两不直接相连。 简单分析可以得出，对于任意一个二叉树结点，可以分两种情况考虑： 它是被选中的结点，那么它的左右子结点都不是被选中的结点 它不是被选中的结点，这种场景可再细分： 左子结点被选中，右子结点也被选中 左子结点被选中，右子结点不被选中 左子结点不被选中，右子结点被选中 左子结点不被选中，右子结点也不被选中 左右子结点的情况，可以递归处理 从根结点出发，对于每个结点根据上述分析进行遍历，即可得出结果。理论上，每个结点有两种状态（被选中与不被选中），若结点数为n，那么时间复杂度为O(2^n)，但是对于父结点被选中的情况，子结点很多场景其实不需要考虑（称为剪枝），最终时间复杂度远不及O(2^n)。另外需要考虑结点值为负的情况，每个结点在遍历时，应该取0作为初始值。 C++代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: int rob(TreeNode* root) &#123; int max_sum = 0; // 根结点为空，直接返回0 if (!root) &#123; return max_sum; &#125; // 根结点被选中的情况 int tmp = travel(root, true) + root-&gt;val; if (tmp &gt; max_sum) &#123; max_sum = tmp; &#125; // 根结点未被选中的情况 tmp = travel(root, false); if (tmp &gt; max_sum) &#123; max_sum = tmp; &#125; return max_sum; &#125; // 遍历某个结点，获取它的左右子结点各种情况的和的最大值 int travel(TreeNode* node, bool cur_state) &#123; int tmp; // 计算左结点各种情况的最大值 int max_left = 0; if (node-&gt;left) &#123; // 不选中左子结点的情况 tmp = travel(node-&gt;left, false); if (tmp &gt; max_left) &#123; max_left = tmp; &#125; // 若当前结点未被选中，则考虑选中左子结点的情况 if (!cur_state) &#123; tmp = travel(node-&gt;left, true) + node-&gt;left-&gt;val; if (tmp &gt; max_left) &#123; max_left = tmp; &#125; &#125; &#125; // 计算右结点各种情况的最大值 int max_right = 0; if (node-&gt;right) &#123; // 不选中右子结点的情况 tmp = travel(node-&gt;right, false); if (tmp &gt; max_right) &#123; max_right = tmp; &#125; // 若当前结点未被选中，则考虑选中右子结点的情况 if (!cur_state) &#123; tmp = travel(node-&gt;right, true) + node-&gt;right-&gt;val; if (tmp &gt; max_right) &#123; max_right = tmp; &#125; &#125; &#125; // 返回左右子结点最大值之和 return max_left + max_right; &#125;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 236 [Lowest Common Ancestor of a Binary Tree]]]></title>
      <url>%2F2017%2F03%2F13%2Fleetcode-236%2F</url>
      <content type="text"><![CDATA[题目链接 Lowest Common Ancestor of a Binary Tree 题目描述 给定一颗二叉树，查找两个结点的最近公共祖先。最近公共祖先的定义：二叉树里面同时拥有两个给定后代结点的高度最低的结点，一个结点可以作为它自己的祖先结点。 解题思路 一开始的思路 只要根结点不为NULL，那么它肯定是任意两个结点的公共祖先。从根结点出发到达两个给定结点，可以得到两条路径（类似链表），这两条路径的起始结点都是根结点，沿着路径逐个对比，它们最后一个公共结点就是待查找的最近公共祖先。实际处理的时候，不需要以链表的方式生成一条路径，我的想法是使用包含bool元素的vector记录每次向左还是向右走，这样也可以表示路径信息。 这种方法的问题在于需要使用额外的存储来记录路径信息，在计算资源足够的情况下，这种方式可以得到结果，但本题的一个测试用例包含约10000个结点，测试时直接报”Runtime Error”，因此这种思路不能作为本题的正确解法。 参考别人的思路 在Solutions里看到别人发表的非常简洁的递归解法，只有4行，真的非常精妙，并且不需要额外的变量存储空间。 这种解法的思路如下： 把根结点作为当前结点，查找以当前结点为根的树。查找结果可以分为四种： 两个给定结点都在当前的树里 只有第一个结点在当前的树里 只有第二个结点在当前的树里 两个给定结点都不在当前的树里 如果当前结点为NULL，则表示查找结果为上述第四种，返回NULL；如果当前结点是给定的第一个结点，则表示查找结果为上述第二种，返回当前结点；如果当前结点是给定的第二个结点，则表示查找结果为上述第三种，返回当前结点；如果前面几种情况都不满足，那么就需要查找它的左右子树了，左右子树的查找规则跟本步骤一模一样，查找结果同样可以分为四种： 在左子树中找到了一个给定结点，左子树查找结果不为NULL；在右子树中也找到了一个给定结点，右子树查找结果不为NULL。这种情况说明最近公共祖先就是当前结点（左右子树的父结点），返回当前结点 在左子树中找到了一个或两个给定结点，左子树查找结果不为NULL；在右子树中没有找到给定结点，右子树查找结果为NULL。这种情况说明最近公共祖先必然在当前结点上方，返回左子树查找结果 在右子树中找到了一个或两个给定结点，右子树查找结果不为NULL；在左子树中没有找到给定结点，左子树查找结果为NULL。这种情况说明最近公共祖先必然在当前结点上方，返回右子树查找结果 在左右子树中都没有找到给定结点，查找结果都为NULL。这种情况说明当前的树不包含任何指定结点，返回NULL 上一步骤的四种查找结果中，第2和第3种都需要返回子树的查找结果，而不是返回当前结点的处理结果，否则会导致最终找到的都是根结点 C++代码123456789101112131415161718/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) &#123; if (!root || root == p || root == q) return root; TreeNode* left = lowestCommonAncestor(root-&gt;left, p, q); TreeNode* right = lowestCommonAncestor(root-&gt;right, p, q); return left &amp;&amp; right ? root : left ? left : right ? right : NULL; &#125;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 450 [Delete Node in a BST]]]></title>
      <url>%2F2017%2F03%2F11%2Fleetcode-450%2F</url>
      <content type="text"><![CDATA[题目链接 Delete Node in a BST 题目描述 给定一颗二叉排序树的根结点以及一个值，要求删除二叉排序树中包含该值的结点，同时返回该树的最新的根结点。一般来说，删除可以通过两个步骤来完成，一是查找该结点，二是删除该结点（如果存在的话）。时间复杂度必须是O(h)，h表示树的高度。 解题思路 题目默认条件 若非特殊说明，二叉排序树不存在两个或多个结点包含同个值的情况。 查找“待删除结点”的步骤 从根结点开始查找，将当前结点的值与给定的值进行比较。 如果当前结点的值比较大，则将当前结点的左结点设置为新的当前结点；如果当前结点的值比较小，则将当前结点的右结点设置为新的当前结点；如果值相等，则说明当前结点即为目标，结束查找过程。 若当前结点为NULL，也结束查找过程。 查找过程需要记录“待删除结点”的父结点，同时还要记录它是父结点的左孩子还是右孩子（下文使用“方向”来表示这个信息）。 删除“待删除结点”的步骤 若第一步查找不到“待删除结点”，则不需要执行删除操作，返回当前的根结点即可。 若找到了要“待删除结点”，则需要分以下几种情况考虑 情况1：“待删除结点”没有任何子结点 如果“待删除结点”是根结点（即它的父结点为NULL），那么释放该结点，返回NULL，结束删除过程 如果“待删除结点”不是根结点，那么释放该结点，同时将它的父结点的对应方向的子结点置为NULL，结束删除过程 情况2：“待删除结点”只有左子结点 如果“待删除结点”是根结点（即它的父结点为NULL），那么释放该结点，返回它的左子结点，结束删除过程 如果“待删除结点”不是根结点，那么释放该结点，同时将它的父结点的对应方向的子结点置为它的左结点，结束删除过程 情况3：待删除的结点只有右子结点（与情况2非常类似） 如果“待删除结点”是根结点（即它的父结点为NULL），那么释放该结点，返回它的右子结点，结束删除过程 如果“待删除结点”不是根结点，那么释放该结点，同时将它的父结点的对应方向的子结点置为它的右结点，结束删除过程 情况4：“待删除结点”同时包含左右子结点 这种情况下，我们的目标是找一个最合适的结点（下文使用“待交换结点”来表示它）来取代“待删除结点”的位置。二叉排序树的特性就是每一个结点都比它左子树的所有结点大，同时比它右子树的所有结点小。因此“待交换结点”要么是左子树中值最大的结点，要么是右子树中值最小的结点。后续描述使用右子树中值最小的结点作为“待交换结点”，左子树中值最大的结点理论上满足题目要求，但我未实际测试。 查找“待交换结点”的步骤 从“待删除结点”的右子结点开始，查找以它为根的子树中值最小的结点，查找过程同样需要记录当前结点的父结点及方向 二叉排序树中值最小的结点必定位于最左侧，因此只要当前结点存在左子结点，则持续将左子结点置为新的当前结点即可 最后一个不包含左子结点的当前结点，就是“待交换结点” “待交换结点”是必定存在的，既然找到了，就可以开始删除步骤了。题目并未要求实际删除该结点，因此通过变更该结点的值也可以实现类似功能。这里要做的，只是简单地把“待交换结点”的值赋给“待删除结点”。 最后要处理“待交换结点”，上文只说了它不包含左子结点，因此要考虑它是否存在右结点。如果它存在右结点，那么它的父结点的对应方向的子结点就是它的右结点，否则它的父结点的对应方向的子结点就是NULL。最后把“待交换结点”释放了即可。 C++代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: TreeNode* deleteNode(TreeNode* root, int key) &#123; TreeNode* node2Del = root; TreeNode* parentOfNode2Del = NULL; int direction = 0; // find the node 2 delete and its parent while (node2Del) &#123; if (node2Del-&gt;val == key) &#123; break; &#125; else if (node2Del-&gt;val &gt; key) &#123; parentOfNode2Del = node2Del; node2Del = node2Del-&gt;left; direction = -1; &#125; else &#123; parentOfNode2Del = node2Del; node2Del = node2Del-&gt;right; direction = 1; &#125; &#125; // node to delete not found, do nothng and return root if (!node2Del) &#123; return root; &#125; if (!node2Del-&gt;left &amp;&amp; !node2Del-&gt;right) &#123; // node to delete has no child // node to delete is root if (!parentOfNode2Del) &#123; delete root; return NULL; &#125; // node to delete is not root delete node2Del; if (direction &lt; 0) &#123; parentOfNode2Del-&gt;left = NULL; &#125; else &#123; parentOfNode2Del-&gt;right = NULL; &#125; &#125; else if (node2Del-&gt;left &amp;&amp; !node2Del-&gt;right) &#123; // node to delete has only left child // node to delete is root if (!parentOfNode2Del) &#123; delete root; return root-&gt;left; &#125; // node to delete is not root if (direction &lt; 0) &#123; parentOfNode2Del-&gt;left = node2Del-&gt;left; &#125; else &#123; parentOfNode2Del-&gt;right = node2Del-&gt;left; &#125; delete node2Del; &#125; else if (node2Del-&gt;right &amp;&amp; !node2Del-&gt;left) &#123; // node to delete has only right child // node to delete is root if (!parentOfNode2Del) &#123; delete root; return root-&gt;right; &#125; // node to delete is not root if (direction &lt; 0) &#123; parentOfNode2Del-&gt;left = node2Del-&gt;right; &#125; else &#123; parentOfNode2Del-&gt;right = node2Del-&gt;right; &#125; delete node2Del; &#125; else &#123; // node to delete has both left and right child // find the least value of right sub tree TreeNode* node2Switch = node2Del-&gt;right; TreeNode* parentOfNode2Switch = node2Del; int directionSwitch = 1; while (node2Switch-&gt;left) &#123; parentOfNode2Switch = node2Switch; node2Switch = node2Switch-&gt;left; directionSwitch = -1; &#125; // switch value node2Del-&gt;val = node2Switch-&gt;val; if (directionSwitch &lt; 0) &#123; parentOfNode2Switch-&gt;left = node2Switch-&gt;right; &#125; else &#123; parentOfNode2Switch-&gt;right = node2Switch-&gt;right; &#125; delete node2Switch; &#125; return root; &#125;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《番茄工作法图解》读后感&实践经历]]></title>
      <url>%2F2017%2F03%2F07%2Ftomato-working-method%2F</url>
      <content type="text"><![CDATA[一直以来我都觉得自己的工作效率不高，因此找了几本提升效率与管理时间的书来啃，这本《番茄工作法图解》，是我第一本完整看完的提升效率的书籍。 作者在书中介绍了人脑是如何处理事情的，也列举了很多针对具体场景的应对方式，同时提出了如何通过“番茄工作法”一步一步让自己的工作时间变得规律与可追踪。这里先抛出我从本书获取的两个主要观点： 一次只做一件事，尽量避免被打断； 活动清单+今日待办+每天回顾+可视化。 下面是我对这两个观点的理解，如果你现在正面临着工作效率不高的问题，也许我的理解可以帮助到你。 我认为学习与工作效率不高，一个原因就是不专注，另一个原因就是没有持续投入。 为什么学生时代的我可以同时学习很多科目的知识，甚至能以在考试前花一两天抱佛脚，就几乎把整本书的内容都记下来？这里不讨论考试前抱佛脚这种方式是否合理，仅从结果来看，应对考试应该是足够的。我认为学生时代我们能做到这样，无外乎我们专注于学习及持续投入。专注体现在学习某一科目的知识或者完成某一科目的作业，在一小段或一大段时间内，我们是专注在这件事情上面的。而持续投入时间，则体现在我们连续一个学期，甚至连续一年都在学习与巩固某一科目的知识。 踏入职场之后，我们的重心自然从学习转变为工作。那么工作如何做到专注与持续投入？我认为专注可以分为两个部分，一是“一次只做一件事”，二是“尽量避免被打断”，即观点一。 一次只做一件事：可能工作的事情很多，但无论什么时候，总能找出一件当前最应该做的事，我们要做的就是找出这件事，开始做这件事情，在这件事情告一段落之前，不考虑其他事情。 尽量避免被打断：工作被打断是很正常的事，下属找你汇报进度，上级找你了解进度，同事找你对齐方案，客户找你咨询问题，等等。我相信绝大多数人都无法完全避免工作被打断，那么我们能做的只能是尽量避免。具体方法见仁见智，我的应对方法是如果能推则推（有些事情其实不需要亲力亲为），能延则延（有些事情其实不是那么紧急，晚几个小时处理没什么问题），定期处理邮件与内部通讯工具，定期处理手机信息。对于实在无法避免的情况，停下手上的事情，保存好当前进度，最好能记录下来，接着做新的事情。如果条件允许，跟对方沟通以后尽量在特定时间找你。 一直以来，我都靠自己的大脑记住什么时候该做什么事情，学生时代我们一般只需要把所有的科目轮询一遍，基本就可以确定不会有遗漏的事情，但工作之后，各种大小事情都需要完成，只能大脑记住的我，自然忘记了某些事情，甚至承诺别人的事情都没有完成。这样的做法其实是个恶性循环，之前被忘记的事情会不断成为打断你当前工作的来源。 如何应对这种困境？我认为最简单有效的方法就是把要做的每一件事情记下来，分清紧急与重要程度，然后一件一件完成。根据书中描述的方法，我实际记录了两份清单，一是“活动清单”，二是“今日待办”，即观点二的前半部分。 “活动清单”用于记录每一件我要处理的事情，在每天工作过程中，这份清单会不断更新，我不仅记录要做什么事情，还会标注这件事情的紧急与重要程度、相关人等信息，确保我后面看到它的时候知道我具体要做什么。“今日待办”则用于记录我今天要做什么事情，我会在开始每天的工作之前，从活动清单里面选择一些我今天要完成的事情放到“今日待办”。如果遇到一些紧急的事情一定要在今天完成，我也会把它加入“今日待办”，但是会标注这些意料之外的事情，要在什么时间点完成。 那么如何衡量工作效率呢？我是通过“番茄时间”来记录我的工作并且衡量效率的。 “番茄工作法”很核心的一个工具，就是“番茄时间”，它其实很简单。一个“番茄时间”，由一个“番茄工作时间”和一个“番茄休息时间”构成，时间长短可以按个人习惯与喜好设定，比较标准的是25分钟工作时间+5分钟休息时间。上面提到一次只做一件事，选定好现在要做的事情之后，持续地在接下来的25分钟里做这件事情。25分钟时间到，记录当前的进度，停下来休息5分钟，可以站起来走走，喝水，上厕所，处理不重要的邮件与通讯信息等等。 我每天把“活动清单”中的事情记录到“今日待办”的时候，会大概估算每一件事情需要耗费我多少时间，即多少个“番茄时间”。每完成一个“番茄工作时间”，我都会记录下来，最终每天完成的“番茄时间”就可以用来衡量当天的工作效率。如果某些事情不需要25分钟即可完成，那么我会把这些事情合并起来做，如果某些事情要花费很多个25分钟，那么我会考虑把这些事情再拆细，方便我估算需要花费的时间。长此以往，把每天完成的“番茄时间”画成表格，就可以看出工作效率变化的趋势了，这也是观点二的后半部分想表达的意思。 这个方法我实践了两周时间，虽然还不够长久，但是工作效率有明显提升，最简单有力的说明就是每天都有不少代码产出，符合预期目标。 真心建议有工作效率问题的朋友，读一读这本书，豆瓣链接在此 《番茄工作法图解》。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[照片集]]></title>
      <url>%2F2017%2F02%2F28%2Feveryday-picture%2F</url>
      <content type="text"><![CDATA[2017 2017-0214th 10th 8th 5th 4th 1st 2017-0131th 27th 26th 24th 23rd 22nd 21st 19th 18th 16th 15th 13th 12th 11th 2016-1230th]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[面试题整理]]></title>
      <url>%2F2017%2F02%2F22%2Finterview-questions%2F</url>
      <content type="text"><![CDATA[编程语言技术要点C/C++Php PythonJavaGoShell面试题目Linux系统命令技术要点监控文件操作面试题目操作系统技术要点进程与线程协程Core dump面试题目网络技术要点DNSHTTPTCP TIME_WAIT UDP面试题目Web服务器技术要点服务器编程模型 同步/异步 阻塞/非阻塞 select/poll/epoll Nginx 服务器模型 模块化 Apache 服务器模型 其他自研server 原理 优缺点 面试题目存储技术要点MySQL 存储引擎 聚簇索引 事务隔离级别 索引优化 TdSQL 自动分库分表 MongoDBRedis 数据类型 持久化 Memcached面试题目分布式系统技术要点异常处理 网络超时 服务失效 分布式事务两阶段提交/三阶段提交Paxos协议Raft协议RPC面试题目中间件技术要点消息队列连接池线程池面试题目项目经历]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[谈谈重构]]></title>
      <url>%2F2017%2F02%2F18%2Fsystem-reconstruction%2F</url>
      <content type="text"><![CDATA[最近几个月专门在做系统重构，写点东西记录自己的心得。 首先说说我的经历。更换团队之后，我从接手系统到开始重构，大概有一年半的时间，时间不短，因此我把它划分为几个阶段来描述： 熟悉阶段 接手系统之后，由于业务需求排期比较急，我并没有足够的时间可以通过阅读代码熟悉系统的所有设计与逻辑，只能摸着石头过河。接到一个新的需求，往往都是先了解需求的前因后果，然后在前辈的指导下了解系统对应部分的大致实现，由于前期分配给我的需求一般都比较小，所以很多时候，我都是先了解系统特定部分的逻辑，然后加以修改。对系统接触和变更越来越多，加上前辈的指导，我对系统的设计与功能也渐渐熟悉。这一阶段大概持续了三个月，虽然有很多业务需求，有些需求很紧急，但毕竟是新人，很多业务压力没有透传到我这里。 改进阶段 熟悉系统之后，我开始接触更多需求，同时也负责部分模块的后续规划与技术优化。这一阶段我不仅要考虑需求的实现，同时要考虑如何合理地实现。所以我做的更多的事情是模块功能与设计的梳理，将耦合在一起的功能拆分成不同的子模块，将实现不合理的部分逻辑重新实现，当然这些事情很多时候都是跟随需求一起完成的，在业务发展迅速的时候，很难停下来专门做优化。也就是在这一阶段，团队发展，有更多的新人加入，我开始带新人接手这些模块，让他们顺利进入“熟悉阶段”。这一阶段大概持续了六个月，我从新人变成了部分系统模块的owner，对系统的整体实现与逻辑都有了足够的了解。 踩坑阶段 业务发展越来越快，需求越来越多，由于系统的复杂度太高，很多时候只能通过增加人力来解决问题，而新人对系统了解不深入，可能让系统越来越复杂，导致恶性循环。我跟老大表明我想重构手上的部分模块，这个让我正式进入踩坑阶段。我之前没有大规模重构的经验，有的只是一些小规模的重新设计与实现，这一次，我希望是把模块部分功能重新实现。我按照自己对业务的了解，以自己认为合理的方式，花了一个多月时间完成了。新逻辑测试发布上线之后，我们不断接到各种咨询与投诉，问题集中在我重写的这部分功能上面，很多边缘功能我并没有考虑清楚，导致线上故障频现。我又花了一个多月时间处理各种投诉，修复新逻辑带来的各种问题。这一阶段也持续三个月左右，这次失败的重构经历让我学习到不少新的东西，大概归类为以下几点： (1) 重构的大前提是系统稳定运行，不能以牺牲系统稳定性为代价 (2) 重构的功能是否符合预期，不能只靠人来保证，要建立完善的自动化测试用例来验证新逻辑 (3) 协议要做好兼容与转换，系统的重构是系统内部的变化，对外暴露的接口应该是不变的 重构准备阶段 踩坑之后，我发现很多系统的边缘功能我还无法完全掌握，所以我又花了相当长的时间梳理现有系统的逻辑与实现，同时着手建立完善的自动化测试用例。每次系统变更，我们都首先在预发布环境上跑一遍用例，保证没问题之后再发布上线。但是系统的设计与实现的不合理，还是成为了我们团队前进的阻力，团队因此专门抽出部分人力进行重构。从上一次失败的重构到立项重新进行重构，我们又经历了大概四个多月。 其次说说我对重构的理解。我认为重构并不是对整个系统进行重写，它应该是平时对系统不合理的设计与实现的不断改进。 接着说说为什么要重构。我觉得重构的原因有两个，其一是程序员的天性，其二是系统的设计与变更。 程序员的天性 作为程序员，无论是刚毕业加入第一家公司，还是工作之后跳槽到其他公司，从零开始开发一个新系统的机率是比较低的。作为新人，我们一般都是先接手并熟悉当前系统，了解它的逻辑与实现，并配合业务需求不断完善系统的功能与设计。绝大多数程序员都是有追求的，当我们足够了解现有系统之后，现有系统实现不合理的地方必然会成为我们的眼中钉，我们总会找机会进行重构。 系统的设计与变更 没有一个系统是完美的，或者说没有一个系统在设计的时候，就能预见到以后的所有需求场景。有的系统可能在设计与实现之初就留下了非常详尽的文档说明，在相当长的时间内，后来者都可以在原来的框架上合理地添加新功能。而有的系统可能连代码注释都极少，后来者往往忽略原来设计的初衷，按自己的理解来开发新的功能。随着接手系统的人越来越多，系统的功能越来越复杂，一些问题逐渐暴露出来。这里列出几类可能促使我们进行系统重构的问题： (1) 系统原本的设计不合理，可能是数据模型设计不合理，也可能是代码框架设计不合理，总之这些不合理的设计使得实现新的需求与特性变得十分困难。 (2) 系统复杂度太高，各类功能与实现耦合在一起，任何的小修改都可能影响到别的功能，任何的系统变更都可能导致现网故障，这个就像我们常说的“系统已经改不动了”。 (3) 系统使用的技术或框架太旧，或者已经没有很好的社区与官方支持。 最后说说如何进行重构。 首先，我们要有能力保证重构不会影响我们系统的稳定运行，一种有效的方式是建立完善的自动化测试用例，覆盖我们系统的所有使用场景，每次变更前都统一执行一遍。建议完善的自动化测试用例的一种方法是对足够长时间内线上系统的请求日志进行归类与记录，用实际的数据与请求模拟线上的行为。 其次，开始设计新系统，从底层数据模型（数据库），到系统层次与功能模块划分，到每个模块的代码结构。 再次，做好新老系统的兼容，这里可能包括数据模型的兼容，比如数据双写、新老数据对账等，也包括系统协议的兼容，比如抽象出统一的协议转换层等。 再次，做好新系统的验证，新系统在人工验证通过之后，必须通过之前建立的自动化测试用例的验证。 最后，新系统上线，建立回滚机制，考虑通过灰度等手段降低切换风险。 我计划接下来读一读《重构：改善既有代码的设计》这本书，学习如何更好地进行重构。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MapReduce论文]]></title>
      <url>%2F2017%2F01%2F28%2Fmap-reduce%2F</url>
      <content type="text"><![CDATA[英文版 MapReduce: Simplified Data Processing on Large ClustersJeffrey Dean and Sanjay Ghemawatjeff@google.com, sanjay@google.comGoogle, Inc AbstractMapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program’s execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google’s clusters every day. 1 IntroductionOver the past five years, the authors and many others at Google have implemented hundreds of special-purpose computations that process large amounts of raw data, such as crawled documents, web request logs, etc., to compute various kinds of derived data, such as inverted indices, various representations of the graph structure of web documents, summaries of the number of pages crawled per host, the set of most frequent queries in a given day, etc. Most such computations are conceptually straightforward. However, the input data is usually large and the computations have to be distributed across hundreds or thousands of machines in order to finish in a reasonable amount of time. The issues of how to parallelize the computation, distribute the data, and handle failures conspire to obscure the original simple computation with large amounts of complex code to deal with these issues. As a reaction to this complexity, we designed a new abstraction that allows us to express the simple computations we were trying to perform but hides the messy details of parallelization, fault-tolerance, data distribution and load balancing in a library. Our abstraction is inspired by the map and reduce primitives present in Lisp and many other functional languages. We realized that most of our computations involved applying a map operation to each logical “record” in our input in order to compute a set of intermediate key/value pairs, and then applying a reduce operation to all the values that shared the same key, in order to combine the derived data appropriately. Our use of a functional model with userspecified map and reduce operations allows us to parallelize large computations easily and to use re-execution as the primary mechanism for fault tolerance. The major contributions of this work are a simple and powerful interface that enables automatic parallelization and distribution of large-scale computations, combined with an implementation of this interface that achieves high performance on large clusters of commodity PCs. Section 2 describes the basic programming model and gives several examples. Section 3 describes an implementation of the MapReduce interface tailored towards our cluster-based computing environment. Section 4 describes several refinements of the programming model that we have found useful. Section 5 has performance measurements of our implementation for a variety of tasks. Section 6 explores the use of MapReduce within Google including our experiences in using it as the basis for a rewrite of our production indexing system. Section 7 discusses related and future work. 2 Programming ModelThe computation takes a set of input key/value pairs, and produces a set of output key/value pairs. The user of the MapReduce library expresses the computation as two functions: Map and Reduce. Map, written by the user, takes an input pair and produces a set of intermediate key/value pairs. The MapReduce library groups together all intermediate values associated with the same intermediate key I and passes them to the Reduce function. The Reduce function, also written by the user, accepts an intermediate key I and a set of values for that key. It merges together these values to form a possibly smaller set of values. Typically just zero or one output value is produced per Reduce invocation. The intermediate values are supplied to the user’s reduce function via an iterator. This allows us to handle lists of values that are too large to fit in memory. 2.1 ExampleConsider the problem of counting the number of occurrences of each word in a large collection of documents. The user would write code similar to the following pseudo-code: 12345678910111213map(String key, String value): // key: document name // value: document contents for each word w in value: EmitIntermediate(w, &quot;1&quot;); reduce(String key, Iterator values): // key: a word // values: a list of counts int result = 0; for each v in values: result += ParseInt(v); Emit(AsString(result)); The map function emits each word plus an associated count of occurrences (just ‘1’ in this simple example). The reduce function sums together all counts emitted for a particular word. In addition, the user writes code to fill in a mapreduce specification object with the names of the input and output files, and optional tuning parameters. The user then invokes the MapReduce function, passing it the specification object. The user’s code is linked together with the MapReduce library (implemented in C++). Appendix A contains the full program text for this example. 2.2 TypesEven though the previous pseudo-code is written in terms of string inputs and outputs, conceptually the map and reduce functions supplied by the user have associated types: map (k1,v1) → list(k2,v2)reduce (k2,list(v2)) → list(v2) I.e., the input keys and values are drawn from a different domain than the output keys and values. Furthermore, the intermediate keys and values are from the same domain as the output keys and values. Our C++ implementation passes strings to and from the user-defined functions and leaves it to the user code to convert between strings and appropriate types. 2.3 More ExamplesHere are a few simple examples of interesting programs that can be easily expressed as MapReduce computations. Distributed Grep: The map function emits a line if it matches a supplied pattern. The reduce function is an identity function that just copies the supplied intermediate data to the output. Count of URL Access Frequency: The map function processes logs of web page requests and outputs (URL, 1). The reduce function adds together all values for the same URL and emits a (URL, total count) pair. Reverse Web-Link Graph: The map function outputs (target, source) pairs for each link to a target URL found in a page named source. The reduce function concatenates the list of all source URLs associated with a given target URL and emits the pair: (target, list(source)) Term-Vector per Host: A term vector summarizes the most important words that occur in a document or a set of documents as a list of (word, frequency) pairs. The map function emits a (hostname, term vector) pair for each input document (where the hostname is extracted from the URL of the document). The reduce function is passed all per-document term vectors for a given host. It adds these term vectors together, throwing away infrequent terms, and then emits a final (hostname, term vector) pair. Inverted Index: The map function parses each document, and emits a sequence of (word, document ID) pairs. The reduce function accepts all pairs for a given word, sorts the corresponding document IDs and emits a (word, list(document ID)) pair. The set of all output pairs forms a simple inverted index. It is easy to augment this computation to keep track of word positions. Distributed Sort: The map function extracts the key from each record, and emits a (key, record) pair. The reduce function emits all pairs unchanged. This computation depends on the partitioning facilities described in Section 4.1 and the ordering properties described in Section 4.2. 3 ImplementationMany different implementations of the MapReduce interface are possible. The right choice depends on the environment. For example, one implementation may be suitable for a small shared-memory machine, another for a large NUMA multi-processor, and yet another for an even larger collection of networked machines. This section describes an implementation targeted to the computing environment in wide use at Google: large clusters of commodity PCs connected together with switched Ethernet [4]. In our environment: Machines are typically dual-processor x86 processors running Linux, with 2-4 GB of memory per machine. Commodity networking hardware is used – typically either 100 megabits/second or 1 gigabit/second at the machine level, but averaging considerably less in overall bisection bandwidth. A cluster consists of hundreds or thousands of machines, and therefore machine failures are common. Storage is provided by inexpensive IDE disks attached directly to individual machines. A distributed file system [8] developed in-house is used to manage the data stored on these disks. The file system uses replication to provide availability and reliability on top of unreliable hardware. Users submit jobs to a scheduling system. Each job consists of a set of tasks, and is mapped by the scheduler to a set of available machines within a cluster. 3.1 Execution OverviewFigure 1: Execution overview The Map invocations are distributed across multiple machines by automatically partitioning the input data into a set of M splits. The input splits can be processed in parallel by different machines. Reduce invocations are distributed by partitioning the intermediate key space into R pieces using a partitioning function (e.g., hash(key) mod R). The number of partitions (R) and the partitioning function are specified by the user. Figure 1 shows the overall flow of a MapReduce operation in our implementation. When the user program calls the MapReduce function, the following sequence of actions occurs (the numbered labels in Figure 1 correspond to the numbers in the list below): The MapReduce library in the user program first splits the input files into M pieces of typically 16 megabytes to 64 megabytes (MB) per piece (controllable by the user via an optional parameter). It then starts up many copies of the program on a cluster of machines. One of the copies of the program is special – the master. The rest are workers that are assigned work by the master. There are M map tasks and R reduce tasks to assign. The master picks idle workers and assigns each one a map task or a reduce task. A worker who is assigned a map task reads the contents of the corresponding input split. It parses key/value pairs out of the input data and passes each pair to the user-defined Map function. The intermediate key/value pairs produced by the Map function are buffered in memory. Periodically, the buffered pairs are written to local disk, partitioned into R regions by the partitioning function. The locations of these buffered pairs on the local disk are passed back to the master, who is responsible for forwarding these locations to the reduce workers. When a reduce worker is notified by the master about these locations, it uses remote procedure calls to read the buffered data from the local disks of the map workers. When a reduce worker has read all intermediate data, it sorts it by the intermediate keys so that all occurrences of the same key are grouped together. The sorting is needed because typically many different keys map to the same reduce task. If the amount of intermediate data is too large to fit in memory, an external sort is used. The reduce worker iterates over the sorted intermediate data and for each unique intermediate key encountered, it passes the key and the corresponding set of intermediate values to the user’s Reduce function. The output of the Reduce function is appended to a final output file for this reduce partition. When all map tasks and reduce tasks have been completed, the master wakes up the user program. At this point, the MapReduce call in the user program returns back to the user code. After successful completion, the output of the mapreduce execution is available in the R output files (one per reduce task, with file names as specified by the user). Typically, users do not need to combine these R output files into one file – they often pass these files as input to another MapReduce call, or use them from another distributed application that is able to deal with input that is partitioned into multiple files. 3.2 Master Data StructuresThe master keeps several data structures. For each map task and reduce task, it stores the state (idle, in-progress, or completed), and the identity of the worker machine (for non-idle tasks). The master is the conduit through which the location of intermediate file regions is propagated from map tasks to reduce tasks. Therefore, for each completed map task, the master stores the locations and sizes of the R intermediate file regions produced by the map task. Updates to this location and size information are received as map tasks are completed. The information is pushed incrementally to workers that have in-progress reduce tasks. 3.3 Fault ToleranceSince the MapReduce library is designed to help process very large amounts of data using hundreds or thousands of machines, the library must tolerate machine failures gracefully. Worker FailureThe master pings every worker periodically. If no response is received from a worker in a certain amount of time, the master marks the worker as failed. Any map tasks completed by the worker are reset back to their initial idle state, and therefore become eligible for scheduling on other workers. Similarly, any map task or reduce task in progress on a failed worker is also reset to idle and becomes eligible for rescheduling. Completed map tasks are re-executed on a failure because their output is stored on the local disk(s) of the failed machine and is therefore inaccessible. Completed reduce tasks do not need to be re-executed since their output is stored in a global file system. When a map task is executed first by worker A and then later executed by worker B (because A failed), all workers executing reduce tasks are notified of the reexecution. Any reduce task that has not already read the data from worker A will read the data from worker B. MapReduce is resilient to large-scale worker failures. For example, during one MapReduce operation, network maintenance on a running cluster was causing groups of 80 machines at a time to become unreachable for several minutes. The MapReduce master simply re-executed the work done by the unreachable worker machines, and continued to make forward progress, eventually completing the MapReduce operation. Master FailureIt is easy to make the master write periodic checkpoints of the master data structures described above. If the master task dies, a new copy can be started from the last checkpointed state. However, given that there is only a single master, its failure is unlikely; therefore our current implementation aborts the MapReduce computation if the master fails. Clients can check for this condition and retry the MapReduce operation if they desire. Semantics in the Presence of FailuresWhen the user-supplied map and reduce operators are deterministic functions of their input values, our distributed implementation produces the same output as would have been produced by a non-faulting sequential execution of the entire program. We rely on atomic commits of map and reduce task outputs to achieve this property. Each in-progress task writes its output to private temporary files. A reduce task produces one such file, and a map task produces R such files (one per reduce task). When a map task completes, the worker sends a message to the master and includes the names of the R temporary files in the message. If the master receives a completion message for an already completed map task, it ignores the message. Otherwise, it records the names of R files in a master data structure. When a reduce task completes, the reduce worker atomically renames its temporary output file to the final output file. If the same reduce task is executed on multiple machines, multiple rename calls will be executed for the same final output file. We rely on the atomic rename operation provided by the underlying file system to guarantee that the final file system state contains just the data produced by one execution of the reduce task. The vast majority of our map and reduce operators are deterministic, and the fact that our semantics are equivalent to a sequential execution in this case makes it very easy for programmers to reason about their program’s behavior. When the map and/or reduce operators are nondeterministic, we provide weaker but still reasonable semantics. In the presence of non-deterministic operators, the output of a particular reduce task R1 is equivalent to the output for R1 produced by a sequential execution of the non-deterministic program. However, the output for a different reduce task R2 may correspond to the output for R2 produced by a different sequential execution of the non-deterministic program. Consider map task M and reduce tasks R1 and R2. Let e(Ri) be the execution of Ri that committed (there is exactly one such execution). The weaker semantics arise because e(R1) may have read the output produced by one execution of M and e(R2) may have read the output produced by a different execution of M. 3.4 LocalityNetwork bandwidth is a relatively scarce resource in our computing environment. We conserve network bandwidth by taking advantage of the fact that the input data (managed by GFS [8]) is stored on the local disks of the machines that make up our cluster. GFS divides each file into 64 MB blocks, and stores several copies of each block (typically 3 copies) on different machines. The MapReduce master takes the location information of the input files into account and attempts to schedule a map task on a machine that contains a replica of the corresponding input data. Failing that, it attempts to schedule a map task near a replica of that task’s input data (e.g., on a worker machine that is on the same network switch as the machine containing the data). When running large MapReduce operations on a significant fraction of the workers in a cluster, most input data is read locally and consumes no network bandwidth. 3.5 Task GranularityWe subdivide the map phase into M pieces and the reduce phase into R pieces, as described above. Ideally, M and R should be much larger than the number of worker machines. Having each worker perform many different tasks improves dynamic load balancing, and also speeds up recovery when a worker fails: the many map tasks it has completed can be spread out across all the other worker machines. There are practical bounds on how large M and R can be in our implementation, since the master must make O(M + R) scheduling decisions and keeps O(M ∗ R) state in memory as described above. (The constant factors for memory usage are small however: the O(M ∗R) piece of the state consists of approximately one byte of data per map task/reduce task pair.) Furthermore, R is often constrained by users because the output of each reduce task ends up in a separate output file. In practice, we tend to choose M so that each individual task is roughly 16 MB to 64 MB of input data (so that the locality optimization described above is most effective), and we make R a small multiple of the number of worker machines we expect to use. We often perform MapReduce computations with M = 200, 000 and R = 5, 000, using 2,000 worker machines. 3.6 Backup TasksOne of the common causes that lengthens the total time taken for a MapReduce operation is a “straggler”: a machine that takes an unusually long time to complete one of the last few map or reduce tasks in the computation. Stragglers can arise for a whole host of reasons. For example, a machine with a bad disk may experience frequent correctable errors that slow its read performance from 30 MB/s to 1 MB/s. The cluster scheduling system may have scheduled other tasks on the machine, causing it to execute the MapReduce code more slowly due to competition for CPU, memory, local disk, or network bandwidth. A recent problem we experienced was a bug in machine initialization code that caused processor caches to be disabled: computations on affected machines slowed down by over a factor of one hundred. We have a general mechanism to alleviate the problem of stragglers. When a MapReduce operation is close to completion, the master schedules backup executions of the remaining in-progress tasks. The task is marked as completed whenever either the primary or the backup execution completes. We have tuned this mechanism so that it typically increases the computational resources used by the operation by no more than a few percent. We have found that this significantly reduces the time to complete large MapReduce operations. As an example, the sort program described in Section 5.3 takes 44% longer to complete when the backup task mechanism is disabled. 4 RefinementsAlthough the basic functionality provided by simply writing Map and Reduce functions is sufficient for most needs, we have found a few extensions useful. These are described in this section. 4.1 Partitioning FunctionThe users of MapReduce specify the number of reduce tasks/output files that they desire (R). Data gets partitioned across these tasks using a partitioning function on the intermediate key. A default partitioning function is provided that uses hashing (e.g. “hash(key) mod R”). This tends to result in fairly well-balanced partitions. In some cases, however, it is useful to partition data by some other function of the key. For example, sometimes the output keys are URLs, and we want all entries for a single host to end up in the same output file. To support situations like this, the user of the MapReduce library can provide a special partitioning function. For example, using “hash(Hostname(urlkey)) mod R” as the partitioning function causes all URLs from the same host to end up in the same output file. 4.2 Ordering GuaranteesWe guarantee that within a given partition, the intermediate key/value pairs are processed in increasing key order. This ordering guarantee makes it easy to generate a sorted output file per partition, which is useful when the output file format needs to support efficient random access lookups by key, or users of the output find it convenient to have the data sorted. 4.3 Combiner FunctionIn some cases, there is significant repetition in the intermediate keys produced by each map task, and the userspecified Reduce function is commutative and associative. A good example of this is the word counting example in Section 2.1. Since word frequencies tend to follow a Zipf distribution, each map task will produce hundreds or thousands of records of the form (the, 1). All of these counts will be sent over the network to a single reduce task and then added together by the Reduce function to produce one number. We allow the user to specify an optional Combiner function that does partial merging of this data before it is sent over the network. The Combiner function is executed on each machine that performs a map task. Typically the same code is used to implement both the combiner and the reduce functions. The only difference between a reduce function and a combiner function is how the MapReduce library handles the output of the function. The output of a reduce function is written to the final output file. The output of a combiner function is written to an intermediate file that will be sent to a reduce task. Partial combining significantly speeds up certain classes of MapReduce operations. Appendix A contains an example that uses a combiner. 4.4 Input and Output TypesThe MapReduce library provides support for reading input data in several different formats. For example, “text” mode input treats each line as a key/value pair: the key is the offset in the file and the value is the contents of the line. Another common supported format stores a sequence of key/value pairs sorted by key. Each input type implementation knows how to split itself into meaningful ranges for processing as separate map tasks (e.g. text mode’s range splitting ensures that range splits occur only at line boundaries). Users can add support for a new input type by providing an implementation of a simple reader interface, though most users just use one of a small number of predefined input types. A reader does not necessarily need to provide data read from a file. For example, it is easy to define a reader that reads records from a database, or from data structures mapped in memory. In a similar fashion, we support a set of output types for producing data in different formats and it is easy for user code to add support for new output types. 4.5 Side-effectsIn some cases, users of MapReduce have found it convenient to produce auxiliary files as additional outputs from their map and/or reduce operators. We rely on the application writer to make such side-effects atomic and idempotent. Typically the application writes to a temporary file and atomically renames this file once it has been fully generated. We do not provide support for atomic two-phase commits of multiple output files produced by a single task. Therefore, tasks that produce multiple output files with cross-file consistency requirements should be deterministic. This restriction has never been an issue in practice. 4.6 Skipping Bad RecordsSometimes there are bugs in user code that cause the Map or Reduce functions to crash deterministically on certain records. Such bugs prevent a MapReduce operation from completing. The usual course of action is to fix the bug, but sometimes this is not feasible; perhaps the bug is in a third-party library for which source code is unavailable. Also, sometimes it is acceptable to ignore a few records, for example when doing statistical analysis on a large data set. We provide an optional mode of execution where the MapReduce library detects which records cause deterministic crashes and skips these records in order to make forward progress. Each worker process installs a signal handler that catches segmentation violations and bus errors. Before invoking a user Map or Reduce operation, the MapReduce library stores the sequence number of the argument in a global variable. If the user code generates a signal, the signal handler sends a “last gasp” UDP packet that contains the sequence number to the MapReduce master. When the master has seen more than one failure on a particular record, it indicates that the record should be skipped when it issues the next re-execution of the corresponding Map or Reduce task. 4.7 Local ExecutionDebugging problems in Map or Reduce functions can be tricky, since the actual computation happens in a distributed system, often on several thousand machines, with work assignment decisions made dynamically by the master. To help facilitate debugging, profiling, and small-scale testing, we have developed an alternative implementation of the MapReduce library that sequentially executes all of the work for a MapReduce operation on the local machine. Controls are provided to the user so that the computation can be limited to particular map tasks. Users invoke their program with a special flag and can then easily use any debugging or testing tools they find useful (e.g. gdb). 4.8 Status InformationThe master runs an internal HTTP server and exports a set of status pages for human consumption. The status pages show the progress of the computation, such as how many tasks have been completed, how many are in progress, bytes of input, bytes of intermediate data, bytes of output, processing rates, etc. The pages also contain links to the standard error and standard output files generated by each task. The user can use this data to predict how long the computation will take, and whether or not more resources should be added to the computation. These pages can also be used to figure out when the computation is much slower than expected. In addition, the top-level status page shows which workers have failed, and which map and reduce tasks they were processing when they failed. This information is useful when attempting to diagnose bugs in the user code. 4.9 CountersThe MapReduce library provides a counter facility to count occurrences of various events. For example, user code may want to count total number of words processed or the number of German documents indexed, etc. To use this facility, user code creates a named counter object and then increments the counter appropriately in the Map and/or Reduce function. For example: 1234567Counter* uppercase;uppercase = GetCounter(&quot;uppercase&quot;);map(String name, String contents): for each word w in contents: if (IsCapitalized(w)): uppercase-&gt;Increment(); EmitIntermediate(w, &quot;1&quot;); The counter values from individual worker machines are periodically propagated to the master (piggybacked on the ping response). The master aggregates the counter values from successful map and reduce tasks and returns them to the user code when the MapReduce operation is completed. The current counter values are also displayed on the master status page so that a human can watch the progress of the live computation. When aggregating counter values, the master eliminates the effects of duplicate executions of the same map or reduce task to avoid double counting. (Duplicate executions can arise from our use of backup tasks and from re-execution of tasks due to failures.) Some counter values are automatically maintained by the MapReduce library, such as the number of input key/value pairs processed and the number of outputkey/value pairs produced. Users have found the counter facility useful for sanity checking the behavior of MapReduce operations. For example, in some MapReduce operations, the user code may want to ensure that the number of output pairs produced exactly equals the number of input pairs processed, or that the fraction of German documents processed is within some tolerable fraction of the total number of documents processed. 5 PerformanceIn this section we measure the performance of MapReduce on two computations running on a large cluster of machines. One computation searches through approximately one terabyte of data looking for a particular pattern. The other computation sorts approximately one terabyte of data. These two programs are representative of a large subset of the real programs written by users of MapReduce – one class of programs shuffles data from one representation to another, and another class extracts a small amount of interesting data from a large data set. 5.1 Cluster ConfigurationAll of the programs were executed on a cluster that consisted of approximately 1800 machines. Each machine had two 2GHz Intel Xeon processors with HyperThreading enabled, 4GB of memory, two 160GB IDE disks, and a gigabit Ethernet link. The machines were arranged in a two-level tree-shaped switched network with approximately 100-200 Gbps of aggregate bandwidth available at the root. All of the machines were in the same hosting facility and therefore the round-trip time between any pair of machines was less than a millisecond. Out of the 4GB of memory, approximately 1-1.5GB was reserved by other tasks running on the cluster. The programs were executed on a weekend afternoon, when the CPUs, disks, and network were mostly idle. 5.2 GrepFigure 2: Data transfer rate over time The grep program scans through 1010 100-byte records, searching for a relatively rare three-character pattern (the pattern occurs in 92,337 records). The input is split into approximately 64MB pieces (M = 15000), and the entire output is placed in one file (R = 1). Figure 2 shows the progress of the computation over time. The Y-axis shows the rate at which the input data is scanned. The rate gradually picks up as more machines are assigned to this MapReduce computation, and peaks at over 30 GB/s when 1764 workers have been assigned. As the map tasks finish, the rate starts dropping and hits zero about 80 seconds into the computation. The entire computation takes approximately 150 seconds from start to finish. This includes about a minute of startup overhead. The overhead is due to the propagation of the program to all worker machines, and delays interacting with GFS to open the set of 1000 input files and to get the information needed for the locality optimization. 5.3 SortFigure 3: Data transfer rates over time for different executions of the sort program The sort program sorts 1010 100-byte records (approximately 1 terabyte of data). This program is modeled after the TeraSort benchmark [10]. The sorting program consists of less than 50 lines of user code. A three-line Map function extracts a 10-byte sorting key from a text line and emits the key and the original text line as the intermediate key/value pair. We used a built-in Identity function as the Reduce operator. This functions passes the intermediate key/value pair unchanged as the output key/value pair. The final sorted output is written to a set of 2-way replicated GFS files (i.e., 2 terabytes are written as the output of the program). As before, the input data is split into 64MB pieces (M = 15000). We partition the sorted output into 4000 files (R = 4000). The partitioning function uses the initial bytes of the key to segregate it into one of R pieces. Our partitioning function for this benchmark has builtin knowledge of the distribution of keys. In a general sorting program, we would add a pre-pass MapReduce operation that would collect a sample of the keys and use the distribution of the sampled keys to compute splitpoints for the final sorting pass. Figure 3 (a) shows the progress of a normal execution of the sort program. The top-left graph shows the rate at which input is read. The rate peaks at about 13 GB/s and dies off fairly quickly since all map tasks finish before 200 seconds have elapsed. Note that the input rate is less than for grep. This is because the sort map tasks spend about half their time and I/O bandwidth writing intermediate output to their local disks. The corresponding intermediate output for grep had negligible size. The middle-left graph shows the rate at which data is sent over the network from the map tasks to the reduce tasks. This shuffling starts as soon as the first map task completes. The first hump in the graph is for the first batch of approximately 1700 reduce tasks (the entire MapReduce was assigned about 1700 machines, and each machine executes at most one reduce task at a time). Roughly 300 seconds into the computation, some of these first batch of reduce tasks finish and we start shuffling data for the remaining reduce tasks. All of the shuffling is done about 600 seconds into the computation. The bottom-left graph shows the rate at which sorted data is written to the final output files by the reduce tasks. There is a delay between the end of the first shuffling period and the start of the writing period because the machines are busy sorting the intermediate data. The writes continue at a rate of about 2-4 GB/s for a while. All of the writes finish about 850 seconds into the computation. Including startup overhead, the entire computation takes 891 seconds. This is similar to the current best reported result of 1057 seconds for the TeraSort benchmark [18]. A few things to note: the input rate is higher than the shuffle rate and the output rate because of our locality optimization – most data is read from a local disk and bypasses our relatively bandwidth constrained network. The shuffle rate is higher than the output rate because the output phase writes two copies of the sorted data (we make two replicas of the output for reliability and availability reasons). We write two replicas because that is the mechanism for reliability and availability provided by our underlying file system. Network bandwidth requirements for writing data would be reduced if the underlying file system used erasure coding [14] rather than replication. 5.4 Effect of Backup TasksIn Figure 3 (b), we show an execution of the sort program with backup tasks disabled. The execution flow is similar to that shown in Figure 3 (a), except that there is a very long tail where hardly any write activity occurs. After 960 seconds, all except 5 of the reduce tasks are completed. However these last few stragglers don’t finish until 300 seconds later. The entire computation takes 1283 seconds, an increase of 44% in elapsed time. 5.5 Machine FailuresIn Figure 3 (c), we show an execution of the sort program where we intentionally killed 200 out of 1746 worker processes several minutes into the computation. The underlying cluster scheduler immediately restarted new worker processes on these machines (since only the processes were killed, the machines were still functioning properly). The worker deaths show up as a negative input rate since some previously completed map work disappears (since the corresponding map workers were killed) and needs to be redone. The re-execution of this map work happens relatively quickly. The entire computation finishes in 933 seconds including startup overhead (just an increase of 5% over the normal execution time). 6 ExperienceWe wrote the first version of the MapReduce library in February of 2003, and made significant enhancements to it in August of 2003, including the locality optimization, dynamic load balancing of task execution across worker machines, etc. Since that time, we have been pleasantly surprised at how broadly applicable the MapReduce library has been for the kinds of problems we work on. It has been used across a wide range of domains within Google, including: large-scale machine learning problems, clustering problems for the Google News and Froogle products, extraction of data used to produce reports of popular queries (e.g. Google Zeitgeist), extraction of properties of web pages for new experiments and products (e.g. extraction of geographical locations from a large corpus of web pages for localized search), and large-scale graph computations. Figure 4: MapReduce instances over time Table 1: MapReduce jobs run in August 2004 Figure 4 shows the significant growth in the number of separate MapReduce programs checked into our primary source code management system over time, from 0 in early 2003 to almost 900 separate instances as of late September 2004. MapReduce has been so successful because it makes it possible to write a simple program and run it efficiently on a thousand machines in the course of half an hour, greatly speeding up the development and prototyping cycle. Furthermore, it allows programmers who have no experience with distributed and/or parallel systems to exploit large amounts of resources easily. At the end of each job, the MapReduce library logs statistics about the computational resources used by the job. In Table 1, we show some statistics for a subset of MapReduce jobs run at Google in August 2004. 6.1 Large-Scale IndexingOne of our most significant uses of MapReduce to date has been a complete rewrite of the production indexing system that produces the data structures used for the Google web search service. The indexing system takes as input a large set of documents that have been retrieved by our crawling system, stored as a set of GFS files. The raw contents for these documents are more than 20 terabytes of data. The indexing process runs as a sequence of five to ten MapReduce operations. Using MapReduce (instead of the ad-hoc distributed passes in the prior version of the indexing system) has provided several benefits: • The indexing code is simpler, smaller, and easier to understand, because the code that deals with fault tolerance, distribution and parallelization is hidden within the MapReduce library. For example, the size of one phase of the computation dropped from approximately 3800 lines of C++ code to approximately 700 lines when expressed using MapReduce.• The performance of the MapReduce library is good enough that we can keep conceptually unrelated computations separate, instead of mixing them together to avoid extra passes over the data. This makes it easy to change the indexing process. For example, one change that took a few months to make in our old indexing system took only a few days to implement in the new system.• The indexing process has become much easier to operate, because most of the problems caused by machine failures, slow machines, and networking hiccups are dealt with automatically by the MapReduce library without operator intervention. Furthermore, it is easy to improve the performance of the indexing process by adding new machines to the indexing cluster. 7 Related WorkMany systems have provided restricted programming models and used the restrictions to parallelize the computation automatically. For example, an associative function can be computed over all prefixes of an N element array in log N time on N processors using parallel prefix computations [6, 9, 13]. MapReduce can be considered a simplification and distillation of some of these models based on our experience with large real-world computations. More significantly, we provide a fault-tolerant implementation that scales to thousands of processors. In contrast, most of the parallel processing systems have only been implemented on smaller scales and leave the details of handling machine failures to the programmer. Bulk Synchronous Programming [17] and some MPI primitives [11] provide higher-level abstractions that make it easier for programmers to write parallel programs. A key difference between these systems and MapReduce is that MapReduce exploits a restricted programming model to parallelize the user program automatically and to provide transparent fault-tolerance. Our locality optimization draws its inspiration from techniques such as active disks [12, 15], where computation is pushed into processing elements that are close to local disks, to reduce the amount of data sent across I/O subsystems or the network. We run on commodity processors to which a small number of disks are directly connected instead of running directly on disk controller processors, but the general approach is similar. Our backup task mechanism is similar to the eager scheduling mechanism employed in the Charlotte System [3]. One of the shortcomings of simple eager scheduling is that if a given task causes repeated failures, the entire computation fails to complete. We fix some instances of this problem with our mechanism for skipping bad records. The MapReduce implementation relies on an in-house cluster management system that is responsible for distributing and running user tasks on a large collection of shared machines. Though not the focus of this paper, the cluster management system is similar in spirit to other systems such as Condor [16]. The sorting facility that is a part of the MapReduce library is similar in operation to NOW-Sort [1]. Source machines (map workers) partition the data to be sorted and send it to one of R reduce workers. Each reduce worker sorts its data locally (in memory if possible). Of course NOW-Sort does not have the user-definable Map and Reduce functions that make our library widely applicable. River [2] provides a programming model where processes communicate with each other by sending data over distributed queues. Like MapReduce, the River system tries to provide good average case performance even in the presence of non-uniformities introduced by heterogeneous hardware or system perturbations. River achieves this by careful scheduling of disk and network transfers to achieve balanced completion times. MapReduce has a different approach. By restricting the programming model, the MapReduce framework is able to partition the problem into a large number of finegrained tasks. These tasks are dynamically scheduled on available workers so that faster workers process more tasks. The restricted programming model also allows us to schedule redundant executions of tasks near the end of the job which greatly reduces completion time in the presence of non-uniformities (such as slow or stuck workers). BAD-FS [5] has a very different programming model from MapReduce, and unlike MapReduce, is targeted to the execution of jobs across a wide-area network. However, there are two fundamental similarities. (1) Both systems use redundant execution to recover from data loss caused by failures. (2) Both use locality-aware scheduling to reduce the amount of data sent across congested network links. TACC [7] is a system designed to simplify construction of highly-available networked services. Like MapReduce, it relies on re-execution as a mechanism for implementing fault-tolerance. 8 ConclusionsThe MapReduce programming model has been successfully used at Google for many different purposes. We attribute this success to several reasons. First, the model is easy to use, even for programmers without experience with parallel and distributed systems, since it hides the details of parallelization, fault-tolerance, locality optimization, and load balancing. Second, a large variety of problems are easily expressible as MapReduce computations. For example, MapReduce is used for the generation of data for Google’s production web search service, for sorting, for data mining, for machine learning, and many other systems. Third, we have developed an implementation of MapReduce that scales to large clusters of machines comprising thousands of machines. The implementation makes efficient use of these machine resources and therefore is suitable for use on many of the large computational problems encountered at Google. We have learned several things from this work. First, restricting the programming model makes it easy to parallelize and distribute computations and to make such computations fault-tolerant. Second, network bandwidth is a scarce resource. A number of optimizations in our system are therefore targeted at reducing the amount of data sent across the network: the locality optimization allows us to read data from local disks, and writing a single copy of the intermediate data to local disk saves network bandwidth. Third, redundant execution can be used to reduce the impact of slow machines, and to handle machine failures and data loss. AcknowledgementsJosh Levenberg has been instrumental in revising and extending the user-level MapReduce API with a number of new features based on his experience with using MapReduce and other people’s suggestions for enhancements. MapReduce reads its input from and writes its output to the Google File System [8]. We would like to thank Mohit Aron, Howard Gobioff, Markus Gutschke, David Kramer, Shun-Tak Leung, and Josh Redstone for their work in developing GFS. We would also like to thank Percy Liang and Olcan Sercinoglu for their work in developing the cluster management system used by MapReduce. Mike Burrows, Wilson Hsieh, Josh Levenberg, Sharon Perl, Rob Pike, and Debby Wallach provided helpful comments on earlier drafts of this paper. The anonymous OSDI reviewers, and our shepherd, Eric Brewer, provided many useful suggestions of areas where the paper could be improved. Finally, we thank all the users of MapReduce within Google’s engineering organization for providing helpful feedback, suggestions, and bug reports. References[1] Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, David E. Culler, Joseph M. Hellerstein, and David A. Patterson. High-performance sorting on networks of workstations. In Proceedings of the 1997 ACM SIGMOD International Conference on Management of Data, Tucson, Arizona, May 1997. [2] Remzi H. Arpaci-Dusseau, Eric Anderson, Noah Treuhaft, David E. Culler, Joseph M. Hellerstein, David Patterson, and Kathy Yelick. Cluster I/O with River: Making the fast case common. In Proceedings of the Sixth Workshop on Input/Output in Parallel and Distributed Systems (IOPADS ’99), pages 10–22, Atlanta, Georgia, May 1999. [3] Arash Baratloo, Mehmet Karaul, Zvi Kedem, and Peter Wyckoff. Charlotte: Metacomputing on the web. In Proceedings of the 9th International Conference on Parallel and Distributed Computing Systems, 1996. [4] Luiz A. Barroso, Jeffrey Dean, and Urs H¨olzle. Web search for a planet: The Google cluster architecture. IEEE Micro, 23(2):22–28, April 2003. [5] John Bent, Douglas Thain, Andrea C.Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, and Miron Livny. Explicit control in a batch-aware distributed file system. In Proceedings of the 1st USENIX Symposium on Networked Systems Design and Implementation NSDI, March 2004. [6] Guy E. Blelloch. Scans as primitive parallel operations. IEEE Transactions on Computers, C-38(11), November 1989. [7] Armando Fox, Steven D. Gribble, Yatin Chawathe, Eric A. Brewer, and Paul Gauthier. Cluster-based scalable network services. In Proceedings of the 16th ACM Symposium on Operating System Principles, pages 78–91, Saint-Malo, France, 1997. [8] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. The Google file system. In 19th Symposium on Operating Systems Principles, pages 29–43, Lake George, New York, 2003. [9] S. Gorlatch. Systematic efficient parallelization of scan and other list homomorphisms. In L. Bouge, P. Fraigniaud, A. Mignotte, and Y. Robert, editors, Euro-Par’96. Parallel Processing, Lecture Notes in Computer Science 1124, pages 401–408. Springer-Verlag, 1996. [10] Jim Gray. Sort benchmark home page. http://research.microsoft.com/barc/SortBenchmark/. [11] William Gropp, Ewing Lusk, and Anthony Skjellum. Using MPI: Portable Parallel Programming with the Message-Passing Interface. MIT Press, Cambridge, MA, 1999. [12] L. Huston, R. Sukthankar, R. Wickremesinghe, M. Satyanarayanan, G. R. Ganger, E. Riedel, and A. Ailamaki. Diamond: A storage architecture for early discard in interactive search. In Proceedings of the 2004 USENIX File and Storage Technologies FAST Conference, April 2004. [13] Richard E. Ladner and Michael J. Fischer. Parallel prefix computation. Journal of the ACM, 27(4):831–838, 1980. [14] Michael O. Rabin. Efficient dispersal of information for security, load balancing and fault tolerance. Journal of the ACM, 36(2):335–348, 1989. [15] Erik Riedel, Christos Faloutsos, Garth A. Gibson, and David Nagle. Active disks for large-scale data processing. IEEE Computer, pages 68–74, June 2001. [16] Douglas Thain, Todd Tannenbaum, and Miron Livny. Distributed computing in practice: The Condor experience. Concurrency and Computation: Practice and Experience, 2004. [17] L. G. Valiant. A bridging model for parallel computation. Communications of the ACM, 33(8):103–111, 1997. [18] Jim Wyllie. Spsort: How to sort a terabyte quickly. http://alme1.almaden.ibm.com/cs/spsort.pdf. A Word FrequencyThis section contains a program that counts the number of occurrences of each unique word in a set of input files specified on the command line. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586#include &quot;mapreduce/mapreduce.h&quot;// User’s map functionclass WordCounter : public Mapper &#123; public: virtual void Map(const MapInput&amp; input) &#123; const string&amp; text = input.value(); const int n = text.size(); for (int i = 0; i &lt; n; ) &#123; // Skip past leading whitespace while ((i &lt; n) &amp;&amp; isspace(text[i])) i++; // Find word end int start = i; while ((i &lt; n) &amp;&amp; !isspace(text[i])) i++; if (start &lt; i) Emit(text.substr(start,i-start),&quot;1&quot;); &#125; &#125;&#125;;REGISTER_MAPPER(WordCounter);// User’s reduce functionclass Adder : public Reducer &#123; virtual void Reduce(ReduceInput* input) &#123; // Iterate over all entries with the // same key and add the values int64 value = 0; while (!input-&gt;done()) &#123; value += StringToInt(input-&gt;value()); input-&gt;NextValue(); &#125; // Emit sum for input-&gt;key() Emit(IntToString(value)); &#125;&#125;;REGISTER_REDUCER(Adder);int main(int argc, char** argv) &#123; ParseCommandLineFlags(argc, argv); MapReduceSpecification spec; // Store list of input files into &quot;spec&quot; for (int i = 1; i &lt; argc; i++) &#123; MapReduceInput* input = spec.add_input(); input-&gt;set_format(&quot;text&quot;); input-&gt;set_filepattern(argv[i]); input-&gt;set_mapper_class(&quot;WordCounter&quot;); &#125; // Specify the output files: // /gfs/test/freq-00000-of-00100 // /gfs/test/freq-00001-of-00100 // ... MapReduceOutput* out = spec.output(); out-&gt;set_filebase(&quot;/gfs/test/freq&quot;); out-&gt;set_num_tasks(100); out-&gt;set_format(&quot;text&quot;); out-&gt;set_reducer_class(&quot;Adder&quot;); // Optional: do partial sums within map // tasks to save network bandwidth out-&gt;set_combiner_class(&quot;Adder&quot;); // Tuning parameters: use at most 2000 // machines and 100 MB of memory per task spec.set_machines(2000); spec.set_map_megabytes(100); spec.set_reduce_megabytes(100); // Now run it MapReduceResult result; if (!MapReduce(spec, &amp;result)) abort(); // Done: &apos;result&apos; structure contains info // about counters, time taken, number of // machines used, etc. return 0;&#125; 中文版 MapReduce: 超大机群上的简单数据处理Jeffrey Dean and Sanjay Ghemawatjeff@google.com, sanjay@google.com谷歌]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Google File System]]></title>
      <url>%2F2017%2F01%2F28%2Fgoogle-file-system%2F</url>
      <content type="text"><![CDATA[转自http://blog.csdn.net/xuleicsu/article/details/526386 GFS是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。它运行于廉价的普通硬件上，但可以提供容错功能。它可以给大量的用户提供总体性能较高的服务。 设计概览设计想法GFS与过去的分布式文件系统有很多相同的目标，但GFS的设计受到了当前及预期的应用方面的工作量及技术环境的驱动，这反映了它与早期的文件系统明显不同的设想。这就需要对传统的选择进行重新检验并进行完全不同的设计观点的探索。 GFS与以往的文件系统的不同的观点如下： 部件错误不再被当作异常，而是将其作为常见的情况加以处理。因为文件系统由成百上千个用于存储的机器构成，而这些机器是由廉价的普通部件组成并被大量的客户机访问。部件的数量和质量使得一些机器随时都有可能无法工作并且有一部分还可能无法恢复。所以实时地监控、错误检测、容错、自动恢复对系统来说必不可少。 按照传统的标准，文件都非常大。长度达几个GB的文件是很平常的。每个文件通常包含很多应用对象。当经常要处理快速增长的、包含数以万计的对象、长度达TB的数据集时，我们很难管理成千上万的KB规模的文件块，即使底层文件系统提供支持。因此，设计中操作的参数、块的大小必须要重新考虑。对大型的文件的管理一定要能做到高效，对小型的文件也必须支持，但不必优化。 大部分文件的更新是通过添加新数据完成的，而不是改变已存在的数据。在一个文件中随机的操作在实践中几乎不存在。一旦写完，文件就只可读，很多数据都有这些特性。一些数据可能组成一个大仓库以供数据分析程序扫描。有些是运行中的程序连续产生的数据流。有些是档案性质的数据，有些是在某个机器上产生、在另外一个机器上处理的中间数据。由于这些对大型文件的访问方式，添加操作成为性能优化和原子性保证的焦点。而在客户机中缓存数据块则失去了吸引力。 工作量主要由两种读操作构成：对大量数据的流方式的读操作和对少量数据的随机方式的读操作。在前一种读操作中，可能要读几百KB，通常达 1MB和更多。来自同一个客户的连续操作通常会读文件的一个连续的区域。随机的读操作通常在一个随机的偏移处读几个KB。性能敏感的应用程序通常将对少量数据的读操作进行分类并进行批处理以使得读操作稳定地向前推进，而不要让它来来回回的读。 工作量还包含许多对大量数据进行的、连续的、向文件添加数据的写操作。所写的数据的规模和读相似。一旦写完，文件很少改动。在随机位置对少量数据的写操作也支持，但不必非常高效。 系统必须高效地实现定义完好的大量客户同时向同一个文件的添加操作的语义。 系统接口GFS提供了一个相似地文件系统界面，虽然它没有向POSIX那样实现标准的API。文件在目录中按层次组织起来并由路径名标识。 体系结构一个GFS集群由一个master和大量的chunk server构成，并被许多客户（client）访问。如图1所示。master和chunk server通常是运行用户层服务进程的Linux机器。只要资源和可靠性允许，chunk server和client可以运行在同一个机器上。 文件被分成固定大小的块。每个块由一个不变的、全局唯一的64位的chunk－handle标识，chunk－handle是在块创建时由 master分配的。chunk server将块当作Linux文件存储在本地磁盘并可以读和写由chunk－handle和位区间指定的数据。出于可靠性考虑，每一个块被复制到多个chunk server上。默认情况下，保存3个副本，但这可以由用户指定。 master维护文件系统所以的元数据（metadata），包括名字空间、访问控制信息、从文件到块的映射以及块的当前位置。它也控制系统范围的活动，如块租约（lease）管理，孤儿块的垃圾收集，chunk server间的块迁移。master定期通过HeartBeat消息与每一个chunk server通信，给chunk server传递指令并收集它的状态。 与每个应用相联的GFS客户代码实现了文件系统的API并与master和chunk server通信以代表应用程序读和写数据。客户与master的交换只限于对元数据（metadata）的操作，所有数据方面的通信都直接和chunk server联系。 客户和chunk server都不缓存文件数据。因为用户缓存的益处微乎其微，这是由于数据太多或工作集太大而无法缓存。不缓存数据简化了客户程序和整个系统，因为不必考虑缓存的一致性问题。但用户缓存元数据（metadata）。chunk server也不必缓存文件，因为块时作为本地文件存储的。 单master只有一个master也极大的简化了设计并使得master可以根据全局情况作出先进的块放置和复制决定。但是我们必须要将master对读和写的参与减至最少，这样它才不会成为系统的瓶颈。client从来不会从master读和写文件数据。client只是询问master它应该和哪个 chunk server联系。client在一段限定的时间内将这些信息缓存，在后续的操作中client直接和chunk server交互。 client使用固定的块大小将应用程序指定的文件名和字节偏移转换成文件的一个块索引（chunk index）。 给master发送一个包含文件名和块索引的请求。 master回应对应的chunk handle和副本的位置（多个副本）。 client以文件名和块索引为键缓存这些信息。（handle和副本的位置）。 client 向其中一个副本发送一个请求，很可能是最近的一个副本。请求指定了chunk handle（chunk server以chunk handle标识chunk）和块内的一个字节区间。 除非缓存的信息不再有效（cache for a limited time）或文件被重新打开，否则以后对同一个块的读操作不再需要client和master间的交互。 通常client可以在一个请求中询问多个chunk的地址，而master也可以很快回应这些请求。 块规模块规模是设计中的一个关键参数。我们选择的是64MB，这比一般的文件系统的块规模要大的多。每个块的副本作为一个普通的Linux文件存储，在需要的时候可以扩展。块规模较大的好处有： 减少client和master之间的交互。因为读写同一个块只是要在开始时向master请求块位置信息。对于读写大型文件这种减少尤为重要。即使对于访问少量数据的随机读操作也可以很方便的为一个规模达几个TB的工作集缓缓存块位置信息。 client在一个给定的块上很可能执行多个操作，和一个chunk server保持较长时间的TCP连接可以减少网络负载。 这减少了master上保存的元数据（metadata）的规模，从而使得可以将metadata放在内存中。这又会带来一些别的好处。 不利的一面： 一个小文件可能只包含一个块，如果很多client访问改文件的话，存储这些块的chunk server将成为访问的热点。但在实际应用中，应用程序通常顺序地读包含多个块的文件，所以这不是一个主要问题。 元数据（metadata）master存储了三中类型的metadata：文件的名字空间和块的名字空间，从文件到块的映射，块的副本的位置。所有的metadata都放在内存中。前两种类型的metadata通过向操作日志登记修改而保持不变，操作日志存储在master的本地磁盘并在几个远程机器上留有副本。使用日志使得我们可以很简单地、可靠地更新master的状态，即使在master崩溃的情况下也不会有不一致的问题。相反，mater在每次启动以及当有chunk server加入的时候询问每个chunk server的所拥有的块的情况。 内存数据结构因为metadata存储在内存中，所以master的操作很快。进一步，master可以轻易而且高效地定期在后台扫描它的整个状态。这种定期地扫描被用于实现块垃圾收集、chunk server出现故障时的副本复制、为平衡负载和磁盘空间而进行的块迁移。 这种方法的一个潜在的问题就是块的数量也即整个系统的容量是否受限与master的内存。实际上，这并不是一个严重的问题。master为每个 64MB的块维护的metadata不足64个字节。除了最后一块，文件所有的块都是满的。类似的，每个文件的名字空间数据也不足64个字节，因为文件名是以一种事先确定的压缩方式存储的.如果要支持更大的文件系统，那么增加一些内存的方法对于我们将元数据（metadata）保存在内存种所获得的简单性、可靠性、高性能和灵活性来说，这只是一个很小的代价。 块位置master并不为chunk server所拥有的块的副本的保存一个不变的记录。它在启动时通过简单的查询来获得这些信息。master可以保持这些信息的更新，因为它控制所有块的放置并通过HeartBeat消息来监控chunk server的状态。 这样做的好处：因为chunk server可能加入或离开集群、改变路径名、崩溃、重启等，一个集群重有成百个server，这些事件经常发生，这种方法就排除了master与chunk server之间的同步问题。 另一个原因是：只有chunk server才能确定它自己到底有哪些块，由于错误，chunk server中的一些块可能会很自然的消失，这样在master中就没有必要为此保存一个不变的记录。 操作日志操作日志包含了对metadata所作的修改的历史记录。它作为逻辑时间线定义了并发操作的执行顺序。文件、块以及它们的版本号都由它们被创建时的逻辑时间而唯一地、永久地被标识。 操作日志是如此的重要，我们必须要将它可靠地保存起来，并且只有在metadata的改变固定下来之后才将变化呈现给用户。所以我们将操作日志复制到数个远程的机器上，并且只有在将相应的日志记录写到本地和远程的磁盘上之后才回答用户的请求。 master可以用操作日志来恢复它的文件系统的状态。为了将启动时间减至最小，日志就必须要比较小。每当日志的长度增长到超过一定的规模后，master就要检查它的状态，它可以从本地磁盘装入最近的检查点来恢复状态。 创建一个检查点比较费时，master的内部状态是以一种在创建一个检查点时并不耽误即将到来的修改操作的方式来组织的。master切换到一个新的日子文件并在一个单独的线程中创建检查点。这个新的检查点记录了切换前所有的修改。在一个有数十万文件的集群中用一分钟左右就能完成。创建完后，将它写入本地和远程的磁盘。 数据完整性名字空间的修改必须是原子性的，它们只能有master处理：名字空间锁保证了操作的原子性和正确性，而master的操作日志在全局范围内定义了这些操作的顺序。 文件区间的状态在修改之后依赖于修改的类型，不论操作成功还是失败，也不论是不是并发操作。如果不论从哪个副本上读，所有的客户都看到同样的数据，那么文件的这个区域就是一致的。如果文件的区域是一致的并且用户可以看到修改操作所写的数据，那么它就是已定义的。如果修改是在没有并发写操作的影响下完成的，那么受影响的区域是已定义的，所有的client都能看到写的内容。成功的并发写操作是未定义但却是一致的。失败的修改将使区间处于不一致的状态。 write操作在应用程序指定的偏移处写入数据，而record append操作使得数据（记录）即使在有并发修改操作的情况下也至少原子性的被加到GFS指定的偏移处，偏移地址被返回给用户。 在一系列成功的修改操作后，最后的修改操作保证文件区域是已定义的。GFS通过对所有的副本执行同样顺序的修改操作并且使用块版本号检测过时的副本（由于chunk server退出而导致丢失修改）来做到这一点。 因为用户缓存了会位置信息，所以在更新缓存之前有可能从一个过时的副本中读取数据。但这有缓存的截止时间和文件的重新打开而受到限制。 在修改操作成功后，部件故障仍可以是数据受到破坏。GFS通过master和chunk server间定期的handshake，借助校验和来检测对数据的破坏。一旦检测到，就从一个有效的副本尽快重新存储。只有在GFS检测前，所有的副本都失效，这个块才会丢失。 系统交互租约（lease）和修改顺序数据流我们的目标是充分利用每个机器的网络带宽，避免网络瓶颈和延迟。为了有效的利用网络，我们将数据流和控制流分离。数据是以流水线的方式在选定的chunker server链上线性的传递的。每个机器的整个对外带宽都被用作传递数据。为避免瓶颈，每个机器在收到数据后，将它收到数据尽快传递给离它最近的机器。 原子性的record AppendGFS提供了一个原子性的添加操作：record append。在传统的写操作中，client指定被写数据的偏移位置，向同一个区间的并发的写操作是不连续的：区间有可能包含来自多个client的数据碎片。在record append中， client只是指定数据。GFS在其选定的偏移出将数据至少原子性的加入文件一次，并将偏移返回给client。 在分布式的应用中，不同机器上的许多client可能会同时向一个文件执行添加操作，添加操作被频繁使用。如果用传统的write操作，可能需要额外的、复杂的、开销较大的同步，例如通过分布式锁管理。在我们的工作量中，这些文件通常以多个生产者单个消费者队列的方式或包含从多个不同 client的综合结果。 record append和前面讲的write操作的控制流差不多，只是在primary上多了一些逻辑判断。首先，client将数据发送到文件最后一块的所有副本上。然后向primary发送请求。Primary检查添加操作是否会导致该块超过最大的规模（64M）。如果这样，它将该块扩充到最大规模，并告诉其它副本做同样的事，同时通知client该操作需要在下一个块上重新尝试。如果记录满足最大规模的要求，primary就会将数据添加到它的副本上，并告诉其它的副本在在同样的偏移处写数据，最后primary向client报告写操作成功。如果在任何一个副本上record append操作失败，client将重新尝试该操作。这时候，同一个块的副本可能包含不同的数据，因为有的可能复制了全部的数据，有的可能只复制了部分。GFS不能保证所有的副本每个字节都是一样的。它只保证每个数据作为一个原子单元被写过至少一次。这个是这样得出的：操作要是成功，数据必须在所有的副本上的同样的偏移处被写过。进一步，从这以后，所有的副本至少和记录一样长，所以后续的记录将被指定到更高的偏移处或者一个不同的块上，即使另一个副本成了primary。根据一致性保证，成功的record append操作的区间是已定义的。而受到干扰的区间是不一致的。 快照（snapshot）快照操作几乎在瞬间构造一个文件和目录树的副本，同时将正在进行的其他修改操作对它的影响减至最小。 我们使用copy-on-write技术来实现snapshot。当master受到一个snapshot请求时，它首先将要snapshot的文件上块上的lease。这使得任何一个向这些块写数据的操作都必须和master交互以找到拥有lease的副本。这就给master一个创建这个块的副本的机会。 副本被撤销或终止后，master在磁盘上登记执行的操作，然后复制源文件或目录树的metadata以对它的内存状态实施登记的操作。这个新创建的snapshot文件和源文件（其metadata）指向相同的块（chunk）。 Snapshot之后，客户第一次向chunk c写的时候，它发一个请求给master以找到拥有lease的副本。master注意到chunk c的引用记数比1大，它延迟对用户的响应，选择一个chunk handle C’,然后要求每一有chunk c的副本的chunk server创建一个块C’。每个chunk server在本地创建chunk C’避免了网络开销。从这以后和对别的块的操作没有什么区别。 MASTER操作MASTER执行所有名字空间的操作，除此之外，他还在系统范围管理数据块的复制：决定数据块的放置方案，产生新数据块并将其备份，和其他系统范围的操作协同来确保数据备份的完整性，在所有的数据块服务器之间平衡负载并收回没有使用的存储空间。 名字空间管理和加锁与传统文件系统不同的是，GFS没有与每个目录相关的能列出其所有文件的数据结构，它也不支持别名（unix中的硬连接或符号连接），不管是对文件或是目录。GFS的名字空间逻辑上是从文件元数据到路径名映射的一个查用表。 MASTER在执行某个操作前都要获得一系列锁，例如，它要对/d1/d2…/dn/leaf执行操作，则它必须获得/d1，/d1/d2，…， /d1/d2/…/dn的读锁，/d1/d2…/dn/leaf的读锁或写锁（其中leaf可以使文件也可以是目录）。MASTER操作的并行性和数据的一致性就是通过这些锁来实现的。 备份存储放置策略一个GFS集群文件系统可能是多层分布的。一般情况下是成千上万个文件块服务器分布于不同的机架上，而这些文件块服务器又被分布于不同机架上的客户来访问。因此，不同机架上的两台机器之间的通信可能通过一个或多个交换机。数据块冗余配置策略要达到连个目的：最大的数据可靠性和可用性，最大的网络带宽利用率。因此，如果仅仅把数据的拷贝置于不同的机器上很难满足这两个要求，必须在不同的机架上进行数据备份。这样即使整个机架被毁或是掉线，也能确保数据的正常使用。这也使数据传输，尤其是读数据，可以充分利用带宽，访问到多个机架，而写操作，则不得不涉及到更多的机架。 产生、重复制、重平衡数据块当MASTER产生新的数据块时，如何放置新数据块，要考虑如下几个因素：（1）尽量放置在磁盘利用率低的数据块服务器上，这样，慢慢地各服务器的磁盘利用率就会达到平衡。（2）尽量控制在一个服务器上的“新创建”的次数。（3）由于上一小节讨论的原因，我们需要把数据块放置于不同的机架上。 MASTER在可用的数据块备份低于用户设定的数目时需要进行重复制。这种情况源于多种原因：服务器不可用，数据被破坏，磁盘被破坏，或者备份数目被修改。每个被需要重复制的数据块的优先级根据以下几项确定：第一是现在的数目距目标的距离，对于能阻塞用户程序的数据块，我们也提高它的优先级。最后， MASTER按照产生数据块的原则复制数据块，并把它们放到不同的机架内的服务器上。 MASTER周期性的平衡各服务器上的负载：它检查chunk分布和负载平衡，通过这种方式来填充一个新的服务器而不是把其他的内容统统放置到它上面带来大量的写数据。数据块放置的原则与上面讨论的相同，此外，MASTER还决定那些数据块要被移除，原则上他会清除那些空闲空间低于平均值的那些服务器。 垃圾收集在一个文件被删除之后，GFS并不立即收回磁盘空间，而是等到垃圾收集程序在文件和数据块级的的检查中收回。 当一个文件被应用程序删除之后，MASTER会立即记录下这些变化，但文件所占用的资源却不会被立即收回，而是重新给文件命了一个隐藏的名字，并附上了删除的时间戳。在MASTER定期检查名字空间时，它删除超过三天（可以设定）的隐藏的文件。在此之前，可以以一个新的名字来读文件，还可以以前的名字恢复。当隐藏的文件在名字空间中被删除以后，它在内存中的元数据即被擦除，这就有效地切断了他和所有数据块的联系。 在一个相似的定期的名字空间检查中，MASTER确认孤儿数据块（不属于任何文件）并擦除他的元数据，在和MASTER的心跳信息交换中，每个服务器报告他所拥有的数据块，MASTER返回元数据不在内存的数据块，服务器即可以删除这些数据块。 过时数据的探测在数据更新时如果服务器停机了，那么他所保存的数据备份就会过时。对每个数据块，MASTER设置了一个版本号来区别更新过的数据块和过时的数据块。 当MASTER授权一个新的lease时，他会增加数据块的版本号并会通知更新数据备份。MASTER和备份都会记录下当前的版本号，如果一个备份当时不可用，那么他的版本号不可能提高，当chunk server重新启动并向MASTER报告他的数据块集时，MASTER就会发现过时的数据。 MASTER在定期的垃圾收集程序中清除过时的备份，在此以前，处于效率考虑，在各客户及英大使，他会认为根本不存在过时的数据。作为另一个安全措施， MASTER在给客户及关于数据块的应答或是另外一个读取数据的服务器数据是都会带上版本信息，在操作前客户机和服务器会验证版本信息以确保得到的是最新的数据。 容错和诊断高可靠性快速恢复不管如何终止服务，MASTER和数据块服务器都会在几秒钟内恢复状态和运行。实际上，我们不对正常终止和不正常终止进行区分，服务器进程都会被切断而终止。客户机和其他的服务器会经历一个小小的中断，然后它们的特定请求超时，重新连接重启的服务器，重新请求。 数据块备份如上文所讨论的，每个数据块都会被备份到放到不同机架上的不同服务器上。对不同的名字空间，用户可以设置不同的备份级别。在数据块服务器掉线或是数据被破坏时，MASTER会按照需要来复制数据块。 MASTER备份为确保可靠性，MASTER的状态、操作记录和检查点都在多台机器上进行了备份。一个操作只有在数据块服务器硬盘上刷新并被记录在MASTER和其备份的上之后才算是成功的。如果MASTER或是硬盘失败，系统监视器会发现并通过改变域名启动它的一个备份机，而客户机则仅仅是使用规范的名称来访问，并不会发现MASTER的改变。 数据完整性每个数据块服务器都利用校验和来检验存储数据的完整性。原因：每个服务器随时都有发生崩溃的可能性，并且在两个服务器间比较数据块也是不现实的，同时，在两台服务器间拷贝数据并不能保证数据的一致性。 每个Chunk按64kB的大小分成块，每个块有32位的校验和，校验和和日志存储在一起，和用户数据分开。 在读数据时，服务器首先检查与被读内容相关部分的校验和，因此，服务器不会传播错误的数据。如果所检查的内容和校验和不符，服务器就会给数据请求者返回一个错误的信息，并把这个情况报告给MASTER。客户机就会读其他的服务器来获取数据，而MASTER则会从其他的拷贝来复制数据，等到一个新的拷贝完成时，MASTER就会通知报告错误的服务器删除出错的数据块。 附加写数据时的校验和计算优化了，因为这是主要的写操作。我们只是更新增加部分的校验和，即使末尾部分的校验和数据已被损坏而我们没有检查出来，新的校验和与数据会不相符，这种冲突在下次使用时将会被检查出来。 相反，如果是覆盖现有数据的写，在写以前，我们必须检查第一和最后一个数据块，然后才能执行写操作，最后计算和记录校验和。如果我们在覆盖以前不先检查首位数据块，计算出的校验和则会因为没被覆盖的数据而产生错误。 在空闲时间，服务器会检查不活跃的数据块的校验和，这样可以检查出不经常读的数据的错误。一旦错误被检查出来，服务器会拷贝一个正确的数据块来代替错误的。 诊断工具广泛而细致的诊断日志以微小的代价换取了在问题隔离、诊断、性能分析方面起到了重大的作用。GFS服务器用日志来记录显著的事件（例如服务器停机和启动）和远程的应答。远程日志记录机器之间的请求和应答，通过收集不同机器上的日志记录，并对它们进行分析恢复，我们可以完整地重现活动的场景，并用此来进行错误分析。 测量测试环境一台主控机，两台主控机备份，16台数据块服务器，16台客户机。 每台机器：2块PIII1.4G处理器，2G内存，2块80G5400rpm的硬盘，1块100Mbps全双工网卡 19台服务器连接到一个HP2524交换机上，16台客户机俩接到领外一台交换机上，两台交换机通过1G的链路相连。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《三体》读后感]]></title>
      <url>%2F2017%2F01%2F16%2Fthree-body%2F</url>
      <content type="text"><![CDATA[人物叶文洁大史罗辑托马斯・维德程心云天明三体人歌者 我毁灭你，与你有何关系 事件叶文洁发射信号给三体人设置面壁者设置执剑人水滴毁灭人类星舰人类星舰黑暗战役三体毁灭太阳系毁灭云天明赠送恒星给程心云天明制造小星球程心归还小星球给宇宙名词黑暗森林]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[对订单系统的一些思考]]></title>
      <url>%2F2017%2F01%2F12%2Forder-system%2F</url>
      <content type="text"><![CDATA[订单是什么 订单是交易的快照信息，用于记录交易发生时的相关信息 订单一般包含哪些信息 用户：商品或服务的消费者 商家：商品或服务的提供者 物品：交易涉及的商品或服务的名称、属性及描述信息 价格：物品的价格，根据需要可以分为原始价格、单位价格、最终价格等 优惠：商家提供给用户的用于减免价格的行为描述，一般包含折扣、代金券、抵扣券等各种类型 时间：一般包含交易发生与结束时间、支付发生与结束时间、物品发货与完成时间等 支付流水号：凡是涉及支付行为的订单，一般都需要与支付流水号相关联，支付流水号一般由其他系统维护，记录支付渠道、账户变更等信息 订单的常见状态与主要流程 创建：用户通过前台售卖页面或API发起下单请求，订单系统根据请求内容创建新的订单，此时订单的状态为“初始化” 支付：若订单最终需要支付的金额不为0，则一般由用户通过前台售卖页面或API发起支付请求，订单系统调用外部支付系统发起支付，若支付请求是同步的，则可根据支付成功与失败决定订单新的状态，若支付请求是异步的，一般需要先将订单状态切换为“支付中”，后续通过订单系统轮询或支付系统回调的方式获取支付结果 发货：已支付的订单或者不需要支付的订单，系统可直接进行发货，发货结果一般由订单系统轮询或外部系统回调进行更新 退款：用户通过前台管理页面或API发起退款请求，一般仅当订单处于“发货完成”或“发货失败”时才可进行退款 查询：用户通过前台售卖页面或API，查询符合条件的单个或多个订单 取消：处于“初始化”状态的订单，一般可由用户主动取消，若订单超过一定时间未支付，系统也可自动取消 删除：对于“已取消”或“已退款”状态的订单，用户可通过前台管理页面或API进行删除，后续查询请求无法获取“已删除”状态的订单 订单系统的安全问题 权限校验：用户任何操作都需要先鉴权，用户仅能操作自己的订单 操作频率限制：系统针对用户特定操作必须有频率限制，若发现异常请求需要能主动拒绝 通用参数校验：对每一个订单相关的通用参数都需要做类型与取值范围的基础校验，同时需要考虑参数与业务逻辑的相关校验 业务参数校验：通常用户购买的商品都会有特定的属性，这些属性必须是合法且有效的，不能信任前台带过来的任何参数 操作日志：用户在何时进行了何种操作，系统都需要记录下来 数据备份：订单数据库需要避免单点问题 订单系统的性能问题订单号的生成 订单server通常需要分布式部署，但订单号必须是全局唯一的，这就要求有特定的算法用于处理订单号的生成规则，一般可以考虑时间戳、随机数、机器ID等因素的组合 读写分离 普通的订单系统一般是读多写少，对于常见的数据库一主多备部署方式，写请求由主库处理，读请求则分散到各个从库处理 缓存 为减轻数据库的压力，可考虑引入缓存，但缓存会增加系统复杂度，重点需要考虑缓存的更新问题 数据大结点 对于to B的订单server，可能10%的用户覆盖了90%的订单，这种情况下订单表分布是极不均匀的，即使命中索引，数据库处理速度仍然不满足要求，这种情况下需要根据使用场景，引入缓存或特殊逻辑处理这些大结点 分布式事务 订单server与数据库一般是独立部署的，不同server之间通过网络调用进行通信，必然会出现各类异常情况，此时可引入分布式事务，解决各个server之间状态不一致的问题，但分布式事务会增加系统复杂度，同时可能导致性能问题，需要根据实际场景做好评估 其他需要考虑的点 接口可重入：订单server对外提供的接口必须是可重入的，网络超时重试是必然会出现的]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis变长字符串(simple dynamic strings)学习笔记]]></title>
      <url>%2F2016%2F10%2F31%2Fredis-data-structure-sds%2F</url>
      <content type="text"><![CDATA[主要文件 sds.h sds.c 结构1234567struct sdshdr &#123; int len; // buf已占用长度 int free; // buf可使用长度 char buf[]; // 实际数据&#125;;typedef char *sds; 知识点flexible array member sds结构一般指向sdshdr-&gt;buf，可以通过sds当前地址计算得到sdshdr地址12const sds s;struct sdshdr *hdr = (void*)(s - (sizeof(struct sdssdr))); 接口说明获取buf的已占用长度定义1static inline size_t sdslen(const sds s); 流程 根据sds当前地址获取sdshdr地址，获取len元素的值 获取buf的可使用长度定义1static inline size_t sdsavail(const sds s); 流程 根据sds当前地址获取sdshdr地址，获取free元素的值 创建一个指定长度的sds，支持初始化定义1sds sdsnewlen(const void *init, size_t initlen); 流程 根据传入的长度创建一个变长字符串，总占用空间为sizeof(struct sdshdr)+initlen+1，再根据需要进行初始化，len设置为initlen，free设置为0 创建一个包含空串的sds定义1sds sdsempty(void); 流程 调用sdsnewlen(“”, 0) 根据指定初始化值创建sds定义1sds sdsnew(const char *init); 流程 调用sdsnewlen(init, initlen)，其中initlen为指定初始化值的长度，若初始化值为NULL，则长度为0 复制sds定义1sds sdsdup(const sds s); 流程 调用sdsnewlen(s, sdslen(s)) 释放sds定义1void sdsfree(sds s); 流程 若传入的sds为NULL，则直接返回。否则先找到sdshdr的起始地址，然后释放其对应内存 根据buf的实际长度更新sdshdr的属性定义1void sdsupdatelen(sds s); 流程 根据sds计算得到sdshdr的起始地址，计算buf的实际长度，len与free的值对应调整 清除sds的内容，使其只包含\0定义1void sdsclear(sds s); 流程 根据sds计算得到sdshdr的起始地址，len设置为0，free对应增加len的原长度 对sds的buf进行扩展定义1sds sdsMakeRoomFor(sds s, size_t addlen); 流程 若当前free长度超过扩展长度，则无需操作，否则根据新长度与SDS_MAX_PREALLOC的关系决定扩展后的长度，最后更新sds的free元素 释放sds多余的内存定义1sds sdsRemoveFreeSpace(sds s); 流程 根据sds计算得到sdshdr的起始地址，计算出实际所需内存大小，释放其余的内存，并将free元素置0 计算sds占用的总内存大小定义1size_t sdsAllocSize(sds s); 流程 根据sds计算得到sdshdr的起始地址，总长度=buf指针大小+已占用长度+可使用长度+1 sds buf的右端增加/减少指定长度定义1void sdsIncrLen(sds s, int incr); 流程 若指定长度为正，则free可用长度必须不小于该值。变更buf内容，将新长度的结尾设置为\0，同时更新len与free元素 将sds buf扩展至指定长度，扩展部分设置为\0定义1sds sdsgrowzero(sds s, size_t len); 流程 若当前已占用长度不小于传入的指定长度，则无需操作，否则调用sdsMakeRoomFor扩展长度，并将扩展的部分置为\0，同时更新len与free元素 扩展sds长度并拼接字符串定义1sds sdscatlen(sds s, const void *t, size_t len); 流程 调用sdsMakeRoomFor扩展len长度，将t拼接至结尾，同时更新len与free元素 在sds末尾拼接字符串定义1sds sdscat(sds s, const char *t); 流程 调用return sdscatlen(s, t, strlen(t)) 在sds末尾拼接另一个sds定义1sds sdscatsds(sds s, const sds t); 流程 调用sdscatlen(s, t, sdslen(t)) 拷贝字符串至sds，指定对应长度定义1sds sdscpylen(sds s, const char *t, size_t len); 流程 若sds长度不够，则调用sdsMakeRoomFor扩展它的长度。复制字符串至buf之后，对应更新len与free元素 拷贝字符串至sds定义1sds sdscpy(sds s, const char *t); 流程 调用sdscpylen(s, t, strlen(t)) 从sds两端删除特定字符定义1sds sdstrim(sds s, const char *cset); 流程 从头部和尾部分别遍历sds buf，获取不满足条件的新头部和尾部的位置，将新头部和尾部的字符复制到buf中，并更新len和free元素 指定头部和尾部的位置截取sds定义1sds sdsrange(sds s, int start, int end); 流程 根据指定的头部和尾部的位置，经过部分边界计算，截取得到新的sds 将sds的字母转换为小写形式定义1void sdstolower(sds s); 流程 遍历sds buf，把字母逐个转换为小写形式 将sds的字母转换为大写形式定义1void sdstoupper(sds s); 流程 遍历sds buf，把字母逐个转换为大写形式 比较两个sds的大小定义1int sdscmp(const sds s1, const sds s2); 流程 获取两个sds对应的sdshdr，从而得到各自的长度，使用memcmp逐个字节比较 将字符串分割成多个sds定义1sds *sdssplitlen(const char *s, int len, const char *sep, int seplen, int *count); 流程 从头至尾扫描sds buf，根据分割串，生成多个对应的sds 释放分隔得到的sds定义1void sdsfreesplitres(sds *tokens, int count); 流程 对tokens指向的sds逐个调用sdsfree释放空间 将long long型整数转换为对应的sds定义1sds sdsfromlonglong(long long value); 流程 使用长度为32的字符串，从value低位至高位逐个获取对应的10进制数字，转换成对应字符存入其中，若value为负，则对应增加负号，最后调用sdsnewlen根据该字符串有效位生成sds 在sds后面拼接repr转换之后的字符串定义1sds sdscatrepr(sds s, const char *p, size_t len); 流程 在sds之后拼接一个双引号，之后按指定长度和字符串进行repr转换和拼接，最后再拼接一个双引号 判断字符是否为16进制相关字符定义1int is_hex_digit(char c); 流程 判断字符是否为0~9或a~f或A~F 将16进制相关字符转换为对应整数值定义1int hex_digit_to_int(char c); 流程 将0~9字符分别转为数字0~9，将a~f或A~F对应转为数字10~15 将命令行参数转换成多个sds定义1sds *sdssplitargs(const char *line, int *argc); 流程 逐个遍历line指向的命令行参数字符串，根据字符串分隔符，同时考虑特殊字符与16进制等情形，得到多个sds 释放命令行参数转换之后得到的sds对应的空间定义1void sdssplitargs_free(sds *argv, int argc); 流程 遍历传入的sds，逐个调用sdsfree释放空间 将sds的字符根据规则替换为别的字符定义1sds sdsmapchars(sds s, const char *from, const char *to, size_t setlen); 流程 遍历sds buf的每个字符，若该字符在集合from中，则对应转换为集合to中的字符 将sds与参数列表按指定格式生成的字符串拼接定义1sds sdscatvprintf(sds s, const char *fmt, va_list ap); 流程 以16为起始长度，根据实际容纳参数列表生成的字符串所需要的长度不断翻倍，最后将调用sdscat进行拼接 将sds与变长参数按指定格式生成的字符串拼接定义1sds sdscatprintf(sds s, const char *fmt, ...); 流程 将变长参数转换为va_list格式，调用sdscatvprintf进行拼接]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PHP轻量框架分享]]></title>
      <url>%2F2016%2F10%2F11%2Fphp-lightweight-framework%2F</url>
      <content type="text"><![CDATA[背景说明 近期需要将之前功能较复杂的后台服务模块拆成小模块，该模块采用了前人开发的基于MVC的php自研框架，框架本身集成了较多功能，包括接口参数统一校验、异构数据库接口抽象封装等 由于历史悠久且缺乏文档说明，随着需求与版本迭代，代码结构变得比较混乱，维护成本随之增加 由于框架本身的设计限制，无法支持新版本PHP（如5.6）的部分特性，引入一些新的概念与技术变得困难 新模块功能相对比较简单，不需要集成较多特性 框架介绍主要特性 协议可定制（推荐json） 引入ORM，不需要自己封装DB基础操作 配置文件使用YAML格式，可读性较高 适合搭建后台简单服务 方便与常见web server集成（推荐nginx） 框架结构1234567891011121314151617181920212223242526272829303132333435363738common # 公共目录 |- Constants.php # 常量 |- Errors.php # 错误码与错误信息 |- ...configs # 配置目录 |- db.yaml # 数据库配置 |- log.yaml # 日志配置 |- ...models # 模型目录 |- ...services # 服务目录 |- ...controllers # 控制器目录 |- ...libraries # 库目录 |- Configuration.php # 配置解析类 |- HttpRequest.php # HTTP处理类 |- DbConnection.php # DB处理类 |- Log.php # 日志处理类 |- ...logs # 日志目录 |- ...scripts # 离线脚本目录 |- ...tests # 测试目录 |- controllers # 控制器测试目录 |- ... |- services # 服务测试目录 |- ... |- models # 模型测试目录 |- ... |- ...vendor # 包管理目录 |- ...composer.lock # composer锁文件composer.json # composer配置文件phpunit.xml # PHP单元测试配置文件index.php # 请求入口类 目录与文件说明 index.php: 请求入口文件，负责解析请求包，选择合适接口处理请求，组装返回包 common: 存放项目公共文件，如常量、错误码与错误信息等 configs: 存放项目配置文件，如数据库配置、日志配置、业务配置等 models: 每个数据表对应一个文件，每个文件封装了对应数据表的ORM操作（依赖illuminate/database包） services: 每一类服务对应一个文件，每个文件封装了针对一个或多个ORM的操作，需要在这一层处理事务 controllers: 存放接口文件，每个文件对应一个外部调用接口，需要在这一层做好接口参数校验 libraries: 存放项目库文件，如日志操作、数据库操作、配置解析等 scriptes: 存放所有离线脚本 tests: 存放所有测试类 phpunit.xml: 单元测试配置文件，用于执行tests目录下的测试类 logs: 存放所有日志文件 vendor: 存放所有项目依赖的包 composer.json: 包管理配置文件 composer.lock: 包管理操作锁 示例项目 商品库存管理模块]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[异步任务管理模块设计分享]]></title>
      <url>%2F2016%2F10%2F09%2Fasync-task-manage-module%2F</url>
      <content type="text"><![CDATA[构建模块的背景业务发展 业务刚推出的时候，产品考虑的是如何让用户使用我们的资源，因此只要提供针对资源的基本操作（购买、续费、升级等）即可 随着互联网的发展与业务发展，用户对于资源的需求量增加，因此出现了同时购买大批量的资源，同时操作大批量资源的情况，由于响应时间较长，用户体验非常不好 业务搞活动的时候，可能出现较多客户同时购买资源的情况，对正常服务造成影响，用户体验同样较差 后台服务演进 大部分后台服务提供的接口在设计之初，主要考虑的是如何完成业务功能，对于接口性能与操作逻辑优化考虑的相对比较少 业务逻辑变更或发展，导致接口逻辑愈发复杂，模块间的网络调用次数增加，同时后台存储的数据量也逐渐增加，接口耗时随之增加 先针对单个资源的操作接口逐渐洐生出批量操作接口，以应对用户的批量操作请求 大多数批量接口实际是伪批量，后台实际逻辑基本是串行的 常规的接口性能优化，包括数据缓存、逻辑优化、网络调用采用并发处理等，虽然能在一定程度上减少接口耗时，但部分优化使得接口逻辑反而变复杂了，同时部分接口耗时依然达不到要求，随着批量处理的数量的增加，接口性能优化带来的收益愈发不明显 业务搞活动时，接口请求量暴涨，部分耗时接口的性能问题会被放大，严重时可能会耗尽服务器资源，影响正常业务流程 请求异步化的思路什么是请求异步化 后台服务收到请求之后，不立即处理请求，而是把请求记录下来 后台服务跟调用方约定获取请求执行结果的方式 后台服务异步执行对应的请求，并记录执行结果 调用方通过轮询或其他方式获取执行结果 为什么需要异步化 网络调用需要做超时处理，调用方不可能无限期等待后台服务的响应，同时用户体验不好，异步可以使调用方即时获得响应，对此做出相应的用户提示与交互 异步执行请求的频率可以调整，对于活动时接口请求量暴涨的情形，可以降低请求的执行频率，保证主线业务正常运行 为接口性能优化带来一种新的方式，不再需要牺牲代码可维护性与扩展性来做性能优化 请求异步化会带来什么问题 调用方逻辑需要改变，原来的直接请求响应的方式需要变更为轮询的方式，若调用方较多，改造成本会相对较高 需要额外的逻辑与存储来处理对应的异步请求与响应 可能引入外部模块，系统架构复杂度上升 异步任务管理模块的引入为什么需要做独立的模块 后台服务涉及的模块比较多，在微服务比较盛行的今天，每一类相对独立的功能都可能被封装为一个服务模块，每个模块各自实现自己的异步任务管理逻辑，改造成本较高，同时也不好维护 独立的模块可以抽象出异步任务相关的API，规范任务的处理流程，对各个调用方保持统一 模块的几种设计思路子进程同步处理+脚本触发任务检查 主要流程 后台服务模块每次收到请求直接，就把请求对应处理之后，调用异步任务管理模块创建任务的接口 异步任务管理模块收到创建请求，生成任务唯一标识，把请求信息写入存储，同时fork子进程回调后台服务处理对应请求，主进程则直接返回任务标识给后台服务模块，后台服务对应将任务标识返回给调用方 子进程同步等待后台服务的处理结果，并将结果记录到存储中 调用方根据任务标识，不断从异步任务管理模块轮询处理结果 离线脚本主要用于触发任务检查，防止部分任务由于子进程创建失败或者其他原因未处理 特点 请求处理及时，正常情况下可保证每个请求创建之后立即被处理 无法限流，若同时产生的任务较多，可能导致异步任务管理模块所在机器资源不足，或者后台服务压力较大 后台服务处理接口需要支持可重入，防止因为网络等原因导致状态不同步 脚本定时处理任务 主要流程 后台服务模块每次收到请求直接，就把请求对应处理之后，调用异步任务管理模块创建任务的接口 异步任务管理模块收到创建请求，生成任务唯一标识，把请求信息写入存储之后直接返回该标识给后台服务模块，后台服务对应返回给调用方 离线脚本触发异步任务管理模块进行任务处理，异步任务管理模块调用后台服务模块同步获取处理结果，并更新到存储中 调用方根据任务标识，不断从异步任务管理模块轮询处理结果 特点 请求处理有一定延时，若任务较多可能导致任务堆积 可以部署多个离线脚本，但是异步任务管理模块自身负载会对应上升 可以限流，比如离线脚本多久执行一次，限制每次处理多少个任务等 后台服务处理接口需要支持可重入，防止因为网络等原因导致状态不同步 消息队列+脚本触发任务检查 主要流程 后台服务模块每次收到请求直接，就把请求对应处理之后，调用异步任务管理模块创建任务的接口 异步任务管理模块收到创建请求，生成任务唯一标识，把请求信息写入存储之后，添加一条消息到消息队列，然后返回任务标识给后台服务模块，后台服务对应返回给调用方 消息队列根据预配置的策略将消息推送至后台服务模块（此处仅考虑推送模式），后台服务模块收到消息之后对应进行处理，然后将处理结果更新至异步任务管理模块 调用方根据任务标识，不断从异步任务管理模块轮询处理结果 离线脚本主要用于触发任务检查，防止部分任务消息添加失败或者其他原因未处理 特点 引入消息队列，后台服务需要在消息队列配置对应策略与接口 请求处理有一定延时，取决于消息队列推送策略 任务堆积的问题理论上可以通过配置合理的消息队列推送策略解决 限流可以通过消息队列推送策略实现 若消息队列与后台服务通信出现问题，异步任务管理模块无法及时感知，可能需要增加对账机制 后台服务处理接口需要支持可重入，防止因为网络等原因导致状态不同步]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[linux系统调用列表]]></title>
      <url>%2F2016%2F09%2F28%2Flinux-system-call%2F</url>
      <content type="text"><![CDATA[进程控制 系统调用名称 功能描述 fork 创建一个新进程 clone 按指定条件创建子进程 execve 运行可执行文件 exit 中止进程 _exit 立即中止当前进程 getdtablesize 进程所能打开的最大文件数 getpgid 获取指定进程组标识号 setpgid 设置指定进程组标志号 getpgrp 获取当前进程组标识号 setpgrp 设置当前进程组标志号 getpid 获取进程标识号 getppid 获取父进程标识号 getpriority 获取调度优先级 setpriority 设置调度优先级 modify_ldt 读写进程的本地描述表 nanosleep 使进程睡眠指定的时间 nice 改变分时进程的优先级 pause 挂起进程，等待信号 personality 设置进程运行域 prctl 对进程进行特定操作 ptrace 进程跟踪 sched_get_priority_max 取得静态优先级的上限 sched_get_priority_min 取得静态优先级的下限 sched_getparam 取得进程的调度参数 sched_getscheduler 取得指定进程的调度策略 sched_rr_get_interval 取得按RR算法调度的实时进程的时间片长度 sched_setparam 设置进程的调度参数 sched_setscheduler 设置指定进程的调度策略和参数 sched_yield 进程主动让出处理器,并将自己等候调度队列队尾 vfork 创建一个子进程，以供执行新程序，常与execve等同时使用 wait 等待子进程终止 wait3 参见wait waitpid 等待指定子进程终止 wait4 参见waitpid capget 获取进程权限 capset 设置进程权限 getsid 获取会晤标识号 setsid 设置会晤标识号 文件系统控制文件读写操作 系统调用名称 功能描述 fcntl 文件控制 open 打开文件 creat 创建新文件 close 关闭文件描述字 read 读文件 write 写文件 readv 从文件读入数据到缓冲数组中 writev 将缓冲数组里的数据写入文件 pread 对文件随机读 pwrite 对文件随机写 lseek 移动文件指针 _llseek 在64位地址空间里移动文件指针 dup 复制已打开的文件描述字 dup2 按指定条件复制文件描述字 flock 文件加/解锁 poll I/O多路转换 truncate 截断文件 ftruncate 参见truncate umask 设置文件权限掩码 fsync 把文件在内存中的部分写回磁盘 文件系统操作 系统调用名称 功能描述 access 确定文件的可存取性 chdir 改变当前工作目录 fchdir 参见chdir chmod 改变文件方式 fchmod 参见chmod chown 改变文件的属主或用户组 fchown 参见chown lchown 参见chown chroot 改变根目录 stat 取文件状态信息 lstat 参见stat fstat 参见stat statfs 取文件系统信息 fstatfs 参见statfs readdir 读取目录项 getdents 读取目录项 mkdir 创建目录 mknod 创建索引节点 rmdir 删除目录 rename 文件改名 link 创建链接 symlink 创建符号链接 unlink 删除链接 readlink 读符号链接的值 mount 安装文件系统 umount 卸下文件系统 ustat 取文件系统信息 utime 改变文件的访问修改时间 utimes 参见utime quotactl 控制磁盘配额 系统控制 系统调用名称 功能描述 ioctl I/O总控制函数 _sysctl 读/写系统参数 acct 启用或禁止进程记账 getrlimit 获取系统资源上限 setrlimit 设置系统资源上限 getrusage 获取系统资源使用情况 uselib 选择要使用的二进制函数库 ioperm 设置端口I/O权限 iopl 改变进程I/O权限级别 outb 低级端口操作 reboot 重新启动 swapon 打开交换文件和设备 swapoff 关闭交换文件和设备 bdflush 控制bdflush守护进程 sysfs 取核心支持的文件系统类型 sysinfo 取得系统信息 adjtimex 调整系统时钟 alarm 设置进程的闹钟 getitimer 获取计时器值 setitimer 设置计时器值 gettimeofday 取时间和时区 settimeofday 设置时间和时区 stime 设置系统日期和时间 time 取得系统时间 times 取进程运行时间 uname 获取当前UNIX系统的名称、版本和主机等信息 vhangup 挂起当前终端 nfsservctl 对NFS守护进程进行控制 vm86 进入模拟8086模式 create_module 创建可装载的模块项 delete_module 删除可装载的模块项 init_module 初始化模块 query_module 查询模块信息 *get_kernel_syms 取得核心符号,已被query_module代替 内存管理 系统调用名称 功能描述 brk 改变数据段空间的分配 sbrk 参见brk mlock 内存页面加锁 munlock 内存页面解锁 mlockall 调用进程所有内存页面加锁 munlockall 调用进程所有内存页面解锁 mmap 映射虚拟内存页 munmap 去除内存页映射 mremap 重新映射虚拟内存地址 msync 将映射内存中的数据写回磁盘 mprotect 设置内存映像保护 getpagesize 获取页面大小 sync 将内存缓冲区数据写回硬盘 cacheflush 将指定缓冲区中的内容写回磁盘 网络管理 系统调用名称 功能描述 getdomainname 取域名 setdomainname 设置域名 gethostid 获取主机标识号 sethostid 设置主机标识号 gethostname 获取本主机名称 sethostname 设置主机名称 socket控制 系统调用名称 功能描述 socketcall socket系统调用 socket 建立socket bind 绑定socket到端口 connect 连接远程主机 accept 响应socket连接请求 send 通过socket发送信息 sendto 发送UDP信息 sendmsg 参见send recv 通过socket接收信息 recvfrom 接收UDP信息 recvmsg 参见recv listen 监听socket端口 select 对多路同步I/O进行轮询 shutdown 关闭socket上的连接 getsockname 取得本地socket名字 getpeername 获取通信对方的socket名字 getsockopt 取端口设置 setsockopt 设置端口参数 sendfile 在文件或端口间传输数据 socketpair 创建一对已联接的无名socket 用户管理 系统调用名称 功能描述 getuid 获取用户标识号 setuid 设置用户标志号 getgid 获取组标识号 setgid 设置组标志号 getegid 获取有效组标识号 setegid 设置有效组标识号 geteuid 获取有效用户标识号 seteuid 设置有效用户标识号 setregid 分别设置真实和有效的的组标识号 setreuid 分别设置真实和有效的用户标识号 getresgid 分别获取真实的,有效的和保存过的组标识号 setresgid 分别设置真实的,有效的和保存过的组标识号 getresuid 分别获取真实的,有效的和保存过的用户标识号 setresuid 分别设置真实的,有效的和保存过的用户标识号 setfsgid 设置文件系统检查时使用的组标识号 setfsuid 设置文件系统检查时使用的用户标识号 getgroups 获取后补组标志清单 setgroups 设置后补组标志清单 进程间通信 系统调用名称 功能描述 ipc 进程间通信总控制调用 信号 系统调用名称 功能描述 sigaction 设置对指定信号的处理方法 sigprocmask 根据参数对信号集中的信号执行阻塞/解除阻塞等操作 sigpending 为指定的被阻塞信号设置队列 sigsuspend 挂起进程等待特定信号 signal 参见signal kill 向进程或进程组发信号 *sigblock 向被阻塞信号掩码中添加信号,已被sigprocmask代替 *siggetmask 取得现有阻塞信号掩码,已被sigprocmask代替 *sigsetmask 用给定信号掩码替换现有阻塞信号掩码,已被sigprocmask代替 *sigmask 将给定的信号转化为掩码,已被sigprocmask代替 *sigpause 作用同sigsuspend,已被sigsuspend代替 sigvec 为兼容BSD而设的信号处理函数,作用类似sigaction ssetmask ANSI C的信号处理函数,作用类似sigaction 消息 系统调用名称 功能描述 msgctl 消息控制操作 msgget 获取消息队列 msgsnd 发消息 msgrcv 取消息 管道 系统调用名称 功能描述 pipe 创建管道 信号量 系统调用名称 功能描述 semctl 信号量控制 semget 获取一组信号量 semop 信号量操作 共享内存 系统调用名称 功能描述 shmctl 控制共享内存 shmget 获取共享内存 shmat 连接共享内存 shmdt 拆卸共享内存]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[skiplist跳跃表]]></title>
      <url>%2F2016%2F06%2F14%2Fskiplist%2F</url>
      <content type="text"><![CDATA[基本情况 对于有序链表，查找、插入、删除的时间复杂度皆为O(n)，不适用于频繁的上述操作 其他适合于上述操作的数据结构各有优劣，比如平衡二叉查找树，比较花费空间，但查询效率可以保证 跳跃表实现比较简单，结合随机特性，可以保证查找、插入与删除的操作的平均时间复杂度低于O(n) 跳跃表的结点 每个结点包含键(key)与值(value) 每个结点可包含多个指向其他结点的指针，至少包含一个，至多包含n个，n为跳跃表的最大层数 跳跃表的结构 除了普通结点之外，还包含头结点和尾结点，并且跳跃表的总层数为n 每一层都包含头尾结点 如果结点位于第k层，那么它必定位于第k-1层，第k-1层的结点数必定不少于第k层 每个结点均位于第1层，后面每一层的结点数越来越少，但是除了首尾结点之外，其他结点必定保持有序 跳跃表包含随机性，随机性指的是每个结点所在的层数是在它被加入跳跃表时随机生成的，这个特性可用于保证各个操作的平均时间复杂度低于O(n) 跳跃表主要操作及复杂度分析插入操作过程描述 设置当前结点为头结点，当前层数为第n层 随机生成一个整数k，满足1&lt;=k&lt;=n，表示即将插入的结点所在的总层数 对比要插入的key与当前结点的下一结点的key（指当前的层的下一个结点，下同），如果比下一结点的key小，则转步骤4，如果比下一结点的key大，则转步骤5，如果等于下一结点的key，说明该key已存在，直接替换下一结点的value并结束本次操作 接步骤3，说明要插入的位置必定在当前结点与下一结点之间，如果k大于等于当前层数，则记录当前结点，最后插入新结点的时候需要用到该信息，如果当前层数不是第1层，那么当前层数减1，返回步骤3继续，否则转步骤6 接步骤3，说明要插入的位置必定在下一结点之后，因此将当前结点指向下一结点，返回步骤3继续 将当前结点加入第1-k层，每一层插入的位置已在步骤4中记录，操作结束 复杂度分析 最坏情况下，每一层都包含所有结点，那么如果要插入的位置是尾结点之前，则时间复杂度是O(n) 由于每个点的层数的随机性，假设完全随机分布，那么大致可以这样理解：第1层包含n个结点，每个结点有50%的概率出现在第2层，则第2层可能的结点数是n/2，第n层可能的结点数是第n-1层的一半 查找位置的过程中，层数越大，则每次比较之后跳过的结点数越多，平均下来看，最终执行插入操作之前遍历的结点数肯定小于总的结点数，而插入结点所需要变动指针的层数k的期望值是n/2，因此总的复杂度必定低于O(n) 查找操作过程描述 设置当前结点为头结点，当前层数为第n层 对比要查找的key与当前结点的下一结点的key，如果比下一结点的key小，则转步骤4，如果比下一结点的key大，则转步骤4，如果等于下一结点的key，直接返回下一结点的value并结束本次操作 接步骤2，说明要查找的key的位置必定在当前结点与下一结点之间，如果当前层数不是第1层，那么当前层数减1，返回步骤3继续，否则说明该key不存在跳跃表中，返回对应信息 接步骤2，说明要查找的key的位置必定在下一结点之后，因此将当前结点指向下一结点，返回步骤2继续 复杂度分析 查找过程的操作与插入过程类似，只是不需要变更跳跃表的结点指针，因此其复杂度一样低于O(n) 删除操作过程描述 设置当前结点为头结点，当前层数为第n层 对比要插入的key与当前结点的下一结点的key，如果比下一结点的key小，则转步骤3，如果比下一结点的key大，则转步骤4，如果等于下一结点的key，则将当前结点在本层的指针指向下一结点的下一结点 接步骤2，说明要删除的位置必定在当前结点与下一结点之间，如果当前层数不是第1层，那么当前层数减1，返回步骤2继续，否则结束本次操作 接步骤2，说明要删除的位置必定在下一结点之后，因此将当前结点指向下一结点，返回步骤2继续 如果需要手动释放结点占用的空间或返回被删除的结点的value，则需要在步骤2记录对应的结点，并在遍历结束之后删除它 复杂度分析 删除过程与插入过程类似，只是把对应的指针操作相反，其复杂度一样低于O(n) 跳跃表的python实现结点类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&quot;&quot;&quot;结点类&quot;&quot;&quot;class Node(object): # 构造函数 def __init__(self, key, value): if type(key) != KEY_TYPE: raise TypeError self.key = key self.value = value self.forwardNodes = &#123;&#125; self.numForward = 0 # 设置某一层的前向结点 def setForwardNode(self, level, node): if not isinstance(node, Node): raise TypeError if type(level) != int: raise TypeError if level &lt; 0: raise ValueError self.forwardNodes[level] = node self.numForward = len(self.forwardNodes) # 获取某一层的前向结点 def getForwardNode(self, level): if type(level) != int: raise TypeError if level &lt; 0: raise ValueError if level not in self.forwardNodes.keys(): raise KeyError return self.forwardNodes[level] # 打印结点信息 def dump(self): print(&apos;key: %s&apos; % str(self.key)) print(&apos;value: %s&apos; % str(self.value)) print(&apos;forward nodes: %s&apos; % str(self.forwardNodes)) print(&apos;num forward: %s&apos; % str(self.numForward)) # 获取前向结点数 def numForward(self): return self.numForward # 获取结点的键 def getKey(self): return self.key # 获取结点的值 def getValue(self): return self.value # 设置结点的值 def setValue(self, value): self.value = value 跳跃表类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148&quot;&quot;&quot;跳跃表类&quot;&quot;&quot;class SkipList(object): # 构造函数 def __init__(self, level = MAX_LEVEL): if type(level) != int: print(type(level)) raise TypeError if level &lt; 1: raise ValueError self.level = int(level) self.nil = Node(-1, -1) self.head = Node(0, 0) for i in xrange(self.level): self.head.setForwardNode(i + 1, self.nil) # 插入结点时获取随机层数 def genRandomLevel(self): return random.randint(1, self.level) # 插入新的结点 def insert(self, key, value): node = Node(key, value) tmpLevel = self.genRandomLevel() tmpNode = self.head backwardNodes = &#123;&#125; while tmpLevel &gt;= 1: forwardNode = tmpNode.getForwardNode(tmpLevel) if forwardNode == self.nil: backwardNodes[tmpLevel] = tmpNode tmpLevel -= 1 continue if forwardNode.getKey() &lt; key: tmpNode = forwardNode continue elif forwardNode.getKey() &gt; key: backwardNodes[tmpLevel] = tmpNode tmpLevel -= 1 continue # 键已存在，直接替换结点的值即可 forwardNode.setValue(value) return for level in backwardNodes.keys(): node.setForwardNode(level, backwardNodes[level].getForwardNode(level)) backwardNodes[level].setForwardNode(level, node) # 搜索某个键对应的值 def search(self, key): if type(key) != KEY_TYPE: raise TypeError tmpLevel = self.level tmpNode = self.head while tmpLevel &gt;= 1: forwardNode = tmpNode.getForwardNode(tmpLevel) if forwardNode == self.nil: tmpLevel -= 1 continue if forwardNode.getKey() &lt; key: tmpNode = forwardNode continue elif forwardNode.getKey() &gt; key: tmpLevel -= 1 continue # 找到对应结点则直接返回它的值 return forwardNode.getValue() # 找不到对应结点，返回None return None # 删除键对应的结点 def delete(self, key): if type(key) != KEY_TYPE: raise TypeError tmpLevel = self.level tmpNode = self.head node = None while tmpLevel &gt;= 1: forwardNode = tmpNode.getForwardNode(tmpLevel) if forwardNode == self.nil: tmpLevel -= 1 continue if forwardNode.getKey() &lt; key: tmpNode = forwardNode continue elif forwardNode.getKey() &gt; key: tmpLevel -= 1 continue # 结点在某一层存在指针，调整该层的指针 tmpNode.setForwardNode(tmpLevel, forwardNode.getForwardNode(tmpLevel)) node = forwardNode value = forwardNode.getValue() # 键对应的结点存在，则删除之，并返回它的值 if node is not None: value = node.getValue() del(node) return value # 找不到对应的结点，抛出异常 raise KeyError # 打印跳跃表 def dump(self): tmpLevel = self.level tmpNode = self.head line = &apos;&apos; while tmpLevel &gt;= 1: forwardNode = tmpNode.getForwardNode(tmpLevel) if forwardNode != self.nil: line += &apos;(%s, %s) -&gt; &apos; % (forwardNode.getKey(), forwardNode.getValue()) tmpNode = forwardNode continue print(&apos;level %d: (head) -&gt; %s (tail)&apos; % (tmpLevel, line)) tmpNode = self.head tmpLevel -= 1 line = &apos;&apos; 简单测试类12345678910111213141516if __name__ == &apos;__main__&apos;: skipList = SkipList() keys = list(range(0, 10)) for key in keys: skipList.insert(key, key) for key in keys: skipList.insert(key, key) skipList.dump() print(&quot;delete 3:&quot;, skipList.delete(3)) skipList.dump() print(&quot;delete 8:&quot;, skipList.delete(8)) skipList.dump() keys = list(range(0, 11)) for key in keys: print(&apos;search key %d&apos; % key) print(skipList.search(key))]]></content>
    </entry>

    
  
  
</search>
